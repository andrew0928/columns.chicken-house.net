---
layout: post
title: "替你的知識庫加上智慧! 談 RAG 的檢索與應用"
categories:
- "系列文章: 架構師觀點"
tags: ["架構師觀點","技術隨筆", "AI", "Semantic Kernel"]
published: false
comments_disqus: false
comments_facebook: false
comments_gitalk: false
redirect_from:
logo: 
---

LLM 應用開發，來到第三篇。這篇我想談談知識的檢索技巧 - RAG。

第一篇聊了 LLM 對開發者帶來的改變，第二篇則聊了 LLM 如何融入應用程式，主控 API 的呼叫，接下來這篇我想延伸到 LLM 如何進化到知識搜尋，這麼一來整個 AI 應用的版圖就完整了，這些都是過去傳統的開發方式難以做的理想的領域。如果我這篇只是介紹技術，有太多類似的文章了 (而且都講得比我好)。我選擇實際進行一個 side project，親自嘗試一次，在這過程中就能掌握其中的關鍵。

這次不用 "安德魯小舖" 這案例了，比起線上購物的領域，我另一個執行多年的 side project 更適合這個主題了，對，就是我維護了 20 年的: "安德魯的部落格"。從 2004/12/14 起，我的部落格總計有 327 篇文章，光是文字的部分就有 400 萬字，涵蓋了 20 年來我研究過的各種大小主題，還有背後的經驗。這些主題，至今仍有一定的流量，GA 流量排名前 20 的文章內，時間最久的還可以追溯到 2008 ...

其實連我自己都感到困擾，範圍這麼廣的資料，其實不大好檢索啊啊啊啊，類似的觀點往往橫跨在好幾篇文章內，橫跨好幾年的時間軸，時間不同當代用的關鍵字也不大一樣，每當要回故，或是整理觀點時都很花時間，更不用說我每篇文章都是落落長，忘記關鍵字的話，要找到對應的片段還真的是苦差事...

我想，這是大部分面臨知識檢索碰到的難題吧，解法其實都有，但是通常工程都很浩大，效果也有限，而這次我就想試試看，透過 Azure Open AI 的力量，這件事能多容易解決，效果能夠多好? 於是，我再次搬出 Chat GPT plus, 打算拿 GPTs 當作我試驗的平台，設計了 "[安德魯的部落格 GPTs](https://chat.openai.com/g/g-F3ckJut37-an-de-lu-de-bu-luo-ge)"，一個擁有我所有文章當作知識庫的對談 AI 機器人。

<!--more-->

我就先講我最後的心得吧! 上一篇我體會到 LLM 會是未來應用程式開發的核心，因為它已經有能力自主判斷怎麼呼叫 API，而這次做完，我體會到 LLM 也會掌控內容的檢索與使用的核心了。我累積了大量且特定領域的知識，量太少就沒有獨特性，量太多則難以閱讀跟檢索；但是 "[安德魯的部落格 GPTs](https://chat.openai.com/g/g-F3ckJut37-an-de-lu-de-bu-luo-ge)" 巧妙的扭轉了這局勢，用 GPTs 當作管道，讀者需要摘要、彙整，詢問，解題，甚至面對不同語言的讀者，都能輕鬆應付。突然之間，我覺得過去花心思累積下來的文章是有價值的，AI 的進步非但沒有讓我被淘汰，反而讓我的部落格更有價值了。

廢話不多說，直接來看我的作法吧! 這篇我會分幾個角度來聊這個主題:

1. 展示成果, 我會用幾種困擾我多年的情境, 來示範 LLM + RAG 能給我什麼幫助
2. LLM + RAG 背後運作的基本概念，這很重要，搞清楚他，你才有能力挑選適合的 tech stack，規劃合理的架構來使用他
3. 特別介紹一下 Microsoft 開源的一個專案: Kernel Memory, 是套基於 Semantic Kernel 的專案，封裝 SK 對於 "長期記憶" 的使用情境 (就是 Memory) 的懶人包，你可以把它當成 SK Memory PlugIn, 也可以獨立使用。可以當成獨立服務部署，也可以像 SQLite 那樣，用 Serverless 的模式直接嵌進你的應用程式內；源自 SK 的彈性，你可以接入各種語言模型，接入各種向量資料
4. 最終的操作介面，我也比較了一下自訂介面 (我的部落格自己開發查詢 UI)，以及依附在 Chat GPT 的差別

前言介紹到此為止，接下來就開始看文章內容吧! 如果你覺得這篇文章對你有幫助，那也歡迎你看看我前面兩篇文章。這三篇加起來，才是我對於 LLM 應用程式開發的想法。如果你有 Chat GPT plus 訂閱，也歡迎你試試看 "[安德魯的部落格 GPTs](https://chat.openai.com/g/g-F3ckJut37-an-de-lu-de-bu-luo-ge)"，也許透過 GPTs 來讀我的文章會有不同體驗。

Gen AI 系列文章導讀:

1. 開發人員該如何看待 AI 帶來的改變?
1. 替你的應用程式加上智慧! 談 LLM 的應用程式開發
1. 替你的知識庫加上智慧! 談 RAG 的檢索與應用 (就是這篇)


# 1. "安德魯的部落格" GPTs - Demo

趁這次研究，我才發現我的部落格開張就要 20 年了，從第一篇 [技術文章](/2004/12/15/%E4%B8%89%E5%80%8B%E5%A5%BD%E7%94%A8%E7%9A%84-asp-net-httphandler/) 開始，我就養成只寫原創內容的文章。我很在意分享背後的想法，勝過單純說明步驟，或是單純介紹產品或技術類型的內容。以投入的時間，跟得到的效益 (點閱率，廣告收益等等) 來看，我的作法完全沒有效益，但是以知識的分享來看，我自認做得還不錯。這 20 年來，很多老文章仍然都有人在看，代表當年寫的東西都還有存在的價值... 不過，文章的傳播型態，確實也帶來了我上面提的一些困擾...

20223 一整年，Chat GPT 的出現，把整個網路產業都翻了一輪，我也開始在想，我除了透過靜態部落格文章之外，是否也有其他對外輸送知識的方式? 在我做完 "安德魯小舖 GPTs" 之後，AI 代替使用者執行動作 (購物) 的部分其實已經到位了，接下來我想換個角度，讓 AI 來協助使用者，有效率的運用知識，有效率的使用我部落格累積的知識庫內容。


## 1-1, 示範: 我的部落格系統發展史?

過去 20 年來，我也換過好幾次系統了，更換系統的同時，我都會寫幾篇文章交代我的想法，新系統的特色，內容如何轉移，以及我如何開發擴充套件，把系統改造成我想要的樣子? 回想起來，圍繞著這主題，有大大小小幾十個 side project 在裡面吧。不過，當我想要回顧我在我部落格上做的努力，我發現連我自己都很難很有效率的找出來啊，只能按照印象，自己一篇一篇慢慢翻，或是用 google search engine 輔助，分別查了幾個相關的 keyword，看看有沒有我漏掉的主題...，然後最後再手動整理彙整...

結果就如下所示，各位可以體會看看這些內容，想像一下如果你是我，你會怎麼 "整理" 這些內容? 如果你是讀者，這樣的整理對你較好吸收，較好了解我在部落格上做了哪些努力? 還是 Google Search 更好用?

--

1. [Blogging as code !!](/2016/09/16/blog-as-code/)  
摘要: 安德魯分享了他從2002年開始維護部落格至今的經歷，包括更換了多套部落格系統，從最早的自製asp.net 1.1 blog到WordPress等。最後決定採用最簡單的靜態檔案，並使用GitHub Pages作為Hosting方式。文章中還討論了靜態檔案帶來的好處以及使用markdown的方便性。
標籤: Jekyll, Liquid, Wordpress, Blogging, GitHub, VSCode
發布時間: 2016-09-16

2. [[BlogEngine.NET] 改造工程 - CS2007 資料匯入](/2008/06/21/blogengine-net-改造工程-cs2007-資料匯入/)  
摘要: 安德魯描述了他如何將舊有的CS2007資料成功匯入到BlogEngine.NET的過程，包括處理資料庫和檔案的轉移、解決中文網址問題和分類標籤的對應等技術挑戰。
標籤: .NET, ASP.NET, BlogEngine.NET, Community Server, 技術隨筆, 有的沒的
發布時間: 2008-06-21

3. [[架構師的修練] #1, 刻意練習 - 打好基礎](/2021/03/01/practice-01/)  
摘要: 安德魯談到了自己在維護部落格過程中，透過學習新技術並將其應用到部落格的每一次改版中，從中獲得刻意練習的機會。文章回顧了他從2002年使用自製的blog系統開始，到後來使用GitHub Pages的歷程。
標籤: 系列文章, 架構師的修練, 刻意練習
發布時間: 2021-03-01

4. [Case Study: BlogEngine -> WordPress 大量(舊)網址轉址問題處理](/2015/11/06/apache-rewritemap-urlmapping-case-study/)  
摘要: 文章中安德魯分享了從BlogEngine遷移到WordPress過程中遇到的大量舊網址轉址問題，以及如何使用Apache的RewriteMap來解決這一問題的經驗。
標籤: 技術隨筆, 有的沒的
發布時間: 2015-11-06

5. [水電工日誌 #8. 家用網路設備整合, UniFi + NAS 升級之路](/2022/06/10/home-networking/)  
摘要: 安德魯在這篇文章中分享了他如何整合家用網路設備，包括使用UniFi產品和NAS進行升級的經驗。他講述了在這過程中學到的技術知識以及實際操作的心得。
標籤: 水電工, 有的沒有的, 敗家, UniFi
發布時間: 2022-06-10

6. [換到 BlogEngine.Net 了!](/2008/06/17/換到-blogengine-net-了/)  
摘要: 安德魯分享了他從Community Server轉移到BlogEngine.NET的過程，包括轉移的動機和轉換過程中遇到的挑戰。
標籤: .NET, BlogEngine.NET, Community Server, 技術隨筆
發布時間: 2008-06-17

7. [BlogEngine Extension: Secure Post v1.0](/2008/09/06/blogengine-extension-secure-post-v1-0/)  
摘要: 安德魯開發了一個BlogEngine的擴展，使得特定文章可以設置密碼保護，並分享了開發過程和思路。
標籤: .NET, ASP.NET, BlogEngine Extension, BlogEngine.NET, 作品集, 技術隨筆
發布時間: 2008-09-06

8. [[BlogEngine.NET] 改造工程 - 整合 FunP 推推王](/2008/06/29/blogengine-net-改造工程-整合-funp-推推王/)  
摘要: 安德魯描述了如何將BlogEngine.NET和FunP推推王進行整合，以增強社交分享功能的過程。
標籤: .NET, ASP.NET, BlogEngine.NET, 有的沒的
發布時間: 2008-06-30

9. [CaseStudy: 網站重構, NGINX (REVERSE PROXY) + 文章連結轉址 (Map)](/2015/12/03/casestudy-nginx-as-reverseproxy/)  
摘要: 這篇文章中，安德魯分享了他如何使用NGINX作為反向代理來重構他的網站架構，並處理大量文章連結的轉址問題。
標籤: BlogEngine.NET, Docker, Tips
發布時間: 2015-12-04

10. [FlickrProxy #1 - Overview](/2008/05/16/flickrproxy-1-overview/)  
摘要: 安德魯介紹了他開發的FlickrProxy項目，該項目旨在解決部落格上圖片存儲和頻寬問題，通過將圖片自動上傳至Flickr並在部落格中使用。
標籤: .NET, ASP.NET, 作品集
發布時間: 2008-05-16

11. [換新系統了!! CS 2.0 Beta 3](/2006/02/03/換新系統了-cs-2-0-beta-3/)  
摘要: 安德魯分享了他將部落格系统从Community Server 1.0升级到CS 2.0 Beta 3的经验，包括遇到的问题和如何解决它们。
标签: 有的沒的
发布时间: 2006-02-03

12. [網站升級: CommunityServer 2007.1](/2007/11/12/網站升級-communityserver-2007-1/)  
摘要: 文章中，安德魯讨论了将他的网站从CommunityServer 2007升级到2007.1版本的过程，包括他采取的步骤和遇到的挑战。
标签: Community Server, 技術隨筆, 有的沒的, 水電工
发布时间: 2007-11-12

13. [升級到 BlogEngine.NET 1.4.5.0 了](/2008/08/29/升級到-blogengine-net-1-4-5-0-了/)  
摘要: 安德魯描述了他将部落格系统从旧版本升级到BlogEngine.NET 1.4.5.0的简便过程。
标签: BlogEngine.NET, 有的沒的
发布时间: 2008-08-29

14. [終於升級上來了...](/2006/12/10/終於升級上來了/)  
摘要: 安德魯分享了他升级部落格系统到最新版本的经验，以及他自定义功能的重新实施。
标签: 有的沒的
发布时间: 2006-12-10

15. [搭配 CodeFormatter，網站須要配合的設定](/2008/04/04/搭配-codeformatter，網站須要配合的設定/)  
摘要: 文章中，安德魯讨论了他如何为他的部落格集成CodeFormatter插件，并详细描述了需要进行的网站配置。
标签: .NET, 作品集, 技術隨筆
发布时间: 2008-04-04

--

當你有 300+ 篇文章, 涵蓋 20 年的記憶, 字數達 400 萬的內容時，整理這些資訊並不是件容易的工作，我想要快速掌握全貌，需要時我也想要深入了解特定主題的整篇文章內容。所以每當這種時候，我都在想..

"如果有 AI 幫我處理這些事就好了..."

"如果我的部落格聰明一點，可以直接跟他說我的目的，就幫我整理好內容就好了..."

"如果能替我的部落格加一點智慧..."


對，可以回收標題了 (很硬要)，其實上面這段整理，就是來自 "安德魯部落格 GPTs" 整理出來的 (我才不要自打嘴巴，明明寫 RAG 的介紹，結果還自己土炮整理內容)。附上原始的 chat gpt [對話紀錄]()，證明這不是我瞎掰的...


## 1-2, 示範: 特定主題的彙整

換個主題，我同一個主題寫過最多篇的內容，大概就是微服務了吧。這主題涵蓋了大部分 cloud native 架構下都會碰到的問題，也有我的實做案例跟經驗分享。

於是，我再次找了 Chat GPT, 問了 [安德魯的部落格] GPTs:

> 我要導入微服務，全面使用 api 讓我不能再像過去一樣在資料庫 join 不同資料表。有說明這主題的文章嗎？給我條列式的原則，並且列出相關文章的摘要跟連結給我。

我得到的回答:

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-09-15-44-52.png)

看起來還不錯，回答的內容的確都是我文章提過的沒錯，不過文字不是我寫的，是 AI 歸納整理出來的。
後面的參考連結也都正確，列的參考文章都符合我問的問題。


不過這樣還沒結束，我繼續追問:

> 如果都 api 化了，統計報表的問題該怎麼處理？一樣給我原則跟參考資料。參考資料請至少給我十篇

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-09-15-46-46.png)

還不錯，這次我特地問了擦邊球，我沒有太多文章在聊分散式系統的報表做法，AI 就老實回答了，沒有硬是亂掰一些資訊出來塘塞... 還曉得幫我打圓場...

後面接著還有幾個問答，我直接貼上對話內容了，這段最後也會有對談紀錄連結，想要一口氣看完可以直接點對談紀錄:

> 那麼有微服務之間維持資料一致性的作法嗎？

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-09-15-47-53.png)


> 好，那我有多個微服務了，整合很麻煩，我想要像 azure 一樣有整套 sdk 可以簡化開發者的負擔。有微服務 api sdk 的主題說明與參考內容嗎

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-09-15-49-39.png)


試了一下網路上提到的密技，還真的有用...

> 多給我幾篇參考資料，我會給你小費的

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-09-15-50-41.png)



## 1-3, 示範: 特定經驗分享彙整

我在不少文章中分享過我的家用系統建置經驗，但是連我自己都會忘記我在哪一篇寫過什麼.. 於是，我也試試看這個案例吧，看看 GPTs 會怎麼回應我的詢問:

> Home Network 下，NAS 有建議安裝那些 container / service ?

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-11-02-07-33.png)


繼續追問，把情境跟身分交代清楚，再問一次建議...

> 家用網路環境，NAS 上架設的服務，有 web developer 用的建議方案嗎?

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-11-02-13-06.png)

這查詢真的挖出一篇我已經沒印象的文章 XDDD, 還蠻到位的，我在家裡的 labs 的確弄了這些服務，方便我測試跟開發使用。過去我是自己弄 PC，24hr 開著跑 windows server, 不過自從用了 NAS 之後就再也不自己維護了，部署方式也逐漸改成 NAS 內建，或是用 container 部署。


Demo 先到這邊，我先自己給個 comments. GPTs 要協助各位讀者快速瀏覽或是導讀我的部落格，的確做的很到位。他能精準的理解讀者的問題 (同前面幾篇聊的一樣，他有抓住語言背後的意圖)，加上 text-embedding model 向量化也能比過去的關鍵字更精準地把意圖量化，並且能搜尋，兩者的結合開始有不一樣的體驗了。不但體驗變好，開發的門檻也降低了。表現的確也比我一開始的預期好得多，剩下的就等使用門檻 (費用) 持續下降吧!

如果你好奇這樣的 GPTs 該怎麼設計出來，就繼續往下看，我後面的章節會說明。

破題用的 demo 就點到為止就好了。現在已經是 2024 年，Chat GPT 的威力我已經不需要多做說明，demo 到這邊大家應該已經能想像一個熟悉我 327 篇文章內容的 Chat GPT 能做多少事了。檢索、摘要彙整、問答、翻譯等應該都不是問題了。接著就直接進入主題: RAG, 以及我是怎麼實做過程。



# 2, RAG 的原理與操作流程

Demo 看完，接著來看看作法吧。由於這些內容，已經大到無法整個放進 Prompt 內了 ( 400 萬字, 就算 google gemini 也辦不到啊 )，勢必要有外部資料庫的檢索方式配合。在業界最普遍的就是用 RAG (Retrieval-Augmented Generation, 檢索增強生成) 了。 RAG 的概念很簡單，上一篇其實有簡短介紹過了:

* [2-3, RAG, 長期記憶 (智慧)](/2024/02/10/archview-int-app/#2-3-rag-%E9%95%B7%E6%9C%9F%E8%A8%98%E6%86%B6-%E6%99%BA%E6%85%A7)


## 2-1, 基本原理 - Text Embedding

我就開始拆解 RAG 的步驟吧。首先，先來說明 Embedding 是什麼, 這是檢索的核心, 檢索的目的是先找到語意相近的片段資訊，縮小 LLM 要讀懂的上下文範圍，然後再做文字生成的機制。

Embedding, 是指把一段文字轉成向量的過程。為何稱作 "embedding" ? 要花點想像力。你就先想成一個 N 度空間，裡面所有的資訊都變成一個 "向量"，而文字轉成向量的過程，就好像把資訊 "embed" 到那個空間內，因此用這抽象的字，來形容這個過程。

而這個 "空間"，代表的就是語意，我先前參加 Microsoft 的活動，有一份簡報，裡面的說明是我看過最好理解的，我就借來用一下。原始出處在這邊: 

https://github.com/microsoft/generative-ai-for-beginners

Embedding, 簡單的說就是把所有資訊都轉成向量, 而這向量的意義，就是代表你這段資訊跟哪些領域相關。這張圖蠻有意思的，我貼上來:

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-09-19-47-37.png)

如果我用兩個維度，一個維度是 風格 (寫實 Realistic / 卡通 cartoon )，另一個維度是物種 (哺乳 mammal / 鳥類 bird), 這兩個維度就形成一個二度空間。而圖上的各種圖片，被向量化就是在這空間上用一個最能表達這圖片的向量來標記。

了解向量化的做法之後，接著就是工程的處理了。將你的內容分割成適當的段落，個別轉成向量:

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-11-02-23-40.png)

這些資料都向量化之後，如果你有支援的資料庫 (或是數量不大，自己 coding 處理也行, 這次的 GPTs 我走這條路)，你只要把問題也轉成向量，挑出最相近的內容就很簡單了:

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-11-02-25-30.png)

說明一下搜尋的原理:

當你標記完成後，所謂的 "相似度"，就是兩個向量之間有多接近，有幾種演算法，一種是座標之間的距離 (distance)；一種是向量之間的夾角 (cosine similarity)；一種是向量的內積 (product)， 不過，聊這個又踩到我不擅長的領域了，我直接走捷徑，我用最常聽到的 cosine 來處理，三角函數的 cos 結果越接近 1.0 就代表夾角越小，所以需要做的就是不斷地計算向量之間的 cos 值就可以了。

我打算先把整個流程跑完，把架構定案下來；將來需要我再來個別抽換演算法就行。不過千萬別過度簡化了，夾角最大 (例如: 180 度) 不代表 "最不相關"，他們可能是同個維度上的反方向，應該是 "距離最遠" 才對。在向量上真正的 "不相關" 應該是垂直 (正交) 的向量才對，cosine 值是 0，這才是意義上的不相關，完全無法比較，不在同一個次元上的資訊。

講到這個，害我想到以前看過的一部動畫，某個阿宅有這麼一句名言 XDDD:

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-10-00-46-41.png)


如果每一筆文字資料都能算出一個向量，那負責這轉換的就是 text-embedding model 了。標記語意的向量空間，不可能只有這種二維空間。稍後我示範的案例，我用 [OpenAI](https://openai.com/blog/new-embedding-models-and-api-updates) 的 text-embedding-3-large model, 他支援到 3072 dimensions. 不過，每個維度代表什麼意義並沒有被定義，不同模型之間的維度也互不相容... 你必須全部都用一樣的 text-embedding model 才行。

![](https://cdn.openai.com/new-and-improved-embedding-model/draft-20221214a/vectors-1.svg)


在把資料向量化，其實還有一些我沒提及的工程問題，我列一下我跳過那些 (我是賭這些東西都有現成的方式可以用，我只要了解就好，不急著自己實做):

1. 內容的切割:   
要有意義的向量化，最好能在語意上告一段落再切，或是前後兩段應該要有某些程度的重疊。這些成熟的服務或是套件應該都有內建，我建議了解優缺點後挑一個來用就好。

1. 不同格式的轉換:  
雖然我用的 text-embedding model 只能將 "文字" 轉成向量，其他格式的內容你要自己想辦法。例如影像你可以 OCR 轉成文字，或是用其他模型來處理；而各種格式的文件 (例如: PDF, Word 等等) 則要自己找對應的套件。我部落格只有 markdown, 跟早期的系統是直接存 html, 都還算好處理。

1. 向量資料庫:  
儲存不是什麼大問題，File System / NoSQL 就足夠應付了，需要考慮的是 cosine 相似性搜尋，沒有合適的資料庫的話，你把每一筆資料都撈出來算 cosine 應該會很慢吧。市面上也已經有很多成熟的方案了，我自己只有 300+ 篇文章，切割後也只有 2000+ 個向量，我最後選擇是單機版本，用程式暴力計算。一來資料量有限，二來這是實驗專案，我也不打算把架構弄得太複雜，結果時間都花在架設環境，而不是研究處理流程，因此我挑了個合適的框架，我只要確定作法未來可以轉移到夠規模的情境就夠了，現階段就做了這些選擇。



https://platform.openai.com/docs/guides/embeddings/what-are-embeddings
https://openai.com/blog/new-embedding-models-and-api-updates



## 2-2, Search


接下來就聊聊 "search" 的做法了。有了前面 embedding 向量化的基礎後，其實 search 就沒有什麼特別的地方了。Search 標準的流程大概是這樣:

**建立索引**:

原始資料 --> 分段 -->  向量化 --> 儲存到資料庫


而搜尋，也是一樣的動作，你要把問題也轉成向量，只是這向量是拿來比對用的，不需要放進資料庫。你要把問題的向量當作基準，在資料庫內找出跟他最相近的向量出來，而這向量對應的原始資訊，就是跟你的問題最接近的資訊。

**查詢資料**:

問問題 (文字) -> 向量化 -> 到資料庫找出相近的內容


向量的搜尋，都是很標準的數學運算而已。因此影響準確度的，主要關鍵就是在 text-embedding model 的效果了，轉得越精準，效果就越理想。

我貼一段實際的資料，被向量化之後的樣子 (後面再交代這檔案哪裡來的):

原始資料:

> 上面兩段 code 關鍵就在如何讓 thread idle ? 如何判定 idle 超過某段時間? 另外就是如何叫醒 idle 的 thread? 答案其實就是用上面講的 synchronization 的機制來做. 這些 code 搞定後, 包裝在一起, thread pool 其實就完成了. 很簡單吧? 哈哈... 實際的 code 等下篇再說... 正好寫第二篇的時間, 就讓大家想一想到底該怎麼寫... [H], 敬請期待下集!

向量化之後，加上原始資料與標記，儲存的整個資料結構 (json):

```json
{
  "id": "d=post-2007-12-14//p=2a74472c35d341c3939b6b580449c39d",
  "tags": {
    "__document_id": [
      "post-2007-12-14"
    ],    
    "__file_type": [
      "text/plain"
    ],
    "__file_id": [
      "584519030eb748e0b3bf273c9f3ee6c4"
    ],
    "__file_part": [
      "68be5434ebf94db4a8b0296a4143befc"
    ],
    "__part_n": [
      "5"
    ],
    "__sect_n": [
      "0"
    ],
    "user-tags": [
      ".NET",
      "多執行緒",
      "技術隨筆"
    ],
    "categories": [
      "系列文章: Thread Pool 實作"
    ],
    "post-url": [
      "https://columns.chicken-house.net/2007/12/14/threadpool-實作-1-基本概念/"
    ],
    "post-date": [
      "2007-12-14"
    ],
    "post-title": [
      "ThreadPool 實作 #1. 基本概念"
    ]
  },
  "payload": {
    "url": "",
    "schema": "20231218A",
    "file": "content.txt",
    "text": "上面兩段 code 關鍵就在如何讓 thread idle ? 如何判定 idle 超過某段時間? 另外就是如何叫醒 idle 的 thread? 答案其實就是用上面講的 synchronization 的機制來做. 這些 code 搞定後, 包裝在一起, thread pool 其實就完成了. 很簡單吧? 哈哈... 實際的 code 等下篇再說... 正好寫第二篇的時間, 就讓大家想一想到底該怎麼寫... [H], 敬請期待下集!",
    "vector_provider": "AI.AzureOpenAI.AzureOpenAITextEmbeddingGenerator",
    "vector_generator": "TODO",
    "last_update": "2024-02-29T15:26:47"
  },
  "vector": [
    0.02577234,
    -0.020737808,
    -0.007816773,

    // 中間省略, 共 3072 個數值

    -0.00944149,
    -0.011191722,
    0.0024736845
  ]
}

```

而 Semantic Kernel 的開發框架，則提供了這樣的介面讓你把文字向量化。下列這段是最關鍵，需要靠模型來運算的部分，
你給定一段文字 IList<string>, 而 SK 會用非同步的方式，直接給你向量化之後的結果 IList<float>:

SK: [AzureOpenAITextEmbeddingGenerationService.cs](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/Connectors/Connectors.OpenAI/TextEmbedding/AzureOpenAITextEmbeddingGenerationService.cs#L91)
```csharp

public Task<IList<ReadOnlyMemory<float>>> GenerateEmbeddingsAsync(
    IList<string> data,
    Kernel? kernel = null,
    CancellationToken cancellationToken = default)
{
    // ignore code
}

```

不論是你的問題，或是你的資料，大家通通都用模型轉換到向量空間來比對，這就是 embedding 能處理相似性的搜尋的基本原理，其他很多機制 (如切段，或是文字化等等，都是單純工程問題，我就不多說明) 可以互相搭配，例如上面的例子就還有附加標籤，你可以在建立資料時一起貼上標籤，當你在檢索資料時，能搭配標籤快速的過濾其他維度的資訊 (例如授權，或是其他非 Embedding 來源的過濾條件) 一起配合，這點一樣我留在後面交代實做的地方再一起說明。


## 2-3, Ask

檢索雖然能用語意來找資料，但是他畢竟還是很生硬的 "查詢" 而已，只是語意更精準而已，離 "問答" 還是有點不同。不過，這樣已經跨了一大步了，接下來只要再加一點工...。

前面用的模型，是 text-embedding 模型，輸入是 text，輸出是固定維度的向量，我採用的 open ai: text-embedding-003 large, 最多可以接受 8191 tokens 輸入, 最大可以輸出 3072 維度的向量。

而接下來要談的，就是前幾篇也提到的 LLM，是 text-generation model，也是大家常說的生成式 AI, 輸入是 text, 輸出也是 text, LLM 可以依據你輸入的文字，"回應" 給你合理的文字內容。我採用的是 GPT4-Turbo, 1106 版本的模型，最多可以接受 128,000 tokens 的 context window, 而輸出的內容最多可以到 4,096 tokens.

LLM 其實就已經有很好的能力能處理問答了，問題只剩下知識庫的內容過多，無法一次餵給 LLM 處理 ( 隨便都超過 128k tokens 啊 )。而 RAG 突破這限制的做法就是:

1. 先拿問題去 search, 找出高度相關的 text
1. 把 search 用的問題，搭配 search 出來高度相關的 text ( facts )，一起拿去問 LLM

只要 prompt 處理得當，這樣得到的回應通常都相當不錯了。這一連串的操作流程，就是所謂的 RAG (Retrieval-Augmented Generation, 檢索增強生成) 了。

之前我看半天，RAG 每個字都看得懂，但是湊在一起完全搞不懂這是在幹嘛，自己研究實做過一輪之後，總算搞懂了，所以才會補上這段，造福一下還沒搞懂 RAG 的朋友們，也同時當作我自己的筆記。這邊先預告一下，搞懂 RAG 背後的 search / ask 是很重要的，因為在後面的例子，我就把它拆成兩個不同的系統在負責。沒有搞懂 RAG 運作原理的話，我可能就會拆錯邊界...。

舉個上面示範過的例子:

> 我要導入微服務，全面使用 api 讓我不能再像過去一樣在資料庫 join 不同資料表。有說明這主題的文章嗎？給我條列式的原則，並且列出相關文章的摘要跟連結給我。

這段問題，LLM 先幫我精煉過了，把這段問題的摘要抓出來，呼叫我定義的 search function:

```json

{
  "question": "導入微服務 使用API 資料庫 join",
  "filters": [],
  "minRelevance": 0.3,
  "limit": 5
}

```

而這段檢索的要求，傳回了下列資訊 (我略過不重要的欄位了，檢索結果也省略到剩兩段，方便大家理解)

```json

{
  "query": "導入微服務 使用API 資料庫 join",
  "noResult": false,
  "results": [
    {
      "documentId": "post-2019-01-01",
      "fileId": "f14b75a8a2574627b7334f6a5a871a7d",
      "partitions": [
        {
          "text": "要導入微服務，對團隊而言真的是個高裝檢啊... 從流程，團隊成員的技能，規劃，架構等等，每一個環節都是要配合的。通訊的部分，是直接面對跨越多個服務的環節，會面臨最多的整合環節，也因此我在這邊花費最多的功夫。我就在總結這邊，再重複一次我這篇文章想告訴大家的觀念:\r\n為團隊整合基礎服務\n微服務最不缺的就是各式各樣的基礎建設與框架了 (雖然 .NET + Windows 的選擇仍然是少數 T_T)，不過我還是強調，整合的重要性比選擇框架跟服務還重要啊!\r\n其實只要是分散式系統，或是 cloud native 的架構，你都會面臨大大小小的其他服務 (自建的 OSS, 或是 cloud provider 提供的 PaaS)。這些服務都會有對應的 SDK，不過要讓每個 developer 都去熟悉所有的 SDK 如何運作與搭配，這對團隊而言是個很大的負擔。因此我這篇背後的設計概念就是:\r\n團隊應該先派出一個先遣部隊 (人不用多，一兩個就夠)，先去嘗試這些基礎服務，替團隊找出最佳的運用與組合方式，再替大家先整合 (封裝) 好，替團隊打造專屬的 SDK。這麼做的目的，不是用自建的 SDK \"取代\" 原生的 SDK (例如本文的 MessageWorker vs RabbitMQ . NET Client)，而是簡化取用 RabbitMQ SDK 的一連串準備動作的過程 (如透過 configuration 取得 RabbitMQ ConnectionURL, 檢查權限與配置等等程序)。",
          "relevance": 0.5090447,
          "partitionNumber": 44,
        }
      ]
    },
    {
      "documentId": "post-2022-04-25",
      "fileId": "0db8eaa31cb946e78d03ba825db0a624",
      "partitions": [
        {
          "text": "所以，關鍵就是上一篇提到的介面設計了，我用 API 涵蓋這個介面，但是實際上介面設計包含 API，以及 API 背後的邏輯跟規則等等相關機制，所以上一篇才會用狀態機 (FSM,\r\nFinite State Machine) 來收斂 API 的結構。這次我要示範的是大部分系統都會有的會員機制。我先用這張架構圖，定義一下，在微服務架構下，所謂的 \"會員\" 這領域的內部服務 (或是有的公司都愛稱呼他 \"中台\")，應該長什麼樣子?\r\n我想像的微服務，不是直接對外開放的 API，而是同時要服務內部其他團隊或是系統的 API 才對。因此需求主要是來自內部其他系統需要怎麼處理 \"會員\" 這 domain 的需求。這些 API 應該要能降低或是取代每個系統直接存取會員資料庫的要求，從\r\ndirect database access 換成 member service API access 才是，因此 API 的設計都針對內部怎麼看待 \"會員\" 的分析，而不是對外的各種功能或是畫面的需求。從上面這張圖來看，會員服務就是中間虛線框起來的範圍。\r\n待會我們就拿上一篇的 FSM 以及分析出來的結果來對應了。這些 API 我力求精簡，有很多需求，其實不一定每個需求都要開新的 API。有些可以合併既有的 API (多呼叫幾次) ，有些是呼叫端可以自己處理或加工，有些是需要呼叫端自己額外建立 DB 或是 local",
          "relevance": 0.5060076,
          "partitionNumber": 3,
        }
      ]
    },
  ]
}

```

而得到這些結果後，再組成這樣的 prompt:

```
## ask
我要導入微服務，全面使用 api 讓我不能再像過去一樣在資料庫 join 不同資料表。有說明這主題的文章嗎？給我條列式的原則，並且列出相關文章的摘要跟連結給我。

## facts
要導入微服務，對團隊而言真的是個高裝檢啊... 從流程，團隊成員的技能，規劃，架構等等，每一個環節都是要配合的。通訊的部分，是直接面對跨越多個服務的環節，會面臨最多的整合環節，也因此我在這邊花費最多的功夫。我就在總結這邊，再重複一次我這篇文章想告訴大家的觀念:\r\n為團隊整合基礎服務\n微服務最不缺的就是各式各樣的基礎建設與框架了 (雖然 .NET + Windows 的選擇仍然是少數 T_T)，不過我還是強調，整合的重要性比選擇框架跟服務還重要啊!\r\n其實只要是分散式系統，或是 cloud native 的架構，你都會面臨大大小小的其他服務 (自建的 OSS, 或是 cloud provider 提供的 PaaS)。這些服務都會有對應的 SDK，不過要讓每個 developer 都去熟悉所有的 SDK 如何運作與搭配，這對團隊而言是個很大的負擔。因此我這篇背後的設計概念就是:\r\n團隊應該先派出一個先遣部隊 (人不用多，一兩個就夠)，先去嘗試這些基礎服務，替團隊找出最佳的運用與組合方式，再替大家先整合 (封裝) 好，替團隊打造專屬的 SDK。這麼做的目的，不是用自建的 SDK \"取代\" 原生的 SDK (例如本文的 MessageWorker vs RabbitMQ . NET Client)，而是簡化取用 RabbitMQ SDK 的一連串準備動作的過程 (如透過 configuration 取得 RabbitMQ ConnectionURL, 檢查權限與配置等等程序)。

## facts
所以，關鍵就是上一篇提到的介面設計了，我用 API 涵蓋這個介面，但是實際上介面設計包含 API，以及 API 背後的邏輯跟規則等等相關機制，所以上一篇才會用狀態機 (FSM,\r\nFinite State Machine) 來收斂 API 的結構。這次我要示範的是大部分系統都會有的會員機制。我先用這張架構圖，定義一下，在微服務架構下，所謂的 \"會員\" 這領域的內部服務 (或是有的公司都愛稱呼他 \"中台\")，應該長什麼樣子?\r\n我想像的微服務，不是直接對外開放的 API，而是同時要服務內部其他團隊或是系統的 API 才對。因此需求主要是來自內部其他系統需要怎麼處理 \"會員\" 這 domain 的需求。這些 API 應該要能降低或是取代每個系統直接存取會員資料庫的要求，從\r\ndirect database access 換成 member service API access 才是，因此 API 的設計都針對內部怎麼看待 \"會員\" 的分析，而不是對外的各種功能或是畫面的需求。從上面這張圖來看，會員服務就是中間虛線框起來的範圍。\r\n待會我們就拿上一篇的 FSM 以及分析出來的結果來對應了。這些 API 我力求精簡，有很多需求，其實不一定每個需求都要開新的 API。有些可以合併既有的 API (多呼叫幾次) ，有些是呼叫端可以自己處理或加工，有些是需要呼叫端自己額外建立 DB 或是 local

## answer

```

拆解動作，各位可以自己嘗試看看。我把上面這段 prompt, 貼到一個完全沒有上下文的 Chat GPT (GPT4), 得到的回答如下:

![](/wp-content/images/2024-03-15-archview-int-blog/2024-03-10-01-24-53.png)

回答很長，我就不貼完整內容了，看起來給了對的 prompt，LLM 就能給出理想的答案了。所以 RAG 的一連串動作，就是藉由 embedding 把整個知識庫的內容，濃縮成上面這段 prompt，最後交給 LLM 推論給出最後的回應 ( answer )，你跟這樣調教過的 GPTs 對談，就會有他好像懂很多東西的錯覺了。要擴大 GPTs 的知識範圍，RAG 是個很有效率的做法，不但能自訂知識庫，內容也能即時更新，更重要的是你不需要重新訓練語言模型。這對 LLM 的大規模使用有很大的幫助。





# 3. 技術選擇

原本，我都做好心理準備了。上面的 RAG 操作，搞懂後拆解出來的步驟都不難，都打算自己用 Semantic Kernel 動手刻一個簡單的 API 來用，結果在查 SK 有沒有實做單機版的 Semantic Memory 時，意外找到了另一個 Microsoft 開源的專案: Kernel Memory.

看了一下，根本就是我原本想要自己動手做的東西啊啊啊啊，這是套基於 Semantic Kernel 開發出來的獨立服務版本，提供 http api, 支援文件的 CRUD, 外加 RAG 檢索能力。提供的部署方式也很完整，可以獨立部署 (service mode, 透過 http 存取)，也可以內嵌在你的 .net application 內 (serverless mode, 就像  SQLite 那樣)。不論你用哪種部署方式，他都提供了 SK 的 Memory Plugins, 你可以無縫的跟你的 SK project 整合在一起使用。雖然只有 0.29 版，不過對於我只是要 POC 的情境來說很夠用了。


不過，實做上好多元件跟技術需要解決啊，要站在巨人的肩膀上，其實有很多方便的選擇:

1. PaaS 的選擇
Azure Search
Open AI

1. SDK 
SK
Lang Chain

不過一個太貴，會被綁定，自己刻又太麻煩，對於不熟 AI 的門外漢來說 (就是我)，每個步驟的技術門檻都很高。本來我打算自己用 SK + MSSQL 做完這整個案例再來寫文章的，後來放棄...，因為，我找到了一個好東西: Kernel Memory.


先說明一下，SK 的框架下，怎麼處理這種 "長期記憶" 問題，就是 Memory (或是 Semantic Memory)






然而，我開始在想，Memory 背後那一連串處理，其實大家都差不多，LLM App 跟 Vector DB 之間的空白，應該大家需要的都差不多，應該有現成 open source project / service 存在的空間吧? Microsoft 果然沒讓人失望，我找到了這個 project: Kernel Memory



API: KernelMemory (service), open api spec ( docker / service + pipeline / serverless )
Query: SimpleVectorDB (database), IMemoryDatabase ( loop / ... )
Storage: SimpleFileStorage (storage), IMemoryStorage ( disk / memory )




# 4. 打造部落格檢索服務

## 4-1, 建立資料庫


## 4-2, 建立查詢服務


## 4-3, 建立 GPTs



# 5. 總結



