# 微服務基礎建設: 斷路器 #1, 服務負載的控制

## 摘要提示
- 流量管控基礎: 先量化「服務量」與限制策略，才能有效啟用斷路器避免雪崩效應
- 架構師視角: 要能在現成解法與自研之間做選擇與整合，掌握 Dev 與 Ops 的協同
- 「服務量」定義: 核心是單位時間的處理量，需明確規則與可實作的度量方法
- Counter 限流: 簡單易懂但有 time window 邊界問題，波動大不穩定
- 平滑統計: 以滑動視窗改善統計邊界，穩定度提升但仍是改良式的 counter
- Leaky Bucket: 以有界 queue 平滑輸出速率，能吸收尖峰但引入等待時間
- Token Bucket: 預先累積處理令牌，允許有界突發，成功率高且無等待
- 指標觀測: 用成功/拒絕/執行數與平均等待時間觀察限流策略表現
- 自動化治理: 以限流數據觸發斷路、擴縮容與資源調度，提前於故障發生前介入
- QoS/SLA 應用: 搭配服務發現與分組標籤，為不同客群設定差異化限流與服務等級

## 全文重點
本文聚焦於微服務中的「服務量（流量）管控」作為斷路器策略的基礎。作者指出，許多斷路器文章多在故障已發生時才啟用保護；而正確做法應是先量化與掌握服務量，於臨界前啟動保護與調度，避免擴散成系統性雪崩。服務量的本質是「單位時間內的處理量」，但如何定義與實作會影響控制精度、效能與可擴展性。單靠現成 API Gateway 的速率限制未必能滿足複雜業務情境，因此架構師與開發者須具備抽象建模與自行落地的能力。

文章以一個抽象類別 ThrottleBase 出題，要求實作 ProcessRequest，並提供測試主程式用多執行緒製造穩定、尖峰與離峰流量，按秒輸出統計（總請求、成功、拒絕、已執行、平均等待時間），借由圖表比對各種演算法成效。首先示範 DummyThrottle（不控流），作為基線。接著依序比較四種策略：

1) CounterThrottle：在固定 time window 內累加計數並重置。優點是簡單；缺點是窗口邊界造成瞬時放量（例如跨兩個窗口瞬間釋放雙倍額度），導致成功與執行曲線劇烈波動，難以保護後端或預測容量。縮短窗口可稍微平滑，但仍波動大。

1.5) StatisticEngineThrottle（滑動視窗統計）：以連續統計（滑動窗口）替代分段計數，改善邊界突波問題，使曲線較平滑。雖比單純 counter 更精準，但本質仍是統計限流，對尖峰吸收與後端保護能力有限，屬改良非根治。

2) Leaky Bucket：將請求視為倒入桶子的水，後端以固定速率「漏出」。桶子容量有限（等於可吸收的突發量），能平滑輸出、保護後端；尖峰期間請求可先入桶排隊，只有桶滿才拒絕。優點是執行速率幾近水平線（穩定），便於保護後端並提供可解釋的最大等待時間（time window 對應 upper bound）；缺點是引入等待，若非同步做得不好，可能佔用前端連線。此模式亦有助於 SRE 依拒絕率與等待時間決策擴容（估算 worker 數與吞吐）。

3) Token Bucket：反向機制，以固定速率產生令牌放入桶中，有令牌的請求立即執行；桶子容量即可允許的突發量。優點是在可控突發範圍內做到「成功且即時」處理，不引入等待；尖峰後需等待令牌回填，之後才可再吸收新一波尖峰。與 Leaky Bucket 相比，Token Bucket 更能提高資源利用與成功率，但對後端的即時負載壓力較高，需確保回填速率與桶深與系統彈性匹配。

作者強調，限流不僅是 infra 的責任，開發者需理解其本質與取捨，才能在需求變動下靈活整合。例如：以服務發現標籤建立多組服務池，為 VIP 與一般用戶配置不同限流策略與 SLA；以瞬時拒絕率、平均等待時間與錯誤率作為觸發條件，與斷路器、健康檢查、組態管理、擴縮容策略協同；在 client 也可透過程式細緻控制調用與保護。最終目標是以數據驅動的自動化治理，於「將發未發」之際先行預防，達到穩定性與體驗兼顧。

## 段落重點
### 前言與問題意識
作者從微服務治理難題切入，指出斷路器的有效性仰賴對「服務量」的精準掌握。僅靠 API Gateway 的 rate limit 難以滿足多元業務（例如按品類、時段、車流等限制）。架構師需能跨 Dev 與 Ops 在現成方案與自研之間抉擇，具備抽象思維與整合能力。文中以面試題形式要求在程式碼層面實作限流，並透過可視化數據驗證，期望讀者養成能在缺少現成解法時自行打造輪子的能力。

### 微服務の斷路器
說明雪崩效應：某服務超載後，呼叫端重試導致負載擴散，整體系統崩潰。解法是服務量管控與斷路器的結合：當錯誤率上升或負載逼近/超過安全線，暫停新請求，保護整體。難點不在選擇框架，而是能否精準量測各服務的負載數據，並整合自動化調節（含擴縮容）。作者強調本文目標是打穩限流基礎，以便日後正確運用斷路器，而非重造斷路器。

### 如何定義「服務量」？
「服務量」即單位時間處理量；真正難題在於可操作的定義與實作。例如以每秒、每分或滑動時間窗統計，會影響準確性與性能。需先回答兩個核心：如何定義並計算流量（可否用於分散式架構）；超過限制後如何處理（拒絕、排隊、延遲、降級）。一旦主流程明確（是否受理、如何處理），剩下的挑戰在實作與效能。

### 題目: 抽象化的 ThrottleBase 類別
提供抽象類別 ThrottleBase，出題要求實作 ProcessRequest(amount, exec)。若受理回 true 並安排執行 delegate；若不受理回 false。作者提供測試程式：製造穩定流（約 600 RPS）、定期尖峰（每 17 秒持續 2 秒高峰）、與離峰（每 21 秒有 3 秒無流量），並每秒輸出統計（總數、成功、拒絕、已執行、平均等待時間），用 Excel 圖表檢視策略行為。DummyThrottle 作為無控流基線。此測試框架可作為團隊 POC、教育訓練與面試題。

### 解法 1, CounterThrottle
以固定 time window 計數，超過上限則拒絕。優點是直觀、簡單；缺點是窗口「邊界效應」嚴重：在窗口切換瞬間可能釋放雙倍額度，導致瞬時尖峰，實際執行曲線高低起伏、不可預測，後端易遭壓垮。縮小窗口可稍緩，但仍存在波動與不穩。在保護後端與容量預測上不足，僅適合基本用量限制或計費。

### 解法 1.5, StatisticEngineThrottle
用滑動視窗（連續統計）取代分段計數，避免窗口交界處突波，使曲線更平滑、數據更精準。測試顯示穩定度優於純 counter，但仍屬統計限流思路，對吸收尖峰與保障後端穩態的能力有限，是「改良而非根治」，仍有進一步優化空間。

### 解法 2, Leaky Bucket
模型：請求如水注入桶，桶底以固定速率漏出（對應後端消化速率）；桶容量即能吸收的突發量。行為特性：執行速率近似水平線（穩定）、有界吸收尖峰（桶滿前不拒絕）、但引入等待（平均等待時間受 time window 上限約束）。優點：可精準承諾最大等待時間、有效保護後端，且指標有助於 SRE 擴容決策（以拒絕率與等待時間判斷調整 worker 或速率）。注意：若非同步處理不足，等待會佔住連線，time window 過大恐致前端連線耗盡。

### 解法 3, Token Bucket
模型：以固定速率往桶中投放令牌；請求需消耗令牌方可立即執行。桶深為可允許的突發量。行為特性：在有界突發範圍內達到「成功且無等待」，提升資源利用與成功率；尖峰後需等待令牌回補才可再吸收新尖峰。相較 Leaky Bucket，Token Bucket 對後端即時負載壓力較高，但整體吞吐與即時性更佳，適用於後端具備一定彈性且偏好立即處理的情境。

### 總結
限流是斷路器與容量治理的前提。僅在故障時啟動斷路是「事後補救」，應以量測與限流在臨界前介入，配合自動化斷路、資源調度與擴縮容。從 Counter 到滑動統計，再到 Leaky/Token Bucket，控制精度與穩定性依序提升。設計時需結合服務發現、組態管理、健康檢查與 QoS：以服務標籤分組（如 VIP 與一般），套用差異化限流與 SLA；以成功/拒絕/等待等指標驅動治理決策。真正的價值在於掌握概念與整合能力，即便沒有現成方案，也能以少量代碼在 client/service 端靈活落地，實現穩定、可預測、可度量的微服務系統。

## 資訊整理

### 知識架構圖
1. 前置知識：學習本主題前需要掌握什麼？
- 基礎網路與併發概念（RPS、延遲、重試、背壓）
- 微服務基礎（服務發現、健康檢查、組態管理）
- DevOps 能力（監控與告警、Auto Scaling、CI/CD）
- 佇列與非同步處理模型（Producer/Consumer、Queue/Worker）
- 斷路器基本原理（熔斷、半開、恢復）

2. 核心概念：本文的 3-5 個核心概念及其關係
- 服務量/流量控制（Throttle/Rate Limiting）：以「單位時間處理量」為核心，穩定負載、避免雪崩
- 量化與度量：定義與量測窗口（Time Window）、平均值與尖峰、分段與滑動統計
- 限流演算法：Counter、滑動統計引擎、Leaky Bucket、Token Bucket（及其優缺點與適用）
- 與斷路器的關係：先掌控服務量→可預測極限→於趨近失效時啟動熔斷保護
- 與基礎設施整合：API Gateway/NGINX 限流、監控、Auto Scaling、QoS/SLA、VIP 分流

3. 技術依賴：相關技術之間的依賴關係
- 限流策略依賴於可信的度量與統計（計數器/滑動窗口）
- Leaky/Token Bucket 依賴隊列/令牌補充器與時間驅動器（Timer）
- 斷路器依賴錯誤率/延遲/吞吐的監測訊號
- Auto Scaling 依賴限流與監控指標（Fail rate、等待時間、執行速率）
- 服務發現/標籤化依賴於配置中心與健康檢查（用於 QoS 分群與路由）

4. 應用場景：適用於哪些實際場景？
- API 呼叫配額與速率限制（如每秒 60 次）
- 業務規則型限流（如每小時最多出貨 1000 件）
- 尖峰吸收與平滑（活動/促銷、突發流量）
- 系統保護與背壓（避免重試風暴與雪崩）
- QoS/SLA 分級（VIP 與一般客戶差異化服務）
- 匝道管制類比的流量治理（流量稀釋、等待上限承諾）

### 學習路徑建議
1. 入門者路徑：零基礎如何開始？
- 了解 RPS、延遲、錯誤率、重試對系統的影響
- 讀懂基本限流策略（固定窗口、滑動窗口）與簡單計數器實作
- 使用現成工具做限流（如 NGINX/Kong Rate Limiting）並觀察監控圖表
- 練習以 CSV/圖表化觀察 Success/Fail/Executed/AvgWait 的變化

2. 進階者路徑：已有基礎如何深化？
- 實作滑動統計（In-memory/分散式）與時間窗口選型
- 實作並比較 Leaky Bucket 與 Token Bucket，理解優缺點
- 將限流訊號與斷路器結合（臨界前預防熔斷、半開測試）
- 將限流指標接入 Auto Scaling 與告警（根據 Fail/等待時間擴縮容）

3. 實戰路徑：如何應用到實際專案？
- 為不同服務/客群貼標籤與分組（Service Discovery + 標籤路由）
- 按服務等級配置差異化限流與 SLA（VIP 專屬實例與門檻）
- 建立端到端觀測（執行速率、拒絕率、等待時間、尖峰吸收度）
- 制定重試與背壓策略（最大等待時間、同步/非同步邊界、連線占用風險）
- 用流量治理數據驅動容量規劃與成本最佳化

### 關鍵要點清單
- 服務量定義與量化：將「單位時間處理量」可測可控，才能談限流與保護（優先級: 高）
- 固定窗口計數器（Counter）：簡單易實作，但窗口邊界造成尖峰不穩（優先級: 中）
- 滑動窗口統計：平滑跨窗口尖峰，較能反映真實吞吐（優先級: 高）
- Leaky Bucket（漏桶）：以固定出水速率+有限桶容量，平滑執行速率但引入等待（優先級: 高）
- Token Bucket（令牌桶）：固定補充令牌，允許短暫突發立即執行，利用率較高（優先級: 高）
- 尖峰吸收與等待上限：桶容量與窗口共同決定可吸收突發與最大等待時間（優先級: 高）
- 指標體系：Total/Success/Fail/Executed/AvgWait 五指標觀測治理效果（優先級: 高）
- 重試風暴與雪崩：未限流與未熔斷會放大失效，需前置治理與熔斷配合（優先級: 高）
- 同步 vs 非同步：漏桶引入等待可能佔用連線，需以非同步/隊列化消解（優先級: 中）
- 與斷路器關係：先控量再熔斷，臨界前預防，臨界後隔離故障面（優先級: 高）
- 與 Auto Scaling：以拒絕率與等待時間觸發擴容，將限流轉為容量決策訊號（優先級: 中）
- QoS/SLA 與標籤路由：按客群/服務標籤配置差異化限流與路由（優先級: 中）
- 分散式實作考量：統計一致性、令牌發放/桶容量在多實例間的協調（優先級: 中）
- 工具與自研取捨：能思辨與原型化，現成方案不足時具備自研能力（優先級: 中）
- 度量驅動迭代：用圖表化與實驗（CSV/Chart/PoC）比較策略並持續調參（優先級: 中）