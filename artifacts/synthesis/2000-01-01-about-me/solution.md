以下內容依據文章所列之演講主題與專長領域，抽取與整理可落地實作的「問題解決案例」。原文多為主題與連結清單，未含專案細節與量測數據；因此各案例之實作步驟與程式碼為通用範式與教學設計，實測數據欄位提供建議指標與評估方式，供練習與專案演練使用。

## Case #1: API First：從理想的 API Spec 到自動化交付

### Problem Statement（問題陳述）
- 業務場景：大型 B2B2C 團隊中，前後端與第三方商務夥伴密集整合，常因 API 契約溝通不一致導致開發重工、上線延期與整合測試卡關。需要一個可協作、可驗證、可自動化的 API 設計與交付流程。
- 技術挑戰：API 契約易漂移、文件與實作不一致、Mock/測試資源難維護、相依團隊排程互卡。
- 影響範圍：跨團隊協作效率、缺陷率、交付節奏、上線風險。
- 複雜度評級：中

### Root Cause Analysis（根因分析）
- 直接原因：
  1. 缺少規範化契約（OpenAPI/JSON Schema）與 Lint 檢查，導致定義不一致。
  2. 文件與程式碼分離，缺乏契約測試與生成工具鏈。
  3. 整合測試後置，Mock 與測試資料不可追蹤。
- 深層原因：
  - 架構層面：未將 API 作為產品管理，缺少版本策略與門禁機制。
  - 技術層面：未標準化工具（Spec、Mock、Codegen、契約測試）。
  - 流程層面：契約先行/審核/驗收節點缺失，CI/CD 未導入契約品質檢核。

### Solution Design（解決方案設計）
- 解決策略：採用 API First。以 OpenAPI 3.1 為單一來源，導入規則檢查、合約審核、Mock 服務、Server/Client Codegen、Consumer-Driven Contract 測試與 API Gateway 發佈。讓契約驗收前移，並在 CI/CD 中落實自動化品質門禁。

### 實施步驟
1. 建立 API 契約庫與規則
- 實作細節：OpenAPI 3.1 + Spectral Lint 規則 + PR 審核流程
- 所需資源：Stoplight/Spectral、Git 平台
- 預估時間：1-2 天
2. 導入 Mock 與 Codegen
- 實作細節：Prism/MockServer；NSwag/AutoRest 生成 Server/Client
- 所需資源：Prism、NSwag、CI Runner
- 預估時間：1-2 天
3. 契約測試與發佈
- 實作細節：Pact（CDC）+ 合約驗收 Gate + API Gateway 發佈
- 所需資源：Pact Broker、Kong/Apim、CI/CD
- 預估時間：3-5 天

### 關鍵程式碼/設定
```yaml
# openapi.yaml (片段)
openapi: 3.1.0
info: { title: Orders API, version: 1.0.0 }
paths:
  /orders:
    post:
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateOrder'
      responses:
        '201': { description: Created }
components:
  schemas:
    CreateOrder:
      type: object
      required: [customerId, items]
      properties:
        customerId: { type: string, format: uuid }
        items:
          type: array
          items:
            type: object
            required: [sku, qty]
            properties:
              sku: { type: string }
              qty: { type: integer, minimum: 1 }

# .spectral.yaml 片段（API Lint 規則）
extends: ["spectral:recommended"]
rules:
  operation-description: warn
  info-contact: error

# GitHub Actions 片段：Lint + Codegen
name: api-first
on: [pull_request]
jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Spectral Lint
        run: npx @stoplight/spectral-cli lint openapi.yaml
      - name: NSwag Codegen
        run: npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g csharp -o gen/csharp
```

- 實際案例：出處「API First, 從理想的 API Spec 設計開始（.NET Conf 2022）」與「DevOpsDays Taipei 2022」。原文未含專案細節，此為通用實作。
- 實作環境：OpenAPI 3.1、Spectral、Prism、OpenAPI Generator/NSwag、Pact、GitHub Actions/Azure DevOps、Kong/Azure API Management
- 實測數據：
  - 改善前：跨組整合常因契約不一致導致重工與延誤
  - 改善後：以契約為門禁與自動化生成，整合缺陷顯著下降
  - 改善幅度：依團隊而異；建議追蹤「契約違反 Bug 數」「整合周期」

Learning Points（學習要點）
- 核心知識點：
  - Spec-First 與契約驅動開發（CDC）
  - API Lint/Mock/Codegen 工具鏈
  - 契約門禁在 CI/CD 的實作
- 技能要求：
  - 必備技能：OpenAPI、Git、CI 基礎
  - 進階技能：Pact、Gateway 發佈策略、版本治理
- 延伸思考：
  - 如何設計 Breaking/Non-breaking 改動策略？
  - 多產品線共享契約的治理模型？
  - 與安全掃描/流量控管的整合？
- Practice Exercise：
  - 基礎：為三支 API 寫 OpenAPI 3.1 與 Spectral 規則（30 分）
  - 進階：導入 Prism Mock 與前端串接驗證（2 小時）
  - 專案：CDC + Codegen + Gateway 發佈完整流（8 小時）
- Assessment Criteria：
  - 功能完整性（40%）：契約→Mock→Codegen→測試→發佈全通
  - 程式碼品質（30%）：規則化、版本化、審核清晰
  - 效能優化（20%）：生成與測試時間、快取利用
  - 創新性（10%）：自製 Lint 規則、契約分析報表

---

## Case #2: 從 API First 到 AI First：在現有服務中導入 LLM 能力

### Problem Statement（問題陳述）
- 業務場景：既有 API 生態成熟（訂單、客服、商品搜尋），欲快速導入 LLM 能力（對話客服、內容生成、分類）提升體驗與效率，同時必須控管成本、延遲與風險。
- 技術挑戰：提示管理與版本控管、可觀測性、毒性/洩漏風險、非決定性行為、冷啟動延遲與降級策略。
- 影響範圍：客服回覆品質、SLA/SLO、成本結構、品牌與合規風險。
- 複雜度評級：高

### Root Cause Analysis（根因分析）
- 直接原因：
  1. 缺少 Prompt/Tooling 的版本與門禁機制。
  2. 無系統化的評測與觀測（質量、成本、延遲）。
  3. 缺乏輸出約束與安全檢查（PII、越權）。
- 深層原因：
  - 架構層面：缺 LLMOps 流水線與策略（降級、熔斷、回退）。
  - 技術層面：未導入結構化輸出、Guardrails、功能工具化。
  - 流程層面：Prompt 與模型更新缺少審核/灰度發布。

### Solution Design（解決方案設計）
- 解決策略：建立 AI First 增量能力層，包括 Prompt/Tool Registry、Guardrails（毒性/PII/拒答）、結構化輸出、Tool-Use、可觀測性與成本監控。導入離線評測+線上 AB/灰度與回退策略，確保穩定、安全、可控成本。

### 實施步驟
1. Prompt/Tool 版本治理
- 實作細節：Git 版本、變更審核、配置管理
- 所需資源：Git、Feature Flags
- 預估時間：1-2 天
2. Guardrails 與結構化輸出
- 實作細節：Moderation、PII redaction、JSON Schema/函式呼叫
- 所需資源：OpenAI/Azure OpenAI、Validator
- 預估時間：2-3 天
3. 可觀測性與降級
- 實作細節：指標（quality/cost/latency）、熔斷/快取/回退
- 所需資源：APM/Logs/Tracing、Redis
- 預估時間：3-5 天

### 關鍵程式碼/設定
```csharp
// C#：呼叫 LLM 並強制 JSON Schema 結構化輸出與基本 Guardrails
var client = new OpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));
var schema = BinaryData.FromString("""
{ "type":"object","properties":{"intent":{"type":"string"},"answer":{"type":"string"}},"required":["intent","answer"] }
""");

var options = new ChatCompletionsOptions()
{
    Messages =
    {
        new ChatRequestSystemMessage("You are a safe, helpful assistant. Redact PII."),
        new ChatRequestUserMessage("請幫我查詢訂單 123-45-678 狀態")
    },
    ResponseFormat = ChatCompletionsResponseFormat.CreateJsonSchemaFormat("resp", schema, allowAdditionalProperties: false)
};

var resp = await client.GetChatCompletionsAsync(deployment, options);
var json = JsonDocument.Parse(resp.Value.Choices[0].Message.Content[0].Text).RootElement;

// 基本安全檢查與降級
if (ContainsToxic(json) || !IsValid(json))
{
    // 回退：走非 AI API 或回傳標準 FAQ
    return await FallbackAnswerAsync();
}
return json;
```

- 實際案例：出處「DevOpsDays Taipei 2024 - 從 API First 到 AI First」。原文未含實作細節，此為通用模式。
- 實作環境：Azure OpenAI/OpenAI、.NET 8、Feature Flags、APM（AppInsights/Datadog）
- 實測數據：
  - 改善前：缺乏品質與成本可觀測性；風險無法量化
  - 改善後：具品質/成本/延遲指標與灰度、回退機制
  - 改善幅度：以目標指標（準確率、每請求成本、P95 延遲）衡量

Learning Points
- 核心知識點：LLMOps、Prompt/Tool 版本治理、Guardrails 與結構化輸出
- 技能要求：.NET 呼叫 LLM、JSON Schema、AB/灰度策略
- 延伸思考：如何定義離線/線上評測？成本上限與配額控管？
- Practice：建一條對話客服 API，含 JSON Schema 輸出與回退（8 小時）
- Assessment：功能覆蓋、風險控制、可觀測性完整性、成本控制策略

---

## Case #3: 模型降級驗證：確保服務契約的向後相容

### Problem Statement（問題陳述）
- 業務場景：服務頻繁演進，對接多個消費者（APP/後台/合作商），需要確保新版本釋出不破壞舊客戶端，並能安全降級或回滾。
- 技術挑戰：Breaking change 隱性出現；多客戶端版本難驗證；回滾流程缺少自動化驗證。
- 影響範圍：故障率、回滾時間、客戶端崩潰、客訴。
- 複雜度評級：中

### Root Cause Analysis
- 直接原因：
  1. 缺少 Consumer-Driven Contract（CDC）測試。
  2. Schema 演進缺乏策略（新增欄位、欄位移除）。
  3. 回滾前無自動化相容性驗證。
- 深層原因：
  - 架構：未版本化契約與相容規則。
  - 技術：缺 Pact/Schemacheck、影子流量驗證。
  - 流程：未設計回滾路徑與門禁。

### Solution Design
- 解決策略：導入 CDC（Pact）與 JSON Schema/Protobuf 演進規範；在 CI/CD 加入相容性門禁與影子流量驗證。提供回滾模板與自動化降級驗證腳本。

### 實施步驟
1. CDC 契約與 Broker
- 實作：Pact（consumer/provider）、Broker 發佈與驗證
- 資源：PactNet、Pact Broker
- 時間：1-2 天
2. Schema 規範與 Lint
- 實作：非破壞性規則、欄位廢止流程
- 資源：JSON Schema、Spectral
- 時間：1 天
3. 降級/回滾驗證
- 實作：影子流量 Replay + 自動比較
- 資源：Traffic Mirror、Diff 工具
- 時間：2-3 天

### 關鍵程式碼/設定
```csharp
// PactNet 消費者測試（片段）
var pact = Pact.V3("MobileApp", "OrdersAPI", new PactConfig()).WithHttpInteractions();
pact.UponReceiving("create order")
    .WithRequest(HttpMethod.Post, "/orders")
    .WithJsonBody(new { customerId = Match.Regex(Guid.NewGuid(), ".+"), items = new[] { new { sku = "ABC", qty = 1 } } })
    .WillRespond().WithStatus(HttpStatusCode.Created);

await pact.VerifyAsync(async ctx =>
{
    var client = new HttpClient { BaseAddress = new Uri(ctx.MockServerUri) };
    var res = await client.PostAsJsonAsync("/orders", new { customerId = Guid.NewGuid(), items = new[] { new { sku = "ABC", qty = 1 } } });
    Assert.Equal(HttpStatusCode.Created, res.StatusCode);
});
```

- 實際案例：出處「模型的降級驗證技巧（.NET Conf 2023）」主題。此為通用實踐。
- 實作環境：.NET 8、PactNet、Pact Broker、CI/CD
- 實測數據：
  - 改善前：回滾風險高、破壞性變更易漏
  - 改善後：釋出前自動發現破壞性變更，回滾時間縮短
  - 建議追蹤：回滾平均時間、破壞性缺陷數

Learning Points
- 核心：CDC 測試、Schema 演進規範、影子流量驗證
- 技能：PactNet、JSON Schema 規範化、流量重放
- 延伸：藉 Feature Flag 做雙寫/雙讀兼容
- Practice：為兩個 API 端點建立 CDC 與回滾驗證（2 小時）
- Assessment：契約覆蓋率、破壞性變更攔截率、回滾腳本可重用性

---

## Case #4: 服務模型的持續交付：Schema 演進與流水線落地

### Problem Statement
- 業務場景：多服務共享領域模型（DTO/Events），多團隊並行開發，版本與相依管理複雜，釋出容易相互影響。
- 技術挑戰：Schema 變更擴散、相依套件更新難同步、環境漂移。
- 影響範圍：交付頻率、整合缺陷、回滾成本。
- 複雜度：中

### Root Cause Analysis
- 直接原因：
  1. 模型與程式碼耦合、無單獨版本治理。
  2. 無語義化版本與自動散佈。
  3. 缺跨專案相容性測試。
- 深層：
  - 架構：缺模型中心（Schema Registry/內部 NuGet）
  - 技術：缺相依圖與自動回溯建置
  - 流程：審核與發佈未標準化

### Solution Design
- 解決策略：建立「模型倉庫」與語義化版本；自動生成契約/DTO 套件（NuGet/NPM），在 CI 中進行相容測試，於 CD 觸發相依服務建置與灰度釋出。

### 實施步驟
1. 模型倉庫與版本規範
- 實作：SemVer、Changelog、發佈工作流
- 資源：Git、Release Drafter
- 時間：1 天
2. 相容性測試與套件生成
- 實作：SchemaCheck、NuGet 打包與署名
- 資源：JSON Schema、dotnet pack、Private NuGet
- 時間：2 天
3. 相依服務觸發建置
- 實作：相依圖 + 觸發/矩陣建置
- 資源：CI/CD、Graph 工具
- 時間：2-3 天

### 關鍵程式碼/設定
```yaml
# GitHub Actions: 套件發佈與相依觸發
name: model-cd
on:
  push:
    tags: ["v*.*.*"]
jobs:
  package:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: dotnet pack src/Contracts/ -c Release -o out
      - name: Publish to internal NuGet
        run: dotnet nuget push out/*.nupkg -s ${{ secrets.NUGET_FEED }} -k ${{ secrets.NUGET_KEY }}
  trigger-dependents:
    needs: package
    runs-on: ubuntu-latest
    steps:
      - name: Call downstream pipelines
        run: |
          gh workflow run service-a.yml -f contracts_version=${GITHUB_REF_NAME}
          gh workflow run service-b.yml -f contracts_version=${GITHUB_REF_NAME}
```

- 實際案例：出處「服務模型的持續交付（DevOpsDays 2023）」主題。
- 實作環境：.NET、私有 NuGet、CI/CD
- 實測數據：建議追蹤「契約更新到生產時間」「相依建置成功率」「破壞性變更攔截數」

Learning Points：模型中心化、SemVer、相依觸發
Practice：建立 Contracts 套件倉庫→NuGet→相依服務更新（8 小時）
Assessment：釋出自動化程度、相容性驗證、回滾策略

---

## Case #5: 大型團隊落實 CI/CD：長流水線與不穩定測試的治理

### Problem Statement
- 業務場景：數十個服務、百人團隊，流水線冗長、測試不穩定、環境資源緊繃，導致交付節奏受阻。
- 技術挑戰：快取/增量建置不足、測試資料管理困難、排程互卡。
- 影響範圍：Lead Time、變更失敗率、部署頻率。
- 複雜度：高

### Root Cause Analysis
- 直接原因：
  1. 缺少測試分片與併行策略。
  2. 環境不可拋棄，測試資料污染。
  3. 未善用快取與增量建置。
- 深層：
  - 架構：無可快速配置的臨時環境
  - 技術：測試與資料生成工具缺失
  - 流程：排程/優先級策略不明

### Solution Design
- 解決策略：將測試併行化與分片化、導入可拋棄環境（Preview/PR Envs）、快取/增量建置、資料種子自動化與金絲雀/藍綠部署，提升吞吐與穩定。

### 實施步驟
1. 快取與增量建置
- 實作：依專案 Hash 決定建置；NuGet/npm 快取
- 資源：CI 快取機制
- 時間：1 天
2. 測試分片與併行
- 實作：根據歷史耗時切片；重試 flakies
- 資源：Test Splitting、重試插件
- 時間：2 天
3. 臨時環境與資料
- 實作：Infra as Code + Seed/Fixture；PR 自動建立
- 資源：K8s、Terraform、DB Snapshot
- 時間：3-5 天

### 關鍵程式碼/設定
```yaml
# Azure DevOps: 測試分片與快取
jobs:
- job: build
  pool: { vmImage: 'ubuntu-latest' }
  steps:
    - task: Cache@2
      inputs: { key: 'nuget | "$(Agent.OS)" | **/packages.lock.json', path: ~/.nuget/packages }
    - script: dotnet restore && dotnet build -c Release

- job: tests
  dependsOn: build
  strategy:
    parallel: 4
  steps:
    - script: dotnet test --filter "FullyQualifiedName~Tests" --logger trx -- RunConfiguration.ResultsDirectory=$(Agent.TempDirectory)/test$(System.JobPositionInPhase)
```

- 實際案例：「大型團隊落實 CI/CD 的挑戰（DevOpsDays 2021）」主題。
- 實作環境：Azure DevOps/GitHub Actions、K8s、Terraform
- 實測數據：追蹤 DORA（Lead Time/Deploy Freq/MTTR/Change Fail Rate）與 P95 Pipeline Duration

Learning Points：併行與分片、快取、環境即程式
Practice：將單 repo 測試時間減半的優化（2 小時）
Assessment：DORA 指標改善、Pipeline 穩定性、重試策略合理性

---

## Case #6: 非同步系統的 SLO 設計：可觀測與可承諾

### Problem Statement
- 業務場景：使用訊息佇列處理訂單、庫存、通知等工作，需對「延遲、吞吐、失敗率」制定 SLO，對外承諾並內部治理。
- 技術挑戰：端到端延遲測量困難、佇列積壓與重試風暴、死信監控不足。
- 影響範圍：訂單延誤、SLA 違約、營運風險。
- 複雜度：中

### Root Cause Analysis
- 直接原因：
  1. 缺端到端追蹤與 Lag 指標。
  2. 缺死信/重試治理與告警。
  3. 缺容量/高峰壓力測試。
- 深層：
  - 架構：缺整體觀測與回壓策略
  - 技術：無標準化指標（RED/USE）
  - 流程：SLO 未與容量計畫綁定

### Solution Design
- 解決策略：定義 SLI（處理延遲、Lag、DLQ 比率、吞吐），配 SLO 與錯誤預算；實作可觀測性（metrics/log/trace）、回壓與 DLQ 治理，持續壓測與容量規劃。

### 實施步驟
1. 指標與告警
- 實作：延遲直方圖、Lag、DLQ、重試計數
- 資源：prometheus-net/Grafana
- 時間：1-2 天
2. 回壓與重試策略
- 實作：指數退避、最大並行、速率限制
- 資源：佇列 SDK、Polly
- 時間：1-2 天
3. 壓測與預算
- 實作：壓測曲線、錯誤預算看板
- 資源：k6、JMeter
- 時間：2 天

### 關鍵程式碼/設定
```csharp
// .NET 指標：處理延遲與 DLQ 計數
static readonly Histogram latency = Metrics.CreateHistogram("worker_latency_seconds", "Process latency");
static readonly Counter dlqCounter = Metrics.CreateCounter("worker_dlq_total", "DLQ count");

var sw = Stopwatch.StartNew();
try
{
    await ProcessAsync(msg);
    latency.Observe(sw.Elapsed.TotalSeconds);
}
catch (Exception)
{
    dlqCounter.Inc();
    await SendToDLQ(msg);
}
```

- 實際案例：主題「非同步系統的 SLO 設計」「服務水準(SLO)保證」。
- 實作環境：.NET、Prometheus、Grafana、k6
- 實測數據：追蹤 P95/P99 延遲、Lag、DLQ 比率、錯誤預算消耗

Learning Points：SLI/SLO/錯誤預算、RED/USE
Practice：設計並儀表化三個 SLI + 告警（2 小時）
Assessment：指標正確性、告警噪音控制、行動化 Runbook

---

## Case #7: 事件驅動：用非同步平滑高峰與削減耦合

### Problem Statement
- 業務場景：大促/行銷活動導致下單高峰，單體同步流程資源耗盡、超時、失敗率飆升。
- 技術挑戰：同步耦合、長交易、低彈性。
- 影響範圍：轉換率、營收、品牌。
- 複雜度：中

### Root Cause Analysis
- 直接原因：
  1. 同步 RPC 串聯太長。
  2. 缺緩衝與回壓。
  3. 交易一致性過度嚴苛。
- 深層：
  - 架構：缺事件驅動解耦
  - 技術：無 idempotency/重試/死信
  - 流程：容量規劃與高峰預案不足

### Solution Design
- 解決策略：引入 Message Broker（Kafka/ASB/RabbitMQ），以 Outbox + Idempotency + Saga 管理最終一致性，縮短同步路徑，將非必要操作移至異步管道。

### 實施步驟
1. Outbox 與訊息發佈
- 實作：本地交易寫 Outbox，背景發佈
- 資源：EFCore、Worker
- 時間：2 天
2. 消費者與冪等
- 實作：去重鍵、重試退避、死信
- 資源：Broker SDK、Cache
- 時間：2 天
3. Saga/補償
- 實作：跨服務補償與狀態機
- 資源：Workflow（Dapr/Temporal）
- 時間：3-5 天

### 關鍵程式碼/設定
```csharp
// Outbox 寫入（同 DB 交易）
using var tx = db.Database.BeginTransaction();
db.Orders.Add(order);
db.Outbox.Add(new OutboxMessage { Type="OrderCreated", Payload = JsonSerializer.Serialize(order) });
await db.SaveChangesAsync();
await tx.CommitAsync();
```

- 實際案例：主題「非同步系統」「微服務資料管理」系列。
- 實作環境：.NET、EF Core、Kafka/ASB
- 實測數據：追蹤同步路徑延遲下降、峰值吞吐提升、失敗率下降

Learning Points：Outbox/Idempotency/Saga
Practice：將下單流程拆分同步+異步（8 小時）
Assessment：資料一致性、失敗補償、峰值穩定性

---

## Case #8: 服務發現：微服務動態定位與健康檢查

### Problem Statement
- 業務場景：數十個服務動態擴縮與滾動更新，手寫端點導致失聯、錯誤路由與長恢復。
- 技術挑戰：服務定位、健康探測、熔斷/重試策略。
- 影響範圍：可用性、延遲、故障隔離。
- 複雜度：中

### Root Cause Analysis
- 直接原因：
  1. 靜態配置過多。
  2. 無註冊表與健康檢查。
  3. 客戶端無重試/熔斷。
- 深層：
  - 架構：缺服務網格/發現機制
  - 技術：不具觀測性
  - 流程：配置漂移未管控

### Solution Design
- 解決策略：導入 Consul（或 K8s DNS/Service Mesh），服務自註冊與健康檢查，客戶端以 SDK 做發現與重試熔斷，逐步替換靜態端點。

### 實施步驟
1. 註冊表與 Agent
- 實作：部署 Consul、Agent、ACL
- 資源：Consul、K8s
- 時間：1-2 天
2. 服務自註冊與健康檢查
- 實作：健康端點、TTL/HTTP 檢查
- 資源：Consul SDK
- 時間：1-2 天
3. 客戶端容錯
- 實作：重試、熔斷、逾時
- 資源：Polly
- 時間：1 天

### 關鍵程式碼/設定
```hcl
# Consul 服務註冊（片段）
service {
  name = "orders"
  port = 5000
  check { http = "http://localhost:5000/health"; interval = "10s" }
}
```

- 實際案例：「微服務基礎建設 - Service Discovery（DevOpsDays 2018）」。
- 實作環境：Consul/K8s、.NET、Polly
- 實測數據：服務失聯時間下降、故障自動恢復率提升

Learning Points：服務發現、健康檢查、客戶端容錯
Practice：將兩個服務接入註冊與發現（2 小時）
Assessment：故障切換、重試/熔斷正確性

---

## Case #9: Message Queue Based RPC：穩定且可控的遠端呼叫

### Problem Statement
- 業務場景：需跨區域/弱網環境呼叫服務，HTTP 超時與尖峰不穩，考慮以 MQ 模式做 RPC。
- 技術挑戰：關聯性、回覆通道、超時與冪等。
- 影響範圍：可靠性、延遲、成本。
- 複雜度：中

### Root Cause Analysis
- 直接原因：
  1. HTTP 同步請求受網路抖動影響。
  2. 缺重試與回退策略。
  3. 無端到端追蹤。
- 深層：
  - 架構：未利用 Broker 的緩衝與持久化
  - 技術：缺 Correlation/ReplyTo
  - 流程：超時與補償策略未定義

### Solution Design
- 解決策略：以 RabbitMQ/ASB 實作 RPC，使用 CorrelationId/ReplyTo、TTL/死信、重試退避，並提供保證至多一次或至少一次語義與補償。

### 實施步驟
1. 定義 RPC 通道
- 實作：請求佇列、回覆佇列
- 資源：RabbitMQ
- 時間：1 天
2. 實作 Correlation 與超時
- 實作：CorrelationId、Wait for reply、超時回退
- 資源：SDK/Polly
- 時間：1 天
3. 觀測與告警
- 實作：追蹤/指標
- 資源：OpenTelemetry
- 時間：1 天

### 關鍵程式碼/設定
```csharp
// RabbitMQ 發送 RPC（片段）
var corrId = Guid.NewGuid().ToString();
var props = channel.CreateBasicProperties();
props.CorrelationId = corrId;
props.ReplyTo = replyQueue;

channel.BasicPublish("", "rpc_queue", props, body);
var tcs = new TaskCompletionSource<byte[]>();
consumer.Received += (s, ea) =>
{
    if (ea.BasicProperties.CorrelationId == corrId)
        tcs.TrySetResult(ea.Body.ToArray());
};
var response = await tcs.Task.TimeoutAfter(TimeSpan.FromSeconds(3));
```

- 實際案例：主題「Message Queue Based RPC（.NET Conf 2018）」。
- 實作環境：RabbitMQ、.NET
- 實測數據：低網品質下成功率提升、HTTP 超時率下降

Learning Points：Correlation/ReplyTo、超時與補償
Practice：完成一對 RPC 服務與客戶端（2 小時）
Assessment：在弱網模擬下成功率與延遲分佈

---

## Case #10: 系統與資料庫重構：Strangler Fig 逐步遷移到微服務

### Problem Statement
- 業務場景：大型單體系統維護困難，需逐步拆解為微服務，避免一次性重寫風險。
- 技術挑戰：路由切分、資料一致性、舊新並存。
- 影響範圍：上線風險、交付節奏、成本。
- 複雜度：高

### Root Cause Analysis
- 直接原因：
  1. 模組界線不清，資料庫高度耦合。
  2. 無網關/路由策略。
  3. 無雙寫/同步策略。
- 深層：
  - 架構：未界定限界上下文
  - 技術：缺事件同步與讀寫拆分
  - 流程：缺遷移步驟與驗證

### Solution Design
- 解決策略：以網關路由漸進轉發，先拆讀後拆寫；資料層導入事件同步（CDC/Outbox），以雙寫/影子流量验证，迭代收斂。

### 實施步驟
1. BFF/網關與路由
- 實作：按路徑/功能路由新舊
- 資源：NGINX/Kong/YARP
- 時間：1-2 天
2. 限界上下文與資料策略
- 實作：讀寫拆分、事件同步
- 資源：Debezium/Outbox
- 時間：3-5 天
3. 影子流量與切換
- 實作：鏡像/比對→灰度切換
- 資源：流量鏡像
- 時間：2-3 天

### 關鍵程式碼/設定
```nginx
# NGINX：按路徑路由到新服務
location /api/v1/orders/ {
  proxy_pass http://orders-service;
}
location /api/v1/legacy/ {
  proxy_pass http://legacy-app;
}
```

- 實際案例：「轉移到微服務架構必經之路（2018）」與「大型 Web Application 轉移到微服務（2017）」主題。
- 實作環境：NGINX/Kong/YARP、Debezium/Kafka、.NET
- 實測數據：逐步遷移過程的缺陷密度與回滾次數、影子與正式差異率

Learning Points：Strangler、CDC/Outbox、灰度與影子流量
Practice：針對一條 API 完成遷移（8 小時）
Assessment：零停機切換、資料一致性與回退方案

---

## Case #11: 微服務資料管理：CQRS 與 Event Sourcing 實踐

### Problem Statement
- 業務場景：訂單/庫存領域需要審計追蹤與時點還原，讀寫模式差異大。
- 技術挑戰：寫入模型設計、讀模型投影、事件一致性與順序。
- 影響範圍：可追溯性、擴展性、一致性策略。
- 複雜度：高

### Root Cause Analysis
- 直接原因：
  1. 單一模型難兼顧讀寫與審計。
  2. 事件存儲與投影機制缺失。
  3. 冪等與重放策略不明。
- 深層：
  - 架構：缺事件驅動核心
  - 技術：事件儲存/投影工具缺少
  - 流程：版本演進與回溯流程未定

### Solution Design
- 解決策略：寫入端採聚合與事件溯源，讀端以投影器生成查詢模型；提供事件重放、快照與讀模型再建；以序號/流版本維持一致性。

### 實施步驟
1. 事件模型與聚合
- 實作：定義事件/聚合邏輯
- 資源：DDD、EventStoreDB
- 時間：2-3 天
2. 投影器與讀模型
- 實作：訂閱並更新投影
- 資源：ESDB Subscriptions
- 時間：2-3 天
3. 回溯與快照
- 實作：重放/快照與壓縮
- 資源：ESDB
- 時間：2 天

### 關鍵程式碼/設定
```csharp
// EventStoreDB 追加事件（片段）
await client.AppendToStreamAsync($"order-{orderId}",
    StreamState.Any,
    new[] { new EventData(Uuid.NewUuid(), "OrderCreated", JsonSerializer.SerializeToUtf8Bytes(evt)) });
```

- 實際案例：「CQRS / Event Sourcing 的應用與實踐（.NET Conf 2021）」。
- 實作環境：.NET、EventStoreDB/Kafka、SQL for read models
- 實測數據：讀寫分離後的讀延遲下降、審計查詢覆蓋率

Learning Points：聚合/事件/投影、重放與快照
Practice：為「訂單」建一條 ES + CQRS 流（8 小時）
Assessment：一致性、回溯正確性、讀模型延遲

---

## Case #12: 從零開始的 DevOps：組織與技術的雙輪驅動

### Problem Statement
- 業務場景：傳統研發流程手動化程度低，發布稀疏且風險高，缺乏可觀測與回饋。
- 技術挑戰：工具鏈分散、流程未標準化、文化推動困難。
- 影響範圍：交付速度、穩定性、團隊士氣。
- 複雜度：中

### Root Cause Analysis
- 直接原因：
  1. 缺版本控制與 CI/CD 一致性。
  2. 測試自動化不足。
  3. 無指標與回饋機制。
- 深層：
  - 架構：不可重建的環境
  - 技術：IaC/測試/監控不足
  - 流程：無度量與改善機制

### Solution Design
- 解決策略：以 DORA 為指標建立改進路線；版本控制全覆蓋、CI/CD 與 IaC、測試金字塔、自動化觀測與告警、事後檢討與持續改善。

### 實施步驟
1. 基礎建設即程式
- 實作：環境可重建
- 資源：Terraform/Ansible
- 時間：2-3 天
2. CI/CD 與測試
- 實作：單元/整合/端到端
- 資源：CI/CD、測試框架
- 時間：3-5 天
3. 度量與回饋
- 實作：DORA 儀錶板、事後檢討
- 資源：Grafana、看板
- 時間：1-2 天

### 關鍵程式碼/設定
```yaml
# GitHub Actions：基本 CI
name: ci
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: dotnet test
```

- 實際案例：「從零開始的 DevOps（2019）」主題。
- 實作環境：Git、CI/CD、IaC、監控
- 實測數據：DORA 四指標的基線與改善曲線

Learning Points：DORA、IaC、測試金字塔
Practice：為一專案導入 CI + IaC（8 小時）
Assessment：自動化覆蓋率、可重建能力、指標可見性

---

## Case #13: 容器驅動開發：開發環境與生產環境一致性

### Problem Statement
- 業務場景：多人開發與多服務依賴，開發機環境差異大，常出現「在我機器可行」問題。
- 技術挑戰：依賴項與版本漂移、資料庫/快取服務啟動困難。
- 影響範圍：開發效率、缺陷率、交付節奏。
- 複雜度：低-中

### Root Cause Analysis
- 直接原因：
  1. 本機手動安裝依賴。
  2. 缺容器化與編排。
  3. 無開發者啟動腳本。
- 深層：
  - 架構：缺標準化基底映像
  - 技術：未使用 Compose/DevContainer
  - 流程：新人成本高

### Solution Design
- 解決策略：以 Dockerfile/Compose 定義本機依賴，VS Code DevContainer 提供一鍵開發環境，與 CI 映像共享，確保一致性。

### 實施步驟
1. 容器化服務
- 實作：Dockerfile、多階段建置
- 資源：Docker
- 時間：1 天
2. Compose 依賴
- 實作：DB/Cache/Message Broker
- 資源：docker-compose
- 時間：1 天
3. DevContainer
- 實作：VS Code 開發容器
- 資源：devcontainer.json
- 時間：0.5 天

### 關鍵程式碼/設定
```dockerfile
# Dockerfile（.NET 8）
FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
WORKDIR /src
COPY . .
RUN dotnet publish -c Release -o /app
FROM mcr.microsoft.com/dotnet/aspnet:8.0
COPY --from=build /app /app
ENTRYPOINT ["dotnet","App.dll"]
```

- 實際案例：「容器驅動開發（.NET Conf 2017）」。
- 實作環境：Docker、VS Code DevContainer、.NET
- 實測數據：新人成本下降、環境問題缺陷數下降

Learning Points：容器化、Compose、DevContainer
Practice：為專案建 Docker + Compose（2 小時）
Assessment：一鍵啟動、與 CI 映像一致性

---

## Case #14: API Gateway 與 BFF：微服務下的前端友好層

### Problem Statement
- 業務場景：前端多形態（Web/APP/後台）對接眾多微服務，畫面載入慢、呼叫複雜且安全風險高。
- 技術挑戰：聚合、快取、權限與節流、設備差異。
- 影響範圍：體驗、延遲、安全。
- 複雜度：中

### Root Cause Analysis
- 直接原因：
  1. 前端直連多服務複雜。
  2. 缺聚合與快取策略。
  3. 權限與節流分散。
- 深層：
  - 架構：缺 BFF 層
  - 技術：無 API Gateway 策略
  - 流程：契約與版本治理不足

### Solution Design
- 解決策略：建立 API Gateway（鑑權/節流/路由）與 BFF（聚合/裁剪/快取），讓前端以體驗為中心對接，同時強化安全與性能。

### 實施步驟
1. Gateway 策略
- 實作：OAuth2、Rate Limit、Circuit Breaker
- 資源：Kong/APIM
- 時間：1-2 天
2. BFF 聚合
- 實作：端點聚合、圖像化（選 GraphQL）
- 資源：YARP/GraphQL
- 時間：2-3 天
3. 快取與觀測
- 實作：針對畫面快取、指標
- 資源：Redis/APM
- 時間：1-2 天

### 關鍵程式碼/設定
```csharp
// .NET: YARP 反向代理與簡單 BFF 聚合
builder.Services.AddReverseProxy().LoadFromConfig(builder.Configuration.GetSection("ReverseProxy"));
app.MapGet("/bff/home", async (IHttpClientFactory f) =>
{
    var api = f.CreateClient("api");
    var products = await api.GetFromJsonAsync<List<Product>>("/products/top");
    var banners = await api.GetFromJsonAsync<List<Banner>>("/banners");
    return new { products, banners };
});
```

- 實際案例：大型 Web App 轉微服務經驗與 API First 系列相關。
- 實作環境：Kong/APIM、YARP、Redis、.NET
- 實測數據：首屏延遲下降、請求數減少、錯誤率下降

Learning Points：Gateway/BFF、聚合與快取
Practice：建一個 BFF 端點聚合兩服務（2 小時）
Assessment：延遲/請求數改善、鑑權與節流正確性

---

## Case #15: .NET Console 應用：組態、環境變數與祕密管理

### Problem Statement
- 業務場景：內部工具與批次程式需要多環境配置與祕密（金鑰/連線字串）管理，避免硬編碼與洩漏。
- 技術挑戰：設定覆寫、環境切換、祕密保護。
- 影響範圍：安全、維護性、可移植性。
- 複雜度：低

### Root Cause Analysis
- 直接原因：
  1. 設定寫死在程式碼。
  2. 祕密與設定混在同檔。
  3. 無環境覆寫策略。
- 深層：
  - 架構：缺配置層
  - 技術：未用 ConfigurationBuilder
  - 流程：祕密發放不合規

### Solution Design
- 解決策略：採 .NET 組態系統（appsettings + 環境變數 + 使用者祕密/KeyVault），建立覆寫優先順序與 schema 驗證。

### 實施步驟
1. 組態來源鏈
- 實作：appsettings、env、Secrets
- 資源：.NET、Secrets Manager
- 時間：0.5 天
2. Schema 驗證
- 實作：綁定 Options + 驗證
- 資源：FluentValidation
- 時間：0.5 天
3. 祕密管理
- 實作：KeyVault/Parameter Store
- 資源：雲端服務
- 時間：1 天

### 關鍵程式碼/設定
```csharp
var config = new ConfigurationBuilder()
  .AddJsonFile("appsettings.json", optional:true)
  .AddJsonFile($"appsettings.{Environment.GetEnvironmentVariable("DOTNET_ENVIRONMENT")}.json", true)
  .AddEnvironmentVariables()
  .AddUserSecrets<Program>(optional: true)
  .Build();

var options = config.GetSection("MyApp").Get<MyOptions>();
```

- 實際案例：主題「在 C# 控制台應用程式中使用變數數據」。
- 實作環境：.NET、Secrets、KeyVault
- 實測數據：祕密洩漏風險降低、配置錯誤率下降

Learning Points：組態層、覆寫順序、祕密管理
Practice：為工具加入多環境設定與祕密（30 分）
Assessment：安全性、易用性、覆寫可驗證性

---

## Case #16: RAG（檢索增強生成）基線：知識型 AI 功能落地

### Problem Statement
- 業務場景：客服/商品問答需使用內部知識庫，模型需可隨資料更新提升準確性，並控制成本。
- 技術挑戰：檢索品質、向量索引、片段切分、更新管線與品質評測。
- 影響範圍：回答準確率、延遲、成本。
- 複雜度：高

### Root Cause Analysis
- 直接原因：
  1. 無結構化資料攝入與向量化管線。
  2. 檢索無法對齊用例。
  3. 無品質評測基線。
- 深層：
  - 架構：缺離線/線上雙迴路
  - 技術：切分/嵌入/重排序選型不當
  - 流程：資料更新與評測未自動化

### Solution Design
- 解決策略：建立資料→切分→嵌入→索引→檢索→重排序→生成的管線；導入評測集與離線自動評測，線上 AB 與觀測成本/延遲/品質，並有快取/回退策略。

### 實施步驟
1. 資料與索引
- 實作：清洗、切分（滑動窗）、嵌入與索引
- 資源：Az Cognitive Search/PGVector
- 時間：2-3 天
2. 檢索與重排序
- 實作：BM25 + 向量混合、重排序器
- 資源：Hybrid Search
- 時間：1-2 天
3. 評測與觀測
- 實作：QA 集、精確率/召回率、成本延遲
- 資源：評測框架、APM
- 時間：2 天

### 關鍵程式碼/設定
```csharp
// 查詢：Hybrid + 生成（概念化）
var docs = await searchClient.SearchAsync(query, new SearchOptions { Vector = embed(query), K=3, UseTextSearch=true });
var prompt = BuildPrompt(query, docs);
var answer = await llm.GenerateAsync(prompt);
```

- 實際案例：出處「LLM 應用程式開發」系列。
- 實作環境：Azure Cognitive Search/PGVector、.NET、Azure OpenAI
- 實測數據：離線準確率提升、P95 延遲與每請求成本受控

Learning Points：切分/嵌入/混合檢索、評測與快取
Practice：建立小型 FAQ RAG（8 小時）
Assessment：準確率、成本/延遲、快取命中率

---

# 案例分類

1) 按難度分類
- 入門級：Case 13、15
- 中級：Case 1、3、4、6、7、8、9、14
- 高級：Case 2、5、10、11、12、16

2) 按技術領域分類
- 架構設計類：Case 1、4、7、8、10、11、14、16
- 效能優化類：Case 5、6、7、14
- 整合開發類：Case 3、9、13、15
- 除錯診斷類：Case 5、6、3（契約斷裂診斷）
- 安全防護類：Case 2（Guardrails）、14（Gateway 鑑權）、15（祕密管理）

3) 按學習目標分類
- 概念理解型：Case 12、11、6
- 技能練習型：Case 13、15、9
- 問題解決型：Case 1、3、4、5、7、8、10、14
- 創新應用型：Case 2、16

# 案例關聯圖（學習路徑建議）
- 起步（基礎設施與基本功）：
  1) Case 13（容器驅動開發）→ 15（組態與祕密）
- API 與契約基線：
  2) Case 1（API First）→ 3（降級/相容驗證）→ 4（模型持續交付）→ 14（Gateway/BFF）
- 微服務與非同步：
  3) Case 8（服務發現）→ 7（事件驅動）→ 6（非同步 SLO）→ 11（CQRS/ES）→ 10（Strangler 遷移）
- 交付與組織能力：
  4) Case 12（從零 DevOps）→ 5（大型團隊 CI/CD）
- AI 能力：
  5) Case 2（AI First 能力層）→ 16（RAG 基線）

依賴關係：
- Case 1 是 Case 3/4/14 的前置。
- Case 7 依賴 Case 8 的發現與容錯。
- Case 6/11 依賴 Case 7 的事件化。
- Case 5 建立在 Case 12 的文化與流程基礎上。
- Case 16 建立在 Case 2 的 LLMOps 能力之上。

完整學習路徑建議：
- 先補齊開發與運行一致性的底座：Case 13 → 15
- 建立契約驅動與前端友好接口：Case 1 → 3 → 4 → 14
- 進入微服務與事件化：Case 8 → 7 → 6 → 11 → 10
- 擴大交付與效率：Case 12 → 5
- 最後導入 AI 能力並建立知識型應用：Case 2 → 16

說明：原文為主題與連結彙整，未提供專案級數據。以上案例已轉化為可教學與實作的通用方案，實測數據請於練習/專案中依指標自行量測。