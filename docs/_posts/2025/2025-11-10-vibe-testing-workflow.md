---
layout: post
title: "AI-First Testing, 以 AI 為核心重新設計測試流程"
categories:
- "系列文章: 架構師觀點"
tags: ["架構師觀點","技術隨筆"]
published: false
comments_disqus: false
comments_facebook: false
comments_gitalk: false
redirect_from:
logo: 
---

首先, 我要打臉我半年前寫的那篇 [聊聊 Vibe Testing 實驗心得](/2025/05/01/vibe-testing-poc/) 的文章｡ 
當時我的思路是, 現在的流程太花人工了, 如何讓 AI 來替代人工手動測試, 讓測試流程更有效率?

不過, 事後發現, 如我我完全沒有既有流程的包袱, 我應該不會這樣做才對..., 有了這警覺, 我重新想了一次這流程, 有了不同的發現 (所以就有了這篇文章)｡ 

這篇我想談的主軸: 從 AI 的能力出發, 重新思考 "測試" 這件事應該怎麼進行比較有效? 怎樣更能從來源解決測試的問題?


<!--more-->

這兩年來, 不管在公司, 或是個人, 或是社群, 大家都在拼命地用 AI 工具來提升自己的工作效率, 這是很正常的。不過很多時候我都在想: AI 的原生世代也會這樣做事嗎? 我想起 Sam Altman 去年的一個專訪, 提到不同世代的人, 對 AI 的使用方式會有很大的差異｡

最令我印象深刻的是這段: 20 歲的年輕人, 把 ChatGPT 當成 "作業系統" 來使用...

> "Older people use ChatGPT as a Google replacement."
> "People in their 20s and 30s use it as a life advisor. "
> "People in college use it as an operating system."

https://youtu.be/uVEjlRK0VWE?si=jDQyFJj2n8m4zrug&t=4

這就是使用深度的差異吧, 對於 Developer 來說, 我想是目前運用 AI Agent 最深度的族群了｡ 我看到很多人已經拿 Claude Code 這類 CLI Tools 來當作它們主要的工作介面, 不只用來寫 code, 也用來寫文件, 更用來做各種其他任務或是自動化...

回到半年前我用 vibe testing 的角度來思考測試這件事, 我發現我的思路是:

> " vibe coding 很炫, 那我是不是也能 vibe testing 呢? "
> " 目前的測試都需要仰賴人工操作, 那我是不是也能用 AI 來代替人工操作呢? "
> " 如何讓 AI tools 無痛的融入現有的測試流程, 解放過去投在裡面的人力?? "

其實這些思路都沒錯, 也都得到效率的提升, 但是我總覺得不對勁, 因為最關鍵的流程 (workflow) 並沒有改變, 只是把 "人" 換成 "AI" 而已｡
AI 有他擅長的能力 (GPU), 人類也有擅長的能力 (Brain), 而傳統的應用程式也是 (CPU), 能鑑別出這些 "運算資源" 的使用範圍跟邊界, 然後重新設計流程, 才是最重要的事情｡

因此, 距離上一篇文章, 隔了六個月, 這段期間我花了不少時間在研究 MCP design, Context Management, 以及 SDD / SpecKit 的使用, 這些基礎我認為都是未來重要的基本技能, 我重新用 TestKit 來包裝我對 Testing Workflow 的想法, 效果還不錯, 於是整理了這篇文章來分享一下我的心得｡

接下來的文章內容, 我會在第一段先介紹想法的轉變, 然後拆成三個步驟來完成 AI-First Testing Workflow 的示範:

- 先用 Decision Table 把「對的測試」定義清楚，而不是一開始就丟給 AI 自己亂測  
- 再把 AI 放在「產生自動化測試資產」的位置，而不是讓它去當永遠跑不完的測試工  
- 最後，用同一份 test case 同時覆蓋 API / UI，讓流程真的有機會縮減，而不是再多生一份文件地獄

下面是我目前的實作版本，還在演化中...
 







# 1. AI Native Testing Workflow: 從「用 AI 代跑人工」到「為 AI 重寫流程」

對我來說, 測試是件相當頭痛的問題, 主要的痛點在於測試的 "量體" 太大, 背後包含寫不完的測試案例, 準備不完的測試資料, 以及跑不完的手動 & 自動測試｡即使有 AI 加持, 你要測完所有的狀況也幾乎是不可能的事情, 勢必要有一些方法, 有效率並且優先完成重要有價值的測試才是正途｡

為了聚焦我的主題跟思路, 不相關的環節我就大幅簡化了, 我用這張圖來說明:


![alt text](/images/2025-11-10-vibe-testing-workflow/image-17.png)


從左到右, 是從需求規格中的驗收條件 (AC, Acceptance Criteria) 開始, 到測試結果的流程｡ 每個步驟上面標的數字, 就是可能要展開的數量 (數量級)｡ 試著估算看看: 如果系統需求列了一項 AC (ex: 購物車結帳時必須檢查折扣計算規則, 以及購買條件限制), 最終應該展開多少筆 Test Cases ? 在系統的生命週期內你需要執行幾次測試? 成本與代價是什麼?


**量體估算**:
- 每 1 個 AC 平均展開 10+ 個 Test Case
- 系統有 4 種操作方式 (操作: Web UI, Android APP, iOS APP, REST API)
- 系統有 10 種非功能性需求 (NFR, Non Functional Requirement, 如資安, 授權, 效能等等)


**執行瓶頸**:
1. **測項數量**:  
如果這些組合都展開, 每一個 AC 就必須要維護 4 + 10 + 10 份規格文件, 以及產生出來的 1 x 4 x 10 x 10 = 400 份不同的測試腳本... 這量體, 光是文件就已經難以維護了.. 這是第一個瓶頸, 測項維護｡

2. **執行次數**:  
就算真的有 400 份測試案例, 一年你會需要跑幾輪測試?  
假設一週 release 一次, 每次 release 前內部 QA 至少測一次, release 後正式環境也至少測一次, 那一年應該要跑 50 x 2 = 100 輪, 總共是 400 x 100 = 40,000 次測試...


一年執行 40000 次測試, 若用 AI 來自動執行, 需要花多少錢? 這時, 我回頭看看我半年前自己寫的文章, 我想打臉我自己的想法了啊 XDD... 如果我真的用 vibe testing 用 AI 當下執行 API test, 每跑完一次約需要 2 分鐘, 對應的 api token 約 USD$ 0.03 (剛好約 NTD$ 1.0) 左右, 一年要花 2 x 40000 = 80000 分鐘來執行 (55.56 天), 整個過程共計要花掉 4 萬台幣的 token 費用... 而這只是完成 "一個 AC" 的要求需要的測試而已...

簡單的計算, 就知道這方法還無法脫離實驗階段... 雖然我的想法的確比單純用 "人力" 來測試好很多, 但是遠遠跟不上測試的通膨..., 而且 AI 執行測試也有缺點, 除了速度跟費用之外, AI 每次執行的結果都有一些差異, 這也是另一個頭痛的問題｡

因此, 我重新想過這兩個瓶頸, 我的直覺告訴我 AI 是能搞定這問題的, 但是絕對不是用 AI 代替人工這麼簡單｡ 我的流程應該要重新思考, 否則我只是讓 AI 自動的執行 "手動測試" 而已 (把 AI 當真人看的話, 我只是拿 token 換時薪而已)｡ 我真正該做的, 是讓 AI 有效率的做真正的自動化測試 (寫自動化測試的程式碼, 然後系統化的自動執行)


因此, 我調整想法, 這兩個瓶頸, 應該要在產生之前就解決掉他｡ 過去這類 pattern 都是 "左移" 就能搞定, 因此我想要:

1. **"文件的維護" 左移**:  
如果我要維護的文件範圍往流程的左邊移動, 我就能在文件的數量放大之前就維護好的話, 數量級就降下來了｡ 假設我能簡化 test case 的內容, 只保留抽象的執行步驟, 不要有任何具體的操作細節 (例如: 按下某個按鈕, 或是呼叫某個明確的 API + 參數...), 我要維護的 test case 就只有 10 個了, 不過最終我仍然需要展開 source code, 這時我需要 "明確操作的規格說明書", 以 api 來說就是 api spec, 加上這些都準備妥當的話, 其實我真正要維護的文件數量, 就從 4 x 10 x 10 + 4 + 10 + 10 = 425 份文件, 降低為 4 + 10 + 10 = 25 份文件 (這合理多了)

2. **"自動化執行" 左移**:
從原本想像的 AI 自動的 "手動執行", 變成一次性的執行, 之後把這次執行的步驟輸出成 source code, 我只需要花費一次 AI 執行的時間成本 (GPU 費用), 之後重複執行的就都是單純運算成本 (CPU 的費用) 了｡ 整個流程只要能讓 AI 寫完 400 份自動測試的 code, 後面的執行成本就低到可以不用特別計算了 (你的網頁被點閱 40000 次要多少錢? 基本上很便宜, 你甚至用既有的測試環境就足以負荷)

改良過的流程, 變成這張圖:

// P40, https://docs.google.com/presentation/d/1ZYtaDZ0igCbUaN4UwKAPeo06YViOFO0CAPlfQYke0uY/edit?slide=id.g3a1eaa2cc57_1_69#slide=id.g3a1eaa2cc57_1_69


![alt text](/images/2025-11-10-vibe-testing-workflow/image-18.png)


不過, 這是 "理想", 實作的過程中藏了不少貓膩, 像是 "執行記錄" 就是之一, 後面我介紹實作的時候再來聊這段｡

到這裡為止, 就是我對真正導入 AI testing 的想法, 包含了技術的掌握 (上一篇: 用 LLM function calling 的能力來執行 API 自動呼叫的能力), 也包含了流程的調整 (就是這篇: AI testing workflow)｡

後面的 demo, 我就按這順序來說明我怎麼構思這個流程｡ 在做這個 side project 的過程中, 正好碰到 speckit 的興起, 我突然發現用這樣的方式交付我構想的流程好像還不錯, 因此我就花了點時間把這些流程包裝成 "testkit" 了｡對比 speckit 有一系列的步驟 (ex: specify, clearify, plan, task ... etc), 我也把 test 的步驟依樣話葫蘆的展開:

1. ```/testkit.gentest```,  
如何從 decision table 展開 ac 的測試要求
2. ```/testkit.api.run```, 如何從 (1) 做好 api auto test 的準備, 用 ai (用我自己開發的 mcp) 找出執行步驟
3. ```/testkit.web.run```, 如何從 (1) 做好 web auto test 的準備, 用 ai (用 playwright mcp) 找出執行步驟

而 (2), (3) 會產出執行記錄, 這很關鍵,我最早測試的版本, 是把 api spec, 跟 test case 直接丟給 AI 當作規格, 讓他直接生成 test code, 這其實可行, 但是產生的 api 呼叫步驟不一定 100% 正確, 我通常要整個 code 寫完才能測試, 效率並不是很好｡ 關鍵原因主要在於, api spec 的資訊, 一般而言都只專注在參數規格, 整個使用的流程並沒有交代的很清楚｡ 而 [上一篇] 談 vibe testing 的示範, 因為是互動的流程, AI 有機會探索 + 重複嘗試, 反而這樣的過程比完全靜態 (只看文件) 更有機會找出正確的執行步驟｡ 我突然聯想到, 真人在寫 code 呼叫 api 的時候, 不也都會在真正寫 code 前, 先用 postman 試著跑看看嗎? 搞懂了那些 api 跟參數用法之後, 才真的把他寫成 code 會容易的多｡ 因此我也在 ai testing 的流程中插入先前研究的 vibe testing, 先讓 ai 跑一次, 並且把執行過程的詳細記錄都保留下來, 就像你讓 ai 先用 postman 跑完後再把 postman 的記錄檔案翻譯成 auto test code 一樣｡

而最後我發現最有效的方法, 也不是自己寫 prompt 讓 ai 寫 code 了, 因為這段研究過程中 speckit 出現了 (這世界到底要變化的多快啊...), 我開始將策略改成: 提供足夠精準的 auto test 需求規格, 用 speckit 幫我寫成 auto test code.. 這是目前我能想到最理想的流程, 於是, 後半段的關鍵就是 - 我能給多好的規格?

從這角度來想, 我發現我都準備好了啊, 一個理想的 auto test code 需要的規格, 不外乎:

1. **測試案例** (test case), 測試本身要做的事情是什麼... (這前面 ac 展開的 test case 就是了)
2. **操作規格** (以 api 而言, 就是 api spec), 說明操作方式跟細節, 讓 AI 能有跡可尋
3. **操作範例** (就是前面提到, postman 執行的記錄, 可以讓 ai 有時記得案例參考, 能一次到位, 寫出正確的 test code)

受惠於上一篇 vibe testing 的研究成果, 只要把 ai 執行的 log 忠實的存下來, 我就能很輕易的補足 (3) 了, 因此後半段的作業流程也都補齊了, 測試效果也很令人滿意｡ 不過礙於篇幅, 後半段的 demo 我就留到下一篇再聊, 這篇我先把前半段的部分補完｡

再繼續往下拆解 demo 過程之前, 我在這邊先總結一下我對 ai (native) test workflow 的想法吧｡ 最近將近兩年的時間, 我都跟著團隊推動各種 AI 相關的工作流程改變, 以及各種 AI 產品開發的架構設計, 我始終有一個感覺, AI 帶來其實是個 "變革", 不只是 "改善" 而已｡ 我一直在提醒自己, 別掉入用 "新工具" 套 "舊流程" 的陷阱, 而 ai 來的太快, 什麼是正確的 "新流程" 也沒人知道, 一切都要靠自己摸索嘗試, 這才是最困難的環節, 不過也是最有趣, 最有成就感的地方｡

最後, 補上我在 facebook 看到幾篇類似觀點的 PO 文 (當然也包括我自己寫的) 給各位參考:

- (my post)
- (AI 變革, 但是國內企業大多停留在 輔助 的階段)

https://www.facebook.com/share/p/1HoM5rq2D9/






# 2. 用 Decision Table 定義 "有價值的測試"

測試, 看的就是涵蓋率, 所有的條件組合中, 有哪些情境你的測試會踩過｡ 這時你沒有用有系統的方式把所有組合列出來的話, 其實你很難確認涵蓋率的, 當然你就更難決定哪些條件組合是重要的, 以及是否有漏掉重要的路徑沒安排測試｡

決策表 (decision table), 是有系統的列出所有組合的方法｡ 我第一個示範,就是用 decision table 來讓 AI 替我建立 decision table, 讓我確認我該怎麼測, 該測哪些組合｡

先講結論, 這是很關鍵的步驟, 你可以大量依賴 AI 幫你整理跟建議, 但是這絕對是要你親自 review 的重要環節, 千萬別讓 AI 列了一版, 然後你看都沒看就回覆 OK... 我自己使用的心得是, AI 替你抓的 decision table 格式通常都正確, 但是他建議的 criteria / action / rule 通常都很糟糕, 你需要花一些心思來 review 這張表格的設計, 這會決定你後面的測試有沒有效率, 有沒有精準涵蓋你的系統流程｡

你需要先掌握 decision table 的使用方式, 你才有能力挑出 AI 的建議哪邊需要修改, 這環節省不掉的, 我強烈建議:

1. 先花一點時間, 好好看懂 decision table 在幹嘛
2. 如果要上課或是找專家, 我推薦敏捷三叔公的文章
3. 你也可以找 AI 教你, 像是 ChatGPT 就有學習模式

接下來, 我就拿我的案例實際操作一次吧! 首先要先準備我自己包裝的 testkit (github repo), 因為是 PoC, 我沒有做太多相關工具, 只有準備關鍵的 prompt, 因此操作要手動...




## 2-1. Init TestKit

有用過 speckit 的大概都清楚我要講什麼, 不過我還沒包裝成套件, 所以... 陽春一點, 就自己把 prompts 跟 mcp.json 複製到你的 workspace 吧, mcp server 我還沒準備好 release, 各位可以先玩 /testkit.gencode ..

測試的標的, 我就用之前我為了驗證 "安得魯小舖" 而開發的 API, 我簡單做了改版, 目的是用來示範這次的 AI native test workflow.. 

簡單交代幾個 "安得魯小舖" 重要的 url:
- web ui: https://shop.chicken-house.net/ , 我是前端大外行, 這是完全用 vibe coding 生出來的 web ui, 破綻很多, 純示範用途..
- rest api: https://shop.chicken-house.net/api , 自帶支援 oauth2
- open api spec: https://shop.chicken-house.net/swagger/index.html


## 2-2. Prepare Decision Table

首先, 我以 vscode + github copilot 當作我主要示範的環境 (你要換 claude code 也可), 我選用的 model: claude haiku 4.5

```

/testkit.gentest

購物車結帳時, 必須依據購物車內的商品項目與數量，配合折扣規則正確計算結帳金額

目前可接受的折扣規則有：
- 指定商品第二件六折

商店系統有這三件商品上架:
1. 啤酒, 第二件六折, 單價 65
2. 可樂, 單價 18
3. 綠茶, 單價 25

系統限制
- 單一訂單內，每種商品限購 10 個

驗證購物的身分：
- username: andrew
- password: 1234

```

其實我已經把驗收條件簡化到不能再簡化了, 這邊敘述的是我要測試結帳流程, 關鍵的驗證是 結帳金額 (包含折扣) 必須正確, 另外結帳限制也必須符合, 不能結帳的條件必須擋下來 (這部分我 api 沒有實作, 預期這個測試會失敗)｡

這些條件, 關聯到 api 以及背後系統的設定, 這邊我預先都先準備好了 (商品已經先上架, 折扣已經先設定好, 帳號已經先準備好)｡

以下是 AI 給我第一版的 decision table:


| 規則 | C1<br>有啤酒 | C2<br>啤酒≥2 | C3<br>有可樂 | C4<br>有綠茶 | C5<br>數量≤10 | A1<br>啤酒折扣 | A2<br>可樂計價 | A3<br>綠茶計價 | A4<br>正確總額 | A5<br>拒絕結帳 |
|------|:------:|:------:|:------:|:------:|:-------:|:--------:|:--------:|:--------:|:--------:|:--------:|
| **R1** | Y | Y | N | N | Y | ✔ | – | – | ✔ | – |
| **R2** | Y | N | N | N | Y | – | – | – | ✔ | – |
| **R3** | N | N | Y | N | Y | – | ✔ | – | ✔ | – |
| **R4** | N | N | N | Y | Y | – | – | ✔ | ✔ | – |
| **R5** | Y | Y | Y | N | Y | ✔ | ✔ | – | ✔ | – |
| **R6** | Y | Y | N | Y | Y | ✔ | – | ✔ | ✔ | – |
| **R7** | Y | Y | Y | Y | Y | ✔ | ✔ | ✔ | ✔ | – |
| **R8** | Y | N | Y | Y | Y | – | ✔ | ✔ | ✔ | – |
| **R9** | Y | Y | N | N | N | – | – | – | – | ✔ |


這是個符合 "規範" 的決策表, 但是並不是個理想的決策表, 因為決策表的條件 (criteria) 部分, 他已經簡化成:

- **C1**: 購物車中包含啤酒商品
- **C2**: 啤酒數量 ≥ 2 件 (可適用第二件六折)
- **C3**: 購物車中包含可樂商品
- **C4**: 購物車中包含綠茶商品
- **C5**: 所有商品單項數量都 ≤ 10 件

而動作 (action) 部分, 也已經簡化成結果, 只陳述有沒有符合:

- **A1**: 計算啤酒折扣 (第二件起六折)
- **A2**: 計算可樂金額 (無折扣)
- **A3**: 計算綠茶金額 (無折扣)
- **A4**: 回傳正確的總金額
- **A5**: 拒絕結帳 (超過購買限制)

因為系統是我設計的, 我很清楚內部運作原理, 以及我想要測試什麼｡ Decision Table 使用技巧我這邊不打算談太多, 有興趣的可以參考我上面給的方式補強｡ 我直接要求 AI 把 decision table 改成我要的樣子:

(對談過程略過, 直接列出最終的 decision table 版本)


---

| 規則 | C1<br>啤酒數量 | C2<br>可樂數量 | C3<br>綠茶數量 | A1<br>啤酒優惠組數 | A2<br>總金額 | A3<br>總優惠 | A4<br>結帳金額 | A5<br>允許結帳 |
|------|:--------:|:--------:|:--------:|:------------:|:--------:|:--------:|:----------:|:--------:|
| **R1** | 0 | 0 | 0 | 0 | $0 | $0 | $0 | ❌ 拒絕 |
| **R2** | 1 | 0 | 0 | 0 | $65 | $0 | $65 | ✅ 允許 |
| **R3** | 2 | 0 | 0 | 1 | $130 | -$26 | $104 | ✅ 允許 |
| **R4** | 3 | 0 | 0 | 1 | $195 | -$26 | $169 | ✅ 允許 |
| **R5** | 4 | 0 | 0 | 2 | $260 | -$52 | $208 | ✅ 允許 |
| **R6** | 10 | 0 | 0 | 5 | $650 | -$130 | $520 | ✅ 允許 |
| **R7** | 11 | 0 | 0 | 5 | $715 | -$130 | $585 | ❌ 拒絕 |
| **R8** | 12 | 0 | 0 | 6 | $780 | -$156 | $624 | ❌ 拒絕 |
| **R9** | 0 | 5 | 0 | 0 | $90 | $0 | $90 | ✅ 允許 |
| **R10** | 0 | 0 | 4 | 0 | $100 | $0 | $100 | ✅ 允許 |
| **R11** | 2 | 3 | 0 | 1 | $184 | -$26 | $158 | ✅ 允許 |
| **R12** | 3 | 2 | 4 | 1 | $331 | -$26 | $305 | ✅ 允許 |
| **R13** | 0 | 11 | 0 | 0 | $198 | $0 | $198 | ❌ 拒絕 |
| **R14** | 5 | 5 | 11 | 2 | $740 | -$52 | $688 | ❌ 拒絕 |

條件定義 (Conditions)
- **C1**: 購買啤酒的數量 (單價 $65，每 2 件中第 2 件 $39)
- **C2**: 購買可樂的數量 (單價 $18)
- **C3**: 購買綠茶的數量 (單價 $25)

動作定義 (Actions)
- **A1**: 啤酒優惠組數
  - 計算公式: `floor(啤酒數量 / 2)`
  - 例: 3 件 → 1 組，4 件 → 2 組，10 件 → 5 組
- **A2**: 總金額 (未折扣)
  - 計算公式: `啤酒數量 × 65 + 可樂數量 × 18 + 綠茶數量 × 25`
- **A3**: 總優惠 (負數)
  - 計算公式: `啤酒優惠組數 × -26`
  - 說明: 每組優惠省 $26 (原價 $65 - 六折 $39)
- **A4**: 結帳金額
  - 計算公式: `總金額 + 總優惠`
- **A5**: 是否允許結帳判定
  - ✅ 允許: 所有商品數量 ≤ 10 且購物車不為空
  - ❌ 拒絕: 任一商品數量 > 10 或購物車為空

規則說明 (測試企圖與重要性)
- **R1**: 空購物車
  - **測試企圖**: 驗證系統能正確拒絕空購物車結帳
  - **重要性**: 防止產生無效訂單，確保業務邏輯完整性

- **R2**: 單件啤酒無優惠
  - **測試企圖**: 驗證未達優惠門檻時，系統按原價計算
  - **重要性**: 確認優惠規則不會誤觸發，保護商家利益

- **R3**: 2 件啤酒觸發優惠 (1組)
  - **測試企圖**: 驗證剛好達到優惠門檻時，系統正確計算 1 組優惠
  - **重要性**: 這是優惠規則的**最小觸發邊界**，必須精準計算以建立客戶信任

- **R4**: 3 件啤酒 (1組優惠 + 1件原價)
  - **測試企圖**: 驗證奇數件數時，優惠組數計算正確 (只有 1 組，第 3 件原價)
  - **重要性**: 確認系統能正確處理**不完整組合**，避免多給或少給優惠

- **R5**: 4 件啤酒 (2組優惠)
  - **測試企圖**: 驗證多組優惠時，系統能正確累加計算
  - **重要性**: 確認優惠規則能正確累加，測試**整數組合**的計算邏輯

- **R6**: 10 件啤酒 (5組優惠，上限邊界)
  - **測試企圖**: 驗證達到系統限制上限時，優惠計算與允許結帳邏輯都正確
  - **重要性**: 測試**最大合法邊界**，確認 5 組優惠計算無誤，且系統不會錯誤拒絕合法訂單


啤酒超限測試
- **R7**: 啤酒超過 10 件 (11件，5組優惠)
  - **測試企圖**: 驗證系統能正確拒絕超過單項限購數量的訂單，但仍能正確計算優惠組數 (供錯誤訊息使用)
  - **重要性**: 測試**最小違規邊界** (10+1)，確保限購機制有效，且錯誤處理時金額計算邏輯不崩潰

- **R8**: 啤酒超過 10 件 (12件，6組優惠)
  - **測試企圖**: 驗證更大的違規數量 (10+2) 也能被拒絕，且能正確計算 6 組優惠
  - **重要性**: 確認限購機制不是**硬編碼單一數值**，而是真正的邏輯判斷 (>10 皆拒絕)

非優惠商品測試
- **R9**: 僅購買可樂 (無優惠商品)
  - **測試企圖**: 驗證非優惠商品的正常計價流程
  - **重要性**: 確保優惠邏輯不會影響非優惠商品，系統能正確區分商品類別

- **R10**: 僅購買綠茶 (無優惠商品)
  - **測試企圖**: 驗證另一個非優惠商品的計價邏輯
  - **重要性**: 與 R9 互補，確保所有非優惠商品都能正確處理

混合商品測試
- **R11**: 混合商品 (含 1 組優惠)
  - **測試企圖**: 驗證優惠商品與非優惠商品混合購買時，系統能正確分別計算
  - **重要性**: 這是**真實購物場景**，必須確保優惠只套用在啤酒，其他商品不受影響

- **R12**: 三種商品混合 (含 1 組優惠)
  - **測試企圖**: 驗證最複雜的購物組合，所有商品類型都存在且啤酒為奇數件時的計算正確性
  - **重要性**: 測試系統在**最複雜情境**下的穩定性，確保優惠計算邏輯不會互相干擾

其他商品超限測試
- **R13**: 可樂超過 10 件 (11件，無優惠)
  - **測試企圖**: 驗證限購規則適用於所有商品，不只優惠商品
  - **重要性**: 確認限購是**全商品通用規則**，避免規則漏洞

- **R14**: 混合商品但綠茶超限 (11件，含 2 組啤酒優惠)
  - **測試企圖**: 驗證混合購物時，系統能正確檢查每項商品的限購規則，即使其他商品合法
  - **重要性**: 確保限購檢查不會因為多商品混合而失效，保護庫存管理

---


我再強調一次, Decision Table 的內容真的要仔細看, 這是跟 AI 對焦測試內容的理解, 補足我前面過度簡略的敘述｡ Decision Table 沒弄好, 可能整組測試都沒有發揮效益 (沒測到有價值的範圍)｡ Decision Table 可以當作是 test case 的規格書, 你只要把這幫表格確定好, 基本上衍生出來的這包 test case 就安全了, 基本上最關鍵的部分就完成了｡

不得不說, 只要你給的回饋正確, 有確實 review, 也有正確回答你的期待, 其實 AI 整理的結果品質是不錯的｡ 這結果很正確, 而令我驚豔的是 AI 抓測試目的其實抓的很正確, 我要他列出測試的企圖跟重要性, 我覺得都有回達到我預期的結果｡

上表的數字, Criteria, Action 的刪減, Rules 的刪減, Table 的內容我都修正過了, 最終結果定案, 就可以繼續下一步了, 真正展開測試案例｡


## 2-3. Generate All Test Cases via Decision Table

我一直強調, 最關鍵的是 decision table, 其實確認完之後, 後面就都是 AI 的 "苦工" 了, 基本上格式在 prompt 都定義好了, 測試範圍在 decision table 都驗證好了, 這步驟就是把所有的 test case 寫出來而已｡

按照前面 decision table 的內容, 總共有 14 個 test case 需要展開, 我直接貼 AI 產完給我的摘要:

---

📁 目錄結構

```
/tests/shopping-cart-checkout/
  ├── decision-table.md              (決策表文件)
  ├── tc-01-空購物車.md
  ├── tc-02-單件啤酒無優惠.md
  ├── tc-03-2件啤酒觸發優惠.md
  ├── tc-04-3件啤酒1組優惠.md
  ├── tc-05-4件啤酒2組優惠.md
  ├── tc-06-10件啤酒5組優惠-上限邊界.md
  ├── tc-07-11件啤酒超限拒絕.md
  ├── tc-08-12件啤酒超限拒絕.md
  ├── tc-09-僅購買可樂.md
  ├── tc-10-僅購買綠茶.md
  ├── tc-11-啤酒加可樂混合.md
  ├── tc-12-三種商品全混合.md
  ├── tc-13-可樂超限拒絕.md
  └── tc-14-混合但綠茶超限.md
```

📊 測試覆蓋率分析
- **條件數**: 3 (啤酒、可樂、綠茶數量)
- **動作數**: 5 (優惠組數、總金額、總優惠、結帳金額、允許判定)
- **規則數**: 14
- **測試案例**: 14

🎯 測試類型分類
1. **正常結帳流程** (R1-R6): 空購物車、啤酒 1-10 件
2. **啤酒超限測試** (R7-R8): 11-12 件
3. **非優惠商品** (R9-R10): 可樂、綠茶
4. **混合購物** (R11-R12): 多商品組合
5. **其他商品超限** (R13-R14): 可樂、綠茶超限

🎁 每個測試案例包含
- ✅ Given-When-Then 格式
- ✅ 明確的測試資料 (輸入/輸出)
- ✅ 詳細的金額計算明細
- ✅ 業務規則驗證點
- ✅ API 呼叫序列建議
- ✅ 測試重要性說明

---


實際上的 test case 我也貼一下, 其實也寫的很不錯, 我的要求都有做到｡ 總共有 14 份, 我貼一份當作代表就好, 我貼 R2:

(為了不影響文章結構, 我移除了 markdown 的章節資訊)


---
**TC-02 單件啤酒無優惠**

**測試目標**
驗證未達優惠門檻時，系統按原價計算，確認優惠規則不會誤觸發。

**前置條件 (Given)**
- 使用者已登入 (username: `andrew`, password: `1234`)
- 購物車內商品:
  - 啤酒: 1 件 (單價 $65)
  - 可樂: 0 件
  - 綠茶: 0 件

**測試步驟 (When)**
1. 使用者將 1 件啤酒加入購物車
2. 使用者進行購物車結帳
3. 系統計算結帳金額

**預期結果 (Then)**
- ✅ 系統允許結帳
- ✅ 啤酒按原價計算 (無優惠)
- ✅ 總金額: $65
- ✅ 總優惠: $0
- ✅ 結帳金額: $65
- ✅ 訂單成立


**測試資料**

輸入參數
- 啤酒數量: 1
- 可樂數量: 0
- 綠茶數量: 0

預期輸出
- 允許結帳: ✅ 允許
- 啤酒優惠組數: 0 (未達 2 件門檻)
- 總金額 (未折扣): $65
- 總優惠: $0
- 結帳金額: $65

金額計算明細
```
啤酒: 1 件 × $65 = $65
可樂: 0 件 × $18 = $0
綠茶: 0 件 × $25 = $0
----------------------------------------
總金額 (未折扣): $65
啤酒優惠組數: floor(1/2) = 0
總優惠: 0 × -$26 = $0
----------------------------------------
結帳金額: $65 + $0 = $65
```

**業務規則驗證**
- ✅ 單件啤酒不觸發優惠 (需 2 件才享第二件六折)
- ✅ 優惠門檻控制正確

**API 呼叫序列**
1. `add_to_cart` - 加入 1 件啤酒
2. `get_cart` - 取得購物車內容
3. `calculate_total` - 計算結帳金額
4. `checkout` - 執行結帳

**測試重要性**
確認優惠規則不會誤觸發，保護商家利益。這是優惠計算的基準線測試。
---





# 3. 讓 AI 真的把 Test Case 跑起來

其實在做這個 side project, 最有成就感的環節就是, 你把一切都準備好, 然後看著他按照你要求動起來的那瞬間, 就像你疊完整屋子的骨牌, 然後推倒的那一瞬間一樣... 

前面產完的 test case, 現在這步驟就要開始執行了｡ 來複習一下, 目前手上我有哪些測試相關的資訊:

1. test case (要測什麼的業務情境)
1. api spec (api 的操作規格, 也就是 swagger)
1. text-to-calls mcp (就是上一篇文章的 PoC 的程式碼, 重新封裝成 mcp server)

理論上有了 (1) + (2), 就有足夠資訊推導出 api 該怎麼呼叫了, 而 (3) 就是讓 AI agent 能真正自動化執行它的工具 (這是給 agent 的工具, 不是給我)

現在萬事具備了, 我就開始啟動它｡ 一次跑完太花時間了, 既然我前面都讓 AI 幫我標記重要性了, 我就挑選第一組: 正常結帳流程 (R1-R6) 來示範｡


## 3-1. Text2Calls MCP

我在 copilot chat 視窗下了這道指令:

```
/testkit.api.run

執行: 正常結帳流程 (R1-R6)
```

整個程序就開始啟動了, 開始之前, 我簡單交代一下我的 MCP 做了什麼... 首先, 我封裝了 4 個 tools 給 agent:

- QuickStart
- ListOperations
- CreateSession
- RunStep


Agent 第一次使用, 就按照 tools description 的指示, 先呼叫 QuickStart. 其實這做法是從 Shopify 學來的 (參照我 facebook post), 我用 tools 的方式傳回明確 (而且可以是動態的提示) 的使用說明給 Agent, 最大化的正確引導 agent 來用我的 mcp:

![alt text](/images/2025-11-10-vibe-testing-workflow/image.png)


接下來, agent 果然按照 QuickStart 的指示, 先查詢 api 可用的操作有哪些了:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-1.png)

這邊特別說一下, 我的 mcp 目的是要將 api 呼叫抽象化｡ 我曾經實作過一個版本, 直接將 api 的 swagger 所有規格都直接提交出去, 不過這時我就在想, 這樣的話我還需要多包一層 mcp 嗎? 市面上有現成的 swagger -> mcp 可以用, 我其實不必要自己寫｡ 不過實驗之後, 我放棄這條路了, 因為:

- api 包含太多不必曝露給 agent 的細節
- api spec / request / response 資訊過多, 呼叫一兩次就把 agent 的 context window 塞爆了
- api 包含太多不適合給 agent 處理的動作 (尤其是機械式的操作, 例如 oauth2)

因此, 我想清楚我自己包一層 mcp 的重點了, 核心任務就是將 "呼叫 api" 這件事抽象化, 讓主要的 agent (在這裡就是 github copilot + claude sonnet 4.5) 能夠專注在主要任務 - 執行 test case 上, 排除一切干擾 (包含不必要的 context, 不必要麻煩他操作的細節)

所以我的抽象化切角就是:

- ListOperations, 用文字敘述 (只有 api-name 是明確 id) 告訴 agent 有哪些 api 可以用
- CreateStssion, 明確告訴 agent, 建立 session 來收納一個 test case 的執行過程記錄
- RunStep, 接受 agent 的指示, 在當前的 session 內要執行指定的 operation, 並且用文字敘述告知 mcp 操作的內容 ( mcp 內部還有另一個 LLM, 會真正負責解析 text -> api call 的參數對應跟生成)

經過這樣拆分, 整個 testkit + mcp 的執行才開始順暢起來, 先前的版本總是卡在奇怪的地方, 然後 agent 就暴走亂回答一通了｡ 這是某種形態的 context engineering, 有效率的控制主要 agent 的 context window 不要被干擾的案例, 這也是某種形式的 sub-agent 縮影, 只是中間通訊的 protocol 是 mcp (model context protocol), 而不是 a2a (agent to agent protocol) 而已.

也因為 RunStep 做的事情是翻譯 text -> api call, 這個 mcp server 我才會取名為 Text2Calls MCP.

Agent 透過 list operations, 並且對照 TC-01 test case, 列出可用的操作有這四個:

- GetCart - 取得購物車內容
- CreateCart - 建立新購物車
- EstimatePrice - 試算結帳金額
- CreateCheckout - 建立結帳交易

用這四個操作就足以組合出 test case 01 的情境了, 接下來 agent 真正要開始執行, 於是就先執行 CreateSession:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-2.png)

建立完 session, 其實 MCP 在背後也先把執行過程中必要的 OAuth2 認證先跑完, 並且順利取得 access token .. 這些操作如果不是在 MCP 內用 code 處理, 我不知道還要寫多少 prompt 才教的會 AI 來做這些事...

![alt text](/images/2025-11-10-vibe-testing-workflow/image-3.png)


接著, 開始真正執行 RunStep, 呼叫 API:

// ![alt text](/images/2025-11-10-vibe-testing-workflow/image-4.png)

![alt text](/images/2025-11-10-vibe-testing-workflow/image-5.png)

我特別貼一下 agent 傳給 RunStep 的資訊, 都被抽象化了, 只剩下 operation, action, context:

```json
{ 
    "operation": "CreateCart", 
    "action": "create an empty cart", 
    "context": "User andrew logged in. Create a new empty cart to test checkout with no items. This should later fail at checkout." 
}
```

基本上 agent 只是交代 mcp 要 run 哪個 api (operation), 想要他完成的任務 (action) 是什麼, 完成這任務的背景是什麼 (context), 其餘就是 mcp 按照這些資訊, 搭配 mcp 內部自己 own 的 LLM, 觸發 function calling 機制來完成 api call 的任務

RunStep 這操作, 執行過程中可能會呼叫 1 ~ N 次不同的 api, 這些過程都會在 _session 目錄下留下足跡｡ 舉例來說 RunStep -> CreateCart, 就真正對應到 POST /api/carts/create, 具體的 api call request / response 都有被記錄下來:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-6.png)

而抽象化的執行順序, 也會被記錄在 session-logs.md 下 (我節錄 RunStep[1] 的片段, 這些都是 mcp 自己留下的記錄):

---

**RunStep[1], CreateCart**

**action**:
create an empty cart

**context**:
User andrew logged in. Create a new empty cart to test checkout with no items. This should later fail at checkout.

- http(5) Post https://shop.chicken-house.net/api/carts/create => 201 Created
  - [request](005-api-request.txt)
  - [response](005-api-response.txt)

**result**:

**answer**:  
已成功建立一個空購物車（id=163）。執行成功且結果如預期：回傳的購物車內容為空（prodQtyMap 與 lineItems 均為空），可用於後續驗證「空購物車結帳應被拒絕」的測試。

**details**:  
依據執行結果，API 回應內容如下（已轉成可讀敘述）：
- 請求：POST https://shop.chicken-house.net:443/api/carts/create
- 回應 Content-Type: application/json; charset=utf-8
- 回應 JSON 內容：{
    "id": 163,
    "prodQtyMap": {},
    "lineItems": []
  }
敘述說明：系統已建立一個新的購物車，購物車編號為 163；目前購物車內沒有任何商品（prodQtyMap 空物件，lineItems 為空陣列），符合「建立空購物車」的需求與測試前置條件。無 HTTP 錯誤或例外回傳，操作順利完成。

**instructions**:  
下一步建議：
1) 嘗試呼叫 CreateCheckout（payload: { cartId: 163 }）以確認系統在建立結帳交易階段是否拒絕空購物車。期望行為：API 應回傳錯誤（或拒絕建立交易），並說明原因（例如：「購物車為空，無法結帳」）。
2) 若 CreateCheckout 被接受（意外情況），請接著模擬付款並呼叫 CompleteCheckout，檢查是否會在完成結帳時被阻擋；若 API 回傳成功建立訂單，視為系統行為不符合預期，需進一步調查。 
3) 測試檢查要點：確認 API 回傳的錯誤碼與錯誤訊息，並在測試結果中斷言系統應拒絕空購物車結帳以防無效訂單產生。
4) 若需要，我可以代為執行 CreateCheckout 並回報其實際回應內容以完成此測試情境驗證。

**summary**:  
前次 context：使用者 andrew 已登入，目的是測試空購物車結帳場景（購物車內無任何商品，系統應拒絕結帳並顯示錯誤訊息）。
此次 action：CreateCart — 建立一個新的空購物車。
此次 answer：建立成功，系統回傳購物車 id=163，prodQtyMap 與 lineItems 均為空，符合建立空購物車的預期。後續請嘗試呼叫 CreateCheckout (cartId=163) 以驗證系統是否正確阻止空購物車結帳；我可以協助執行並回報結果。


---


## 3-2. Session Result

介紹完 Text2Calls MCP 的設計與運作方式之後, 實際上還是要來看看執行的成果｡ 先來看單一測試案例 TC-01, 空的購物車應該要拒絕結帳才對, 不過實際上我的 api 並沒有做這限制, 所以我預期這個測試應該會成功的失敗才對 (咦?


測試的 test case 主要部分如下:

---

**前置條件 (Given)**
- 使用者已登入 (username: `andrew`, password: `1234`)
- 購物車為空
- 購物車內商品數量:
  - 啤酒: 0 件
  - 可樂: 0 件
  - 綠茶: 0 件

**測試步驟 (When)**
1. 使用者嘗試進行購物車結帳
2. 系統檢查購物車內容

**預期結果 (Then)**
- ❌ 系統拒絕結帳
- ✅ 顯示錯誤訊息: "購物車為空，無法結帳"
- ✅ 不產生訂單
- ✅ 購物車狀態維持為空

---

實際執行的步驟, 到 Step3 之前都還正確 (建立購物車, 計算購物車金額-空車):

![alt text](/images/2025-11-10-vibe-testing-workflow/image-7.png)


然而, 到了真正要執行結帳的時候, api 呼叫就不如預期了:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-8.png)


看到這邊, 可以放心了, agent + mcp 有認真替你執行 test case 的每個步驟, 也有確實檢驗結果是否符合預期｡

## 3-3. Test Suite Result

看完一筆 test case 執行過程, 其他剩下的 test case 我就跳過了, 直接看最後結果 (我只 RUN 了基本測試, TC-01 ~ 06):

我有實作基本功能, 但是基本上邊界的檢驗都沒做, 所以基本測試 TC-01 空購物車結帳測試預期會失敗, 其他正常結帳跟折扣計算應該都會通過 (TC-02 ~ 06), 如果後面還有執行超過 10 件商品的測試應該也會失敗 (應該要拒絕結帳, 但是實際會結帳成功), 這些都驗證測試有發會效用｡ 我擷圖讓各位看一下 AI 給我的摘要報告:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-9.png)

針對測試失敗的部分 (TC-01), AI 也特別說明他判定問題出在哪邊

![alt text](/images/2025-11-10-vibe-testing-workflow/image-10.png)

而這些執行過程的記錄, 通通都鉅細靡遺的保存在 session folder 內:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-11.png)

001, 002 -> oauth 認證過程
003 -> 下載保存 openapi spec
004 -> mcp 呼叫 LLM 處理 function calling
005 -> call api: create cart
... (懶的寫了, 總之當成 postman 來看待就好)

openapi-spec.json, 就是 003 保存下來的 api spec 快照
session-logs.md, 就是呼叫過程中的抽象資訊傳輸記錄, agent / mcp 的通訊內容都再這邊, 基本上這是給 "人" 看的歷程...

而我會設計 mcp 要保留這些資訊, 目的很單純, 這些都是將來要寫 "api 自動化測試" 最重要的規格, 以後這些都會是餵給 spec-kit 執行的養分｡

不過, 這篇我不打算示範到 test code generation, 有興趣的可以自己先拿去玩玩看, 或是等我下一篇文章再來介紹｡



# 4. 同一份 Test Case, 同時用於 API 與 UI 測試

前面我提到, 我希望 test case 能排除操作細節, 方便 review 以及降低維護文件的負擔｡ 而被省略的部分, 可以讓 AI 從 API spec 來還原需要的操作細節 (前面 #3 已經證實可行)｡

另一個目的, 則是測試目標的可抽換特性｡ 如果我的服務, 同時提供 web 介面, 也提供 api 介面, 理論上這些測試案例應該都是共通的啊 (不同平台買到的東西, 價格跟內容應該都要一樣才對, 除非特別設計為通路限定), 這一段, 我就拿一模一樣的 test case, 把 mcp 從我自己寫的 Text2Calls MCP 換成 Playwright MCP, 看看能否完成同樣的測試行為?

拜 coding agent 所賜, 我前端大外行, 終於也能生出還過的去的純前端測試網站了 XDD, 這是 React + NodeJS 的前後端架構, 背後 call 同樣的 API 搭建的 web app, 有興趣的人可以進去玩玩看:

Andrew Shop: https://shop.chicken-house.net

這次, 同樣來看看我準備了什麼資訊:

1. test case (要測什麼的業務情境, 同 api)
1. ui spec (我沒有提供, 但是我開發的 web ui, 特別遵循無障礙的設計, accessibility, 後面說明)
1. playwright mcp

mcp 的安裝設定我就不講了, 大家自己看 playwright mcp 的官網, 我開啟 copilot chat 輸入這段指令:

```
/testkit.web.run

執行: 正常結帳流程 (R1-R6)

```

這段我沒有另外再寫 MCP 來支持這些操作, 例如 session, logs 等等, 因此這部份就簡單處理 (我直接寫在 prompt 而已), 所以 agent 一樣會幫我建立 session folder, 只是是靠 shell ..

草草的準備好測試環境之後 (咦? , 正式的測試就要開始了, 第一次呼叫 playwright mcp, 就是啟動首頁:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-12.png)


經過一連串的嘗試 (略過), 完成 OAuth2 認證, 登入成功:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-13.png)

接下來你可以看到 agent 不斷的在跑, 而 chrome 也自己在操作, 等了幾分鐘之後, 開始測試結帳:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-14.png)

看起來操作都很順利, 操作的過程都沒有脫離 test case 的敘述, 跑了大約 20 min (真的很慢), 六個測試項目執行完畢｡

比較不一樣的結果是: 這次六個測試都成功通過!

![alt text](/images/2025-11-10-vibe-testing-workflow/image-15.png)

這並不是測試結果不正確喔, 我在刻這個 web ui 的時候, 我在 api 層級沒有加上的限制或是檢查, 我都有在 ui 這層補上｡ 因此 TC-01 空的購物車不能結帳, 這動作在 UI 的確是有實作的, 我擷圖證實一下:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-16.png)

購物車是空的時候, 結帳的按鈕整個被藏起來沒辦法按, 所以 TC-01 測試通過


## 4-1. API 需要 SPEC, UI 不需要?

同樣的, 這題也是過去我卡很久的題目...

不知道有多少人認真拿 playwright mcp 來做類似的事情?

簡單的網頁都很正常, 複雜一點的, 有的光是 login 就卡半天了, 要嘛就是找不到按鈕再哪裡, 要嗎就是按不到, 總之就是各種詭異的狀況, 明明按鈕或是欄位就在那邊, 你看的到, 但是 AI 就是看不到

其實這就是 "規格" 啊, 人看的到, 是因為最終是從 "視覺" 上來判定的, 但是 AI 不是｡ 其實有 solution, 的確是擷取畫面, 然後進行影像辨識來判定的, 這技術可行, 不過運算資源花太兇了, 現在的運算能力還沒辦法做到即時 (一張 4K 的圖, 要判定按鈕再哪, 該不該按下去, 就要幾十秒鐘了)

我試過其他的套件, 例如 selenium mcp, 它的辨識能力就 "好的多", 因為他直接判定 HTML, 基本上頁面所有細節都能檢測的到, 不過 HTML 太肥了, 動不動就 100kb 以上, 很快的 agent 的 context windows 就被佔滿了, 根本無法實際使用

playwright mcp 實用的多, 因為他會先將 HTML 精簡成自訂格式的 yaml 結構, 只是他是如何判定哪些資訊該保留下來? 我爬了很多文, 最後是厲害的同事逆向工程挖出來的, playwright 靠的是 "無障礙" 的網頁標記資訊｡

這招其實很聰明, 我聽到關鍵字 "無障礙" (Accessibility) 我就想通了, 先前看 A16Z - AI Era 趨勢報告, 就提到這一段: [# 6, Accessibility as the universal interface]

跟人類的感官相比, 現在的 AI 對於你的電腦, 就像個殘障人士.. AI agent 既沒有辦法幫你操作鍵盤滑鼠 (除非你授權), 也看不到你畫面上現在呈現的是什麼 (除非你授權擷圖), 沒辦法聽到你在聽什麼音樂, 沒辦法 ...., 而無障礙的設計, 就是讓應用程式能讓這些人也有機會 "看到" 網頁的內容的設計, 看不到字, 可以用唸的, 看不到圖, 可以用文字描述, 唸給你聽..

講到這邊, 串起來了嗎? 如果你的網頁有確實做好 "無障礙" 設計的話, 對於 AI 這種視覺障礙的人士, 它就多了一個能理解網頁資訊的管道了｡ 而 playwright mcp 也大量依靠這些標記來 "認識" 你的網頁怎麼操作｡ 因此, 我的 Andrew Shop WebUI, 除了 OAuth2 Login 那一頁還沒修正之外 (所以 login 的地方卡比較久), 其他都是這次新開發的, 我在開發之初, 就明確的用 spec-kit 說明, 全站都要遵守無障礙的設計原則 (不過具體是什麼我也搞不懂 XDD), 換來的就是 AI 能夠清楚辨識該如何操作我的頁面了

簡單摘要一下, 我嘗試過的幾種 讓 AI (use playwright mcp) 能 "認得" 你網頁方法:

1. 不處理, 直接上
1. 做好無障礙的設計
1. 在 testcase 標記 HTML id



有興趣的人可以試試看, 我把這個測試專案放到 GitHub, 你可以用 specstory 看我跟 agent 的對談記錄, 也可以看測試產生的測試報告, 你會發現雖然 UI test 跑很慢 (6 個測試案例就花掉 2x 分鐘)



# 5. 結語: 為 AI 設計新流程, 而不是替舊流程加速


>>>

回頭看這次的實驗，其實我做的事可以粗略拆成三層：

1. **觀念層：先承認「測試通膨」是結構問題，不是人力問題**  
   只要還是用「每個 AC 展開一大堆 test case、每次 release 全部重跑」這種思維，再多 AI 也只是在幫你撐一陣子。  
   我一開始用 Vibe Testing 直接讓 AI 代跑測試，看起來很帥，但只要算一下量體和 token 成本，就知道這條路撐不久。

2. **流程層：把 AI 放在「左移」的位置**  
   - 用 Decision Table 把測試空間想清楚  
   - 讓 AI 幫忙把 decision table 展開成成套的 test cases  
   - 再讓 AI 去「探索」一次 API / UI，把探索過程變成操作範例與 log  
   也就是：**AI 重點不是在「多跑幾次」，而是在「幫你產出可以重用的自動化資產」**。  
   後面重跑的部分，交給機器、CI/CD 去處理就好。

3. **實作層：刻意留出讓 AI 好發揮的介面**  
   - API 這邊用 Text2Calls MCP，把「怎麼 call API」抽象成 operation + action + context  
   - UI 這邊遵守 accessibility 的設計，讓 Playwright MCP 看得懂你的頁面在幹嘛  
   - 所有執行過程都留下規格化的 log，未來可以直接餵給 Speckit 這類工具產生自動化 test code  

這套 workflow 對我來說，有幾個很明確的收穫：

- 我更少在跟 AI 對話「請幫我多跑幾次 XXX」，而是把心力放在「這個 decision table 長得對不對」  
- 我開始刻意設計 API / UI 的「可測試性」，而不是等系統做完才問：欸這要怎麼測？  
- 我可以接受某些地方先暫時用 AI interactive 模式跑，等 log 穩定了，再補自動化測試，而不是一開始就企圖從 0 到 100 分

當然，這篇還只是半場而已。  
真正有趣的下半場，是把這些執行記錄、decision table、test case 規格通通餵進 Speckit 之類的工具，讓它幫我把「會動的自動化測試 code」生出來，然後直接掛進 CI pipeline 裡。

如果你現在也在推 AI 導入，尤其是測試／QA 流程相關，我會建議你可以先問自己這個問題：

> 我現在做的是「讓 AI 加速舊流程」，  
> 還是「因為有了 AI，所以我敢把流程拆開重組」？

這篇文章就是我的一個版本答案，後面還會繼續修正，但至少，我已經不太滿足於「多了一個 AI 小幫手」的那種導入方式了。



>>>

