---
layout: post
title: "AI-First Testing, 以 AI 為核心重新設計測試流程"
categories:
- "系列文章: 架構師觀點"
tags: ["架構師觀點","技術隨筆"]
published: false
comments_disqus: false
comments_facebook: false
comments_gitalk: false
redirect_from:
logo: 
---

首先, 我自己打臉我半年前寫的那篇 [聊聊 Vibe Testing 實驗心得](/2025/05/01/vibe-testing-poc/) 的文章｡ 其實那篇文章研究的 "自動執行 API 測試" 是可行的, 但是應該拿來探索測試步驟用, 而不是拿來當最終重複執行用｡ 當時我的思路是:

*" 現在的流程太花人工了, 那我讓 AI 替代人工來跑測試吧 "*

結果, 不知不覺就掉到這 "局部最佳化" 的陷阱裡.., 手上有威力很強的新武器, 我卻還在用傳統作戰的戰術去運用它... 有效是有效, 但是總覺得沒有完全發揮出應有的戰力...

所以我在想, 如果我重新設計工作流程, 是否這一切都會變得簡單且有效率? 因此重新想過一輪之後, 這篇我想談: 

*" 從 AI 的能力出發, 重新思考 "測試" 這件事應該怎麼進行? "*

這陣子思考 + 嘗試實作, 有了一點心得, 我換了個方式拿 AI 來進行軟體測試, 這次的效果比我預期的好, 於是就有了這篇...｡

<!--more-->

我想起 Sam Altman 前陣子的專訪, 有一段話讓我印象深刻, 他提到不同世代的人, 對 AI 的使用方式會有很大的差異, 而其中提到這句話:

*20 歲的年輕人, 把 ChatGPT 當成 "作業系統" 來使用...*

原文如下 [Sam Altman says young people use ChatGPT as an operating system
](https://youtu.be/uVEjlRK0VWE?si=jDQyFJj2n8m4zrug&t=4):

> "  
> Older people use ChatGPT as a Google replacement.  
> People in their 20s and 30s use it as a life advisor.  
> People in college use it as an operating system.  
> "


什麼叫做 *"把 AI 當作 OS 來使用"* ? 對應到我這次的主題, 我應該思考的是:

> "  
> 如果我想從 AI 已經是作業系統般的存在時, 那測試流程會變成什麼樣子?  
> "

回到 30 年前的 DOS 時代, 那時的 OS 沒那麼複雜, 就是處理資料 (file) 跟執行應用 (.exe) 而已, 所以才叫做 DOS ( Disk Operation System ), 而現在的 coding agent, 不就也是同樣的模式嗎? 大部分情況下都是處理資訊 (對話) 跟調用工具 (MCP), 而重點則是你所有事情都依賴他來處理了

這就是使用深度的差異吧, developer 這族群, 算是目前工作流程被 AI 影響最巨大的族群了, 我看到很多人已經拿 Claude Code 這類 CLI Tools 來當作它們主要的工作介面, 不只用來寫 code, 也用來寫文件, 更用來做各種其他任務或是自動化..., 而回到半年前我用 vibe testing 的角度來思考測試這件事, 我發現我的思路是:

> "  
> vibe coding 很炫, 那我是不是也能 vibe testing 呢?  
> 目前的測試都需要仰賴人工操作, 那我是不是也能用 AI 來代替人工操作呢?  
> 如何讓 AI tools 無痛的融入現有的測試流程, 解放過去投在裡面的人力??  
> "

其實這些思路都沒錯, 也都得到效率的提升, 但是我總覺得不對勁, 因為最關鍵的流程 (workflow) 並沒有改變, 只是把 "人" 換成 "AI" 而已｡ 如果 AI 的能力已經是個 "變革", 其實整個環節搭配的平衡都已經改變了, 流程勢必要重新檢視一次才對｡ 人類有擅長的能力 (Brain), AI 也有他擅長的能力 (GPU), 而傳統的應用程式也是 (CPU), 能鑑別出這些 "運算資源" 的使用範圍跟邊界, 適當的資源配置, 然後重新設計流程, 才是最重要的事情｡

過去軟體開發的流程, 最大瓶頸就是 "人寫 code 的速度太慢", 所有流程都圍繞著這點在強化跟改善｡ 突然間出現了 coding agent, 讓 "coding" 這件事突然變成 10x, 平衡被打破了, 瓶頸轉移到需求跟規格不夠清楚, 所以現在軟體開發領域的流程整個被重塑了, 開始也有 SDD 這類嚴謹的流程被提出來..., 這些都是 "基本的流程" 正在被改變的現象｡

因此, 距離我上一次談 vibe testing 這題目隔了五個月, 我重新想這題目的同時, 也研究了 MCP design, Context Engineering, SpecKit 等等其他題目, 重新用 "TestKit" 的想法來包裝 AI-First Testing 這題目, 成果還蠻令人滿意的, 於是整理了這篇文章來分享一下我的心得｡

接下來的文章內容, 我會在第一段先介紹想法的轉變, 然後把測試流程拆成三個關鍵步驟來示範 TestKit:

- 決定你該測什麼: 用 AI 把需求展開成 Decision Table, 輔助專家把「對的測試」定義清楚
- 決定你該怎麼測: 把 AI 放在「探索 + 產生自動化測試程序」的位置，而不是讓它去負責執行測試  
- 決定如何自動化: 把驗證過的測試程序自動化，讓 AI 精準地寫出自動化測試的程式碼

而把這些基礎都做好，還有附加的效益:
- 讓 test case 能重複運用, 搭配不同操作介面的規格，可以同時覆蓋 API / UI 的測試需求
- 不再需要維護多份同樣邏輯不同操作介面的測試案例，降低測試文件的數量級




# 1. AI-First Testing Workflow

概念談完了, 這段就來聊聊對於 AI-First Testing 工作流程的想法吧, 我會把想法從 "理想" 收斂到 "具體可行" 的流程為止, 而文章的後半段, 則是敘述依據這些想法實際做出來的 side project 展示可行性｡


"測試" 要做的好, 其實是很困難的一件事, 第一個痛點就是: 測試的 "量體" 太大了, 這代表有寫不完的測試案例, 準備不完的測試資料, 以及跑不完的手動 & 自動測試｡ 即使有 AI 加持, 你要測完所有的狀況也幾乎是不可能的事情, 勢必要有一些方法, 有效率的展開測試案例, 並且優先完成重要有價值的測試才是正途｡


為了聚焦我的主題跟思路, 不相關的環節我就大幅簡化了, 我用這張圖來說明:


![alt text](/images/2025-11-10-vibe-testing-workflow/image-17.png)


從左到右, 是從需求規格中的驗收條件 (AC, Acceptance Criteria) 開始, 到測試結果的流程｡ 每個步驟上面標的數字, 就是可能要展開的數量 (數量級)｡ 

試著估算看看: 

*如果系統需求列了一項 AC (ex: 購物車結帳時必須檢查折扣計算規則, 以及購買條件限制), 最終應該展開多少筆 Test Cases ? 在系統的生命週期內你需要執行幾次測試? 成本與代價是什麼?*


**測試量體估算**:
- 每 1 個 AC 平均展開 10+ 個 Test Case
- 系統有 4 種操作方式 (操作: Web UI, Android APP, iOS APP, REST API)
- 系統有 10 種非功能性需求 (NFR, Non Functional Requirement, 如資安, 授權, 效能等等)

**測試執行瓶頸**:

1. **測項數量**:  
如果這些組合都展開, 每一個 AC 就必須要維護 4 + 10 + 10 份規格文件, 以及產生出來的 1 x 4 x 10 x 10 = 400 份不同的測試腳本... 這量體, 光是文件就已經難以維護了.. 過去需要人工撰寫, 而現在即使使用 AI 來撰寫, 這麼多的數量就算只要 Review 也是一大負擔。這是第一個瓶頸｡
1. **探索步驟**:  
測試案例通常需要由人工將測試要踩過的商業情境, 轉化成系統的操作步驟。這是另一個非常花費人力的環節。AI 現在有理解意圖的能力, 如果有明確的操作規格 (api spec), 搭配只保留抽象敘述的測試案例, AI 是有能力幫你探索出操作步驟的, 但是這需要花時間讓 AI 理解規格, 並且嘗試呼叫 api 來確認是否完成任務｡ 這是第二個瓶頸｡
1. **執行次數**:  
就算真的有 400 份測試案例, 一年你會需要跑幾輪測試?  
假設一週 release 一次, 每次 release 前內部 QA 至少測一次, release 後正式環境也至少測一次, 那一年應該要跑 50 x 2 = 100 輪, 總共是 400 x 100 = 40,000 次測試... 要有效率, 並且確實, 用合理的成本執行他們, 是第三個瓶頸｡

**試算執行成本**:  
一年執行 40000 次測試, 若用 AI 來自動執行, 需要花多少錢? 
如果我真的用 vibe testing 用 AI 當下執行 API test, 每跑完一次約需要 2 分鐘, 對應的 api token 約 USD$ 0.03 (剛好約 NTD$ 1.0) 左右, 一年要花 2 x 40000 = 80000 分鐘來執行 (55.56 天), 整個過程共計要花掉 4 萬台幣的 token 費用... 而這只是完成 "一個 AC" 的要求需要的測試而已...

簡單的計算, 就知道這方法還無法實際應用｡ 雖然比單純用 "人力" 來測試已經好很多了, 但是遠遠跟不上測試的通膨啊..., 而且 AI 執行測試也有缺點, 除了速度跟費用之外, AI 每次執行的結果都有一些差異, 這也是另一個頭痛的問題｡ 

合理的解套方式, 應該是 "寫自動化測試程式" 才對, 因為他花的是 CPU, 不是很貴的 GPU, 也不是很有限的 Brain (人腦)... (成本: Brain >> GPU >> CPU) 因此整個流程應該導向 "探索" 的結果如何有精準有效率的轉化成 "自動化測試" 程式?

這就是 AI 自動化測試 "左移" 啊, 讓 AI 專注 "探索" 測試步驟｡ 當探索確定後, 剩下單純 "自動化" 的要求, 則再交給 AI 一次性的翻譯成對應的自動化測試程式就好, 這樣既提高效率, 降低成本, 同時也能確保每次執行的測試步驟都會一模一樣 ｡

同樣的, 測試案例的數量問題, 也能 "左移"｡ 如果我能將 AC + 決策表維護好, 是否就能穩定的輸出展開的 test cases? 如果 test case 夠單純, 不涉及操作細節, 需要的當下再靠 AI 的理解能力, 搭配操作規格 (api spec) 就能展開操作步驟的話, 是否就能讓 test case 高度複用?

這些流程越想越清晰, 越能掌握關鍵, 因此既有流程該調整的是:

1. **"文件的維護" 左移**:  
驗收條件 (AC) 的展開, 用一張決策表 (decision table) 來處理, 就能框出測試範圍以及關鍵的控制變因; 而展開的測試案例 (test case) 若能維持抽象層級, 就不需要為了各種操作介面重複產生不同的測試案例了｡ 如果這些都有做到, 我需要維護的是 (decision table) x 1 + (test case) x 10, 而不是原本要維護的 4 (操作方式) x 10 (測試案例) x 10 (非功能性需求) = 400


2. **"自動化執行" 左移**:
自動化執行再拆解, 困難的地方有兩個, 一個是探索 (看規格跟情境, 對應出操作步驟的過程), 這在過去通常都要靠工程師的經驗來決定｡ 而另一個環節, 則是讓他自動化執行｡ 對比每次都用 AI 或人工執行 40000 次測試, 花費的是 40000 x GPU; 而改善過的方法, 你需要執行 400 次探索 (GPU), 400 次測試程式碼生成 (GPU), 40000 次自動化測試 (CPU), 最大的瓶頸跟部確定性, 已經簡化成一般的程式執行了, 從 40000 次 AI 任務, 降低為 400 + 400 次 AI 任務


改良過的流程, 我用這張圖來表達:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-18.png)

剩下的就是親自找一個實際案例來驗證看看了, 這也是這篇文章我想要展示的主要內容｡



## 1-1. TestKit 的構想

接下來, 這流程跟想法, 我想要把他具體化一點, 我直覺想到的就是 SpecKit 的做法, 我自己仿照那個樣貌, 弄了一套簡單的 TestKit 來封裝我想像的 AI-First Testing Workflow. 


前面拆解的步驟, 我用系統的角度再重新定義一次 (括號裡面代表的就是 TestKit 對應的指令):

1. **Generate Test Case from Decision Table** (TestKit.GenTest)  
展開測試: 將 AC 展開成 decision table, 並且決定測試範圍, 收斂出有價值的測試, 並將之轉成可用的 test case(s).  

1. **Explore Test Steps with AI Agent** (TestKit.API.Run / TestKit.WEB.Run)  
探索執行步驟: 讓 AI 探索測試的執行步驟, 詳細記錄執行過程, 並確認測試結果。符合的結果的執行過程, 能當作後續自動化的重要參考。  

1. **Generate Test Code from Test Case & Test Steps** (TestKit.API.GenCode / TestKit.WEB.GenCode)  
自動化: 將 (1) 及 (2) 獲得的成果, 透過 AI 生成能重複執行測試的 test code.


畫成流程圖, 以 "產出" 文件的視角來看, 大概就是這樣:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-19.png)

每個步驟, 都有他主要的目的, 也有關鍵的產出, 我列一張對照表:

| 編號 | 目的 | 產出文件 | 檢核項目 |
|------|------|---------|--------|
| 1 | 決定測試的範圍 | decision table, test case(s) | 條件組合完整性、測試案例邊界值 |
| 2 | 探索測試的步驟 | test session log, summary | 執行步驟正確性、結果驗證精準度 |
| 3 | 自動化執行測試 | test code | 程式碼執行穩定性、正確性 |

構思到這邊, 我想像的流程大致都完善且確認下來了, 接下來就是想辦法實作出來, 證明他的可行性。實際操作這個流程:

1. 靠 AI agent 來輔助 "專家 (真人)" 來做決策, AI 負責內容整理及輸出｡
1. 靠 MCP 來輔助 AI agent 進行探索, 實際操作系統, 不斷嘗試與修正, 直到能順利完成為止。 MCP 需要支援步驟的記錄, 供後續步驟運用｡ 
1. 靠 AI agent + SpecKit 來生成自動化測試程式碼, 生產出大量, 行為一致的自動化測試｡


試想一下步驟 (3), 我需要提什麼 spec, 才能讓 AI 寫出夠理想的測試程式碼? 其實所有必要的資訊, 都包含在 workflow 的每個階段了啊:

1. **操作規格** (以 api 而言, 就是 api spec), 說明操作方式跟細節, 讓 AI 能有跡可尋
1. **測試案例** (test case), 測試本身要做的事情是什麼... (流程已經準備好了)
1. **操作範例** (探索測試留下來的 session log 就是最好的操作範例了)

這些必要的 spec , (1) 是流程中早已準備, (2)(3) 則是流程中間的產物, 其實我都能控制其品質, 有了這些高品質的規格, 要生成穩定可靠的程式碼並不難｡



## 1-2. 小結

關於測試流程的設計, 到這裡為止, 主要問題都解決了, 相關資源的供應鏈也設計好了｡ 想到這邊, 我總算可以說說我對 AI-First (Testing) Workflow 的想法了｡ 在最近將近兩年的時間, 我都跟著團隊推動各種 AI 相關的工作流程改變, 以及各種 AI 產品開發的架構設計, 我始終有一個感覺, AI 實際上帶來的是 "變革", 不是 "改善", 但是大部分的人都會直覺地把它用在 "改善" 的範圍, 我覺得有點可惜, 這樣有很多 AI 的潛力其實都被 "舊流程" 給封印了｡ 

面對的方式, 可以先把 AI 用在 "改善", 能得到立即的效益當然沒問題, 不過別因此而放棄了新流程的研究, 因為只有這麼做, 當 "舊流程" 已經碰到瓶頸的時候, 你才知道 "新流程" 應該怎麼做｡ 這次 AI 變革來的太快, 什麼是正確的 "新流程" 也沒人知道, 一切都要靠自己摸索嘗試, 這才是最困難的環節, 不過也是最有趣, 最有成就感的地方｡

最後, 補上我在 facebook 看到幾篇類似觀點的 PO 文 (當然也包括我自己寫的) 給各位參考:


1. 我在 2025/11/03 的 Facebook [PO 文](https://www.facebook.com/share/p/177qgafB5Z/)  
> AI 轉型，困難的地方在於搭配的流程。因為沒人知道 “AI Native” 的做法是什麼，所以一不小心，你就會掉入 “用新工具來套舊流程” 的困境…。  
> 先前我在研究的 side project: vibe testing 就陷到這困境內了, 總覺得我的作法沒有發揮出 AI 真正的潛力。何謂 AI Native? 用對話就是了嗎? 用了 MCP 就是了嗎? 自己刻 UI 呼叫 AI API 就是了嗎? 想想都不大對，直到前陣子開始認真使用 SpecKit, 我才恍然大悟 ..

2. 我在 2025/10/21 的 Facebook [PO 文](https://www.facebook.com/share/p/1BYCtCqZod/)  
> 做到這邊，我才真正體會到，當你手上掌握的是跨世代的 “新工具"，那你一定要好好思考，是否有比 "舊流程" 還更能發揮工具潛力的新流程可以使用? 想通這些環節，其實成就感比我寫出新工具還要令人興奮 😀



3. 楊大成 2025/10/11 的 Facebook [PO 文](https://www.facebook.com/share/p/1HoM5rq2D9/)  
> AI時代的最大陷阱：別拿新工具去優化舊戰術  
> 最近，我開始制定公司未來三年的 AI 戰略。  
> 但我發現，當 AI 的浪潮來臨時，很多企業的第一反應是：「拿這個新工具來幫我做事更快一點」。例如用 AI 幫忙設計素材、寫文案、生成報告。這種做法當然有用，但可能犯了一個致命錯誤：只把 AI 當成原有戰術的補充，而不是重新構建新戰術體系的核心。  
> 這幾乎是每一個新技術浪潮來臨時，大多數人最容易犯的錯誤。







# 2. 用 Decision Table 定義 "有價值的測試"

前面講完整個流程的想法後, 我重新摘要一下, 我想像的流程怎麼跟系統一一對應, 我列標題就好:

1. **Generate Test Case from Decision Table** (TestKit.GenTest)
1. **Explore Test Steps with AI Agent** (TestKit.API.Run / TestKit.WEB.Run)
1. **Generate Test Code from Test Case & Test Steps** (TestKit.API.GenCode / TestKit.WEB.GenCode)

接下來的實際演練, 我也會按照這三個步驟來逐一說明｡ 第一個就是展開 test case 的過程 - decision table｡ 這邊要面對的第一個大魔王, 是測試的 "量體問題", 而我第一個要解決的題目就是:

> "  
> 我怎麼用最少的文件跟步驟, 決定真正有價值的測試是哪些?  
> "

於是, 經過一番研究, 我找到了 decision table 的做法, 這是老早就存在的技巧, 跟 AI 沒有直接的關聯, 但是當你能用 decision table 說清楚你要測試的範圍時, AI 很容易就能接手幫你完成後面各種繁瑣的苦工..

這環節就是這麼重要, 你可以讓 AI 幫你整理, 但是你一定要 review, 最終的決定權還是在你身上

--

回到起點來看測試: 測試看的就是涵蓋率, 所有的條件組合中, 有哪些情境你的測試會踩過｡ 這時你沒有用有系統的方式把所有組合列出來的話, 其實你很難確認涵蓋率的, 當然你就更難決定哪些條件組合是重要的, 以及是否有漏掉重要的路徑沒安排測試｡

決策表 (decision table), 是有系統的列出所有組合的方法｡ 我第一個示範,就是用 decision table 來讓 AI 替我建立 decision table, 讓我確認我該怎麼測, 該測哪些組合｡

![alt text](/images/2025-11-10-vibe-testing-workflow/image-20.png)

對應到前面提到的流程圖, 那麼這個章節, 我要實作的, 就是藍色框起來的部分, 從 AC 展開決策表, 決定測試範圍, 最後展開成對應的測試案例｡

先講結論, 這是很關鍵的步驟, 你可以大量依賴 AI 幫你整理跟建議, 但是這絕對是要你親自 review 的重要環節, 千萬別讓 AI 列了一版, 然後你看都沒看就回覆 OK... 我自己使用的心得是, AI 替你抓的 decision table 格式通常都正確, 但是他建議的 criteria / action / rule 通常都很糟糕, 你需要花一些心思來 review 這張表格的設計, 這會決定你後面的測試有沒有效率, 有沒有精準涵蓋你的系統流程｡

你需要先掌握 decision table 的使用方式, 你才有能力挑出 AI 的建議哪邊需要修改, 這環節省不掉的, 我強烈建議:

1. 先花一點時間, 好好看懂 decision table 在幹嘛
2. 如果要上課或是找專家, 我推薦敏捷三叔公的文章
3. 你也可以找 AI 教你, 像是 ChatGPT 就有學習模式

接下來, 我就拿我的案例實際操作一次吧! 首先要先準備我自己包裝的 testkit (github repo), 因為是 PoC, 我沒有做太多相關工具, 只有準備關鍵的 prompt, 因此操作要手動...




## 2-1. Init TestKit

有用過 speckit 的大概都清楚我要講什麼, 不過我還沒包裝成套件, 所以... 陽春一點, 就自己把 prompts 跟 mcp.json 複製到你的 workspace 吧, 或是直接 fork 我的 repo: [AndrewDemo.TestKitTemplate](https://github.com/andrew0928/AndrewDemo.TestKitTemplate)


測試的標的, 我就用之前我為了驗證 "安得魯小舖" 而開發的 API, 我簡單做了改版, 目的是用來示範這次的 AI native test workflow.. 

簡單交代幾個 "安得魯小舖" 重要的 url:
- [web ui](https://shop.chicken-house.net/):  我是前端大外行, 這是完全用 vibe coding 生出來的 web ui, 破綻很多, 純示範用途..
- [rest api](https://shop.chicken-house.net/api): 自帶支援 oauth2
- [open api spec](https://shop.chicken-house.net/swagger/index.html)


## 2-2. Prepare Decision Table

首先, 我以 vscode + github copilot 當作我主要示範的環境 (你要換 claude code 也可), 我選用的 model: claude haiku 4.5

```
/testkit.gentest

購物車結帳時, 必須依據購物車內的商品項目與數量，配合折扣規則正確計算結帳金額

目前可接受的折扣規則有：
- 指定商品第二件六折

商店系統有這三件商品上架:
1. 啤酒, 第二件六折, 單價 65
2. 可樂, 單價 18
3. 綠茶, 單價 25

系統限制
- 單一訂單內，每種商品限購 10 個

驗證購物的身分：
- username: andrew
- password: 1234
```


其實我已經把驗收條件簡化到不能再簡化了, 這邊敘述的是我要測試結帳流程, 關鍵的驗證是 結帳金額 (包含折扣) 必須正確, 另外結帳限制也必須符合, 不能結帳的條件必須擋下來 (這部分我 api 沒有實作, 預期這個測試會失敗)｡

這些條件, 關聯到 api 以及背後系統的設定, 這邊我預先都先準備好了 (商品已經先上架, 折扣已經先設定好, 帳號已經先準備好)｡


以下是 AI 給我第一版的 decision table:


| 規則 | C1<br>有啤酒 | C2<br>啤酒≥2 | C3<br>有可樂 | C4<br>有綠茶 | C5<br>數量≤10 | A1<br>啤酒折扣 | A2<br>可樂計價 | A3<br>綠茶計價 | A4<br>正確總額 | A5<br>拒絕結帳 |
|------|:------:|:------:|:------:|:------:|:-------:|:--------:|:--------:|:--------:|:--------:|:--------:|
| **R1** | Y | Y | N | N | Y | ✔ | – | – | ✔ | – |
| **R2** | Y | N | N | N | Y | – | – | – | ✔ | – |
| **R3** | N | N | Y | N | Y | – | ✔ | – | ✔ | – |
| **R4** | N | N | N | Y | Y | – | – | ✔ | ✔ | – |
| **R5** | Y | Y | Y | N | Y | ✔ | ✔ | – | ✔ | – |
| **R6** | Y | Y | N | Y | Y | ✔ | – | ✔ | ✔ | – |
| **R7** | Y | Y | Y | Y | Y | ✔ | ✔ | ✔ | ✔ | – |
| **R8** | Y | N | Y | Y | Y | – | ✔ | ✔ | ✔ | – |
| **R9** | Y | Y | N | N | N | – | – | – | – | ✔ |


這是個符合 "規範" 跟 "格式" 的決策表, 但是對我而言並不是個理想的決策表, 因為條件跟結果過度簡化, 我要安排實際的測試案例不是那麼容易對應｡

先來看條件 (criteria) 部分, 他已經簡化成:
- **C1**: 購物車中包含啤酒商品
- **C2**: 啤酒數量 ≥ 2 件 (可適用第二件六折)
- **C3**: 購物車中包含可樂商品
- **C4**: 購物車中包含綠茶商品
- **C5**: 所有商品單項數量都 ≤ 10 件

而動作 (action) 部分, 也已經簡化成結果, 只陳述有沒有符合:
- **A1**: 計算啤酒折扣 (第二件起六折)
- **A2**: 計算可樂金額 (無折扣)
- **A3**: 計算綠茶金額 (無折扣)
- **A4**: 回傳正確的總金額
- **A5**: 拒絕結帳 (超過購買限制)

因為系統是我設計的, 我很清楚內部運作原理, 以及我想要測試什麼｡ Decision Table 使用技巧我這邊不打算談太多, 有興趣的可以參考我上面給的方式補強｡ 我直接要求 AI 把 decision table 改成我要的樣子:


(對談過程略過, 經過幾輪的對話修正, 我直接列出最終的 decision table 版本)

| 規則 | C1<br>啤酒數量 | C2<br>可樂數量 | C3<br>綠茶數量 | A1<br>啤酒優惠組數 | A2<br>總金額 | A3<br>總優惠 | A4<br>結帳金額 | A5<br>允許結帳 |
|------|:--------:|:--------:|:--------:|:------------:|:--------:|:--------:|:----------:|:--------:|
| **R1** | 0 | 0 | 0 | 0 | $0 | $0 | $0 | ❌ 拒絕 |
| **R2** | 1 | 0 | 0 | 0 | $65 | $0 | $65 | ✅ 允許 |
| **R3** | 2 | 0 | 0 | 1 | $130 | -$26 | $104 | ✅ 允許 |
| **R4** | 3 | 0 | 0 | 1 | $195 | -$26 | $169 | ✅ 允許 |
| **R5** | 4 | 0 | 0 | 2 | $260 | -$52 | $208 | ✅ 允許 |
| **R6** | 10 | 0 | 0 | 5 | $650 | -$130 | $520 | ✅ 允許 |
| **R7** | 11 | 0 | 0 | 5 | $715 | -$130 | $585 | ❌ 拒絕 |
| **R8** | 12 | 0 | 0 | 6 | $780 | -$156 | $624 | ❌ 拒絕 |
| **R9** | 0 | 5 | 0 | 0 | $90 | $0 | $90 | ✅ 允許 |
| **R10** | 0 | 0 | 4 | 0 | $100 | $0 | $100 | ✅ 允許 |
| **R11** | 2 | 3 | 0 | 1 | $184 | -$26 | $158 | ✅ 允許 |
| **R12** | 3 | 2 | 4 | 1 | $331 | -$26 | $305 | ✅ 允許 |
| **R13** | 0 | 11 | 0 | 0 | $198 | $0 | $198 | ❌ 拒絕 |
| **R14** | 5 | 5 | 11 | 2 | $740 | -$52 | $688 | ❌ 拒絕 |

條件定義 (Conditions)
- **C1**: 購買啤酒的數量 (單價 $65，每 2 件中第 2 件 $39)
- **C2**: 購買可樂的數量 (單價 $18)
- **C3**: 購買綠茶的數量 (單價 $25)

動作定義 (Actions)
- **A1**: 啤酒優惠組數
  - 計算公式: `floor(啤酒數量 / 2)`
  - 例: 3 件 → 1 組，4 件 → 2 組，10 件 → 5 組
- **A2**: 總金額 (未折扣)
  - 計算公式: `啤酒數量 × 65 + 可樂數量 × 18 + 綠茶數量 × 25`
- **A3**: 總優惠 (負數)
  - 計算公式: `啤酒優惠組數 × -26`
  - 說明: 每組優惠省 $26 (原價 $65 - 六折 $39)
- **A4**: 結帳金額
  - 計算公式: `總金額 + 總優惠`
- **A5**: 是否允許結帳判定
  - ✅ 允許: 所有商品數量 ≤ 10 且購物車不為空
  - ❌ 拒絕: 任一商品數量 > 10 或購物車為空

規則說明 (測試企圖與重要性)
- **R1**: 空購物車
  - **測試企圖**: 驗證系統能正確拒絕空購物車結帳
  - **重要性**: 防止產生無效訂單，確保業務邏輯完整性

- **R2**: 單件啤酒無優惠
  - **測試企圖**: 驗證未達優惠門檻時，系統按原價計算
  - **重要性**: 確認優惠規則不會誤觸發，保護商家利益

- **R3**: 2 件啤酒觸發優惠 (1組)
  - **測試企圖**: 驗證剛好達到優惠門檻時，系統正確計算 1 組優惠
  - **重要性**: 這是優惠規則的**最小觸發邊界**，必須精準計算以建立客戶信任

- **R4**: 3 件啤酒 (1組優惠 + 1件原價)
  - **測試企圖**: 驗證奇數件數時，優惠組數計算正確 (只有 1 組，第 3 件原價)
  - **重要性**: 確認系統能正確處理**不完整組合**，避免多給或少給優惠

- **R5**: 4 件啤酒 (2組優惠)
  - **測試企圖**: 驗證多組優惠時，系統能正確累加計算
  - **重要性**: 確認優惠規則能正確累加，測試**整數組合**的計算邏輯

- **R6**: 10 件啤酒 (5組優惠，上限邊界)
  - **測試企圖**: 驗證達到系統限制上限時，優惠計算與允許結帳邏輯都正確
  - **重要性**: 測試**最大合法邊界**，確認 5 組優惠計算無誤，且系統不會錯誤拒絕合法訂單


啤酒超限測試
- **R7**: 啤酒超過 10 件 (11件，5組優惠)
  - **測試企圖**: 驗證系統能正確拒絕超過單項限購數量的訂單，但仍能正確計算優惠組數 (供錯誤訊息使用)
  - **重要性**: 測試**最小違規邊界** (10+1)，確保限購機制有效，且錯誤處理時金額計算邏輯不崩潰

- **R8**: 啤酒超過 10 件 (12件，6組優惠)
  - **測試企圖**: 驗證更大的違規數量 (10+2) 也能被拒絕，且能正確計算 6 組優惠
  - **重要性**: 確認限購機制不是**硬編碼單一數值**，而是真正的邏輯判斷 (>10 皆拒絕)

非優惠商品測試
- **R9**: 僅購買可樂 (無優惠商品)
  - **測試企圖**: 驗證非優惠商品的正常計價流程
  - **重要性**: 確保優惠邏輯不會影響非優惠商品，系統能正確區分商品類別

- **R10**: 僅購買綠茶 (無優惠商品)
  - **測試企圖**: 驗證另一個非優惠商品的計價邏輯
  - **重要性**: 與 R9 互補，確保所有非優惠商品都能正確處理

混合商品測試
- **R11**: 混合商品 (含 1 組優惠)
  - **測試企圖**: 驗證優惠商品與非優惠商品混合購買時，系統能正確分別計算
  - **重要性**: 這是**真實購物場景**，必須確保優惠只套用在啤酒，其他商品不受影響

- **R12**: 三種商品混合 (含 1 組優惠)
  - **測試企圖**: 驗證最複雜的購物組合，所有商品類型都存在且啤酒為奇數件時的計算正確性
  - **重要性**: 測試系統在**最複雜情境**下的穩定性，確保優惠計算邏輯不會互相干擾

其他商品超限測試
- **R13**: 可樂超過 10 件 (11件，無優惠)
  - **測試企圖**: 驗證限購規則適用於所有商品，不只優惠商品
  - **重要性**: 確認限購是**全商品通用規則**，避免規則漏洞

- **R14**: 混合商品但綠茶超限 (11件，含 2 組啤酒優惠)
  - **測試企圖**: 驗證混合購物時，系統能正確檢查每項商品的限購規則，即使其他商品合法
  - **重要性**: 確保限購檢查不會因為多商品混合而失效，保護庫存管理

---


我再強調一次, Decision Table 的內容真的要仔細看, 這是跟 AI 對焦測試內容的理解, 補足我前面過度簡略的敘述｡ Decision Table 沒弄好, 可能整組測試都沒有發揮效益 (沒測到有價值的範圍)｡ Decision Table 可以當作是 test case 的規格書, 你只要把這幫表格確定好, 基本上衍生出來的這包 test case 就安全了, 基本上最關鍵的部分就完成了｡

不得不說, 只要你給的回饋正確, 有確實 review, 也有正確回答你的期待, 其實 AI 整理的結果品質是不錯的｡ 這結果很正確, 而令我驚豔的是 AI 抓測試目的其實抓的很正確, 我要他列出測試的企圖跟重要性, 我覺得都有回達到我預期的結果｡

上表的數字, Criteria, Action 的刪減, Rules 的刪減, Table 的內容我都修正過了, 最終結果定案, 就可以繼續下一步了, 真正展開測試案例｡


## 2-3. Generate All Test Cases via Decision Table

我一直強調, 最關鍵的是 decision table, 其實確認完之後, 後面就都是 AI 的 "苦工" 了, 基本上格式在 prompt 都定義好了, 測試範圍在 decision table 都驗證好了, 這步驟就是把所有的 test case 寫出來而已｡

按照前面 decision table 的內容, 總共有 14 個 test case 需要展開, 我直接貼 AI 產完給我的摘要:

---

📁 目錄結構

```
/tests/shopping-cart-checkout/
  ├── decision-table.md              (決策表文件)
  ├── tc-01-空購物車.md
  ├── tc-02-單件啤酒無優惠.md
  ├── tc-03-2件啤酒觸發優惠.md
  ├── tc-04-3件啤酒1組優惠.md
  ├── tc-05-4件啤酒2組優惠.md
  ├── tc-06-10件啤酒5組優惠-上限邊界.md
  ├── tc-07-11件啤酒超限拒絕.md
  ├── tc-08-12件啤酒超限拒絕.md
  ├── tc-09-僅購買可樂.md
  ├── tc-10-僅購買綠茶.md
  ├── tc-11-啤酒加可樂混合.md
  ├── tc-12-三種商品全混合.md
  ├── tc-13-可樂超限拒絕.md
  └── tc-14-混合但綠茶超限.md
```

📊 測試覆蓋率分析
- **條件數**: 3 (啤酒、可樂、綠茶數量)
- **動作數**: 5 (優惠組數、總金額、總優惠、結帳金額、允許判定)
- **規則數**: 14
- **測試案例**: 14

🎯 測試類型分類
1. **正常結帳流程** (R1-R6): 空購物車、啤酒 1-10 件
2. **啤酒超限測試** (R7-R8): 11-12 件
3. **非優惠商品** (R9-R10): 可樂、綠茶
4. **混合購物** (R11-R12): 多商品組合
5. **其他商品超限** (R13-R14): 可樂、綠茶超限

🎁 每個測試案例包含
- ✅ Given-When-Then 格式
- ✅ 明確的測試資料 (輸入/輸出)
- ✅ 詳細的金額計算明細
- ✅ 業務規則驗證點
- ✅ API 呼叫序列建議
- ✅ 測試重要性說明

---


實際上的 test case 我也貼一下, 其實也寫的很不錯, 我的要求都有做到｡ 總共有 14 份, 我貼一份當作代表就好, 我貼 R2:

(為了不影響文章結構, 我移除了 markdown 的章節資訊)


---
**TC-02 單件啤酒無優惠**

**測試目標**
驗證未達優惠門檻時，系統按原價計算，確認優惠規則不會誤觸發。

**前置條件 (Given)**
- 使用者已登入 (username: `andrew`, password: `1234`)
- 購物車內商品:
  - 啤酒: 1 件 (單價 $65)
  - 可樂: 0 件
  - 綠茶: 0 件

**測試步驟 (When)**
1. 使用者將 1 件啤酒加入購物車
2. 使用者進行購物車結帳
3. 系統計算結帳金額

**預期結果 (Then)**
- ✅ 系統允許結帳
- ✅ 啤酒按原價計算 (無優惠)
- ✅ 總金額: $65
- ✅ 總優惠: $0
- ✅ 結帳金額: $65
- ✅ 訂單成立


**測試資料**

輸入參數
- 啤酒數量: 1
- 可樂數量: 0
- 綠茶數量: 0

預期輸出
- 允許結帳: ✅ 允許
- 啤酒優惠組數: 0 (未達 2 件門檻)
- 總金額 (未折扣): $65
- 總優惠: $0
- 結帳金額: $65

金額計算明細
```
啤酒: 1 件 × $65 = $65
可樂: 0 件 × $18 = $0
綠茶: 0 件 × $25 = $0
----------------------------------------
總金額 (未折扣): $65
啤酒優惠組數: floor(1/2) = 0
總優惠: 0 × -$26 = $0
----------------------------------------
結帳金額: $65 + $0 = $65
```

**業務規則驗證**
- ✅ 單件啤酒不觸發優惠 (需 2 件才享第二件六折)
- ✅ 優惠門檻控制正確

**API 呼叫序列**
1. `add_to_cart` - 加入 1 件啤酒
2. `get_cart` - 取得購物車內容
3. `calculate_total` - 計算結帳金額
4. `checkout` - 執行結帳

**測試重要性**
確認優惠規則不會誤觸發，保護商家利益。這是優惠計算的基準線測試。
---





# 3. 讓 AI 探索並記錄 API 的執行步驟

其實在做這個 side project, 最有成就感的環節就是, 你把一切都準備好, 然後看著他按照你要求動起來的那瞬間, 就像你疊完整屋子的骨牌, 然後推倒的那一瞬間一樣... 

![alt text](/images/2025-11-10-vibe-testing-workflow/image-21.png)

這段, 對應到流程圖, 就是藍色框起來的部分,也就是 /testkit.api.run 負責的部分｡ 前面產完的 test case, 現在這步驟就要開始執行了｡ 來複習一下, 目前手上我有哪些測試相關的資訊:

1. test case (要測什麼的業務情境)
1. api spec (api 的操作規格, 也就是 swagger)
1. text-to-calls mcp (就是上一篇文章的 PoC 的程式碼, 重新封裝成 mcp server)

理論上有了 (1) + (2), 就有足夠資訊推導出 api 該怎麼呼叫了, 而 (3) 就是讓 AI agent 能真正自動化執行它的工具 (這是給 agent 的工具, 不是給我)

現在萬事具備了, 我就開始啟動它｡ 一次跑完太花時間了, 既然我前面都讓 AI 幫我標記重要性了, 我就挑選第一組: 正常結帳流程 (R1-R6) 來示範｡


## 3-1. Text2Calls MCP

這個步驟對應到流程, 就是第二步, 讓 AI 負責 "探索", 從規格與測試案例中找出最理想的執行步驟, 並且存成 session logs 讓後續的步驟可以依循｡


--
首先, 我在 copilot chat 視窗下了這道指令:

```
/testkit.api.run

執行: 正常結帳流程 (R1-R6)
```

整個程序就開始啟動了, 開始之前, 我簡單交代一下我的 MCP 做了什麼... 首先, 我封裝了 4 個 tools 給 agent:

- QuickStart
- ListOperations
- CreateSession
- RunStep


Agent 第一次使用, 就按照 tools description 的指示, 先呼叫 QuickStart. 其實這做法是從 Shopify 學來的 (參照我 facebook post), 我用 tools 的方式傳回明確 (而且可以是動態的提示) 的使用說明給 Agent, 最大化的正確引導 agent 來用我的 mcp:

![alt text](/images/2025-11-10-vibe-testing-workflow/image.png)


接下來, agent 果然按照 QuickStart 的指示, 先查詢 api 可用的操作有哪些了:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-1.png)

這邊特別說一下, 我的 mcp 目的是要將 api 呼叫抽象化｡ 我曾經實作過一個版本, 直接將 api 的 swagger 所有規格都直接提交出去, 不過這時我就在想, 這樣的話我還需要多包一層 mcp 嗎? 市面上有現成的 swagger -> mcp 可以用, 我其實不必要自己寫｡ 不過實驗之後, 我放棄這條路了, 因為:

- api 包含太多不必曝露給 agent 的細節
- api spec / request / response 資訊過多, 呼叫一兩次就把 agent 的 context window 塞爆了
- api 包含太多不適合給 agent 處理的動作 (尤其是機械式的操作, 例如 oauth2)

因此, 我想清楚我自己包一層 mcp 的重點了, 核心任務就是將 "呼叫 api" 這件事抽象化, 讓主要的 agent (在這裡就是 github copilot + claude sonnet 4.5) 能夠專注在主要任務 - 執行 test case 上, 排除一切干擾 (包含不必要的 context, 不必要麻煩他操作的細節)

所以我的抽象化切角就是:

- ListOperations, 用文字敘述 (只有 api-name 是明確 id) 告訴 agent 有哪些 api 可以用
- CreateStssion, 明確告訴 agent, 建立 session 來收納一個 test case 的執行過程記錄
- RunStep, 接受 agent 的指示, 在當前的 session 內要執行指定的 operation, 並且用文字敘述告知 mcp 操作的內容 ( mcp 內部還有另一個 LLM, 會真正負責解析 text -> api call 的參數對應跟生成)

經過這樣拆分, 整個 testkit + mcp 的執行才開始順暢起來, 先前的版本總是卡在奇怪的地方, 然後 agent 就暴走亂回答一通了｡ 這是某種形態的 context engineering, 有效率的控制主要 agent 的 context window 不要被干擾的案例, 這也是某種形式的 sub-agent 縮影, 只是中間通訊的 protocol 是 mcp (model context protocol), 而不是 a2a (agent to agent protocol) 而已.

也因為 RunStep 做的事情是翻譯 text -> api call, 這個 mcp server 我才會取名為 Text2Calls MCP.

Agent 透過 list operations, 並且對照 TC-01 test case, 列出可用的操作有這四個:

- GetCart - 取得購物車內容
- CreateCart - 建立新購物車
- EstimatePrice - 試算結帳金額
- CreateCheckout - 建立結帳交易

用這四個操作就足以組合出 test case 01 的情境了, 接下來 agent 真正要開始執行, 於是就先執行 CreateSession:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-2.png)

建立完 session, 其實 MCP 在背後也先把執行過程中必要的 OAuth2 認證先跑完, 並且順利取得 access token .. 這些操作如果不是在 MCP 內用 code 處理, 我不知道還要寫多少 prompt 才教的會 AI 來做這些事...

![alt text](/images/2025-11-10-vibe-testing-workflow/image-3.png)


接著, 開始真正執行 RunStep, 呼叫 API:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-4.png)
![alt text](/images/2025-11-10-vibe-testing-workflow/image-5.png)

我特別貼一下 agent 傳給 RunStep 的資訊, 都被抽象化了, 只剩下 operation, action, context:

```json
{ 
    "operation": "CreateCart", 
    "action": "create an empty cart", 
    "context": "User andrew logged in. Create a new empty cart to test checkout with no items. This should later fail at checkout." 
}
```

基本上 agent 只是交代 mcp 要 run 哪個 api (operation), 想要他完成的任務 (action) 是什麼, 完成這任務的背景是什麼 (context), 其餘就是 mcp 按照這些資訊, 搭配 mcp 內部自己 own 的 LLM, 觸發 function calling 機制來完成 api call 的任務

RunStep 這操作, 執行過程中可能會呼叫 1 ~ N 次不同的 api, 這些過程都會在 _session 目錄下留下足跡｡ 舉例來說 RunStep -> CreateCart, 就真正對應到 POST /api/carts/create, 具體的 api call request / response 都有被記錄下來:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-6.png)

而抽象化的執行順序, 也會被記錄在 session-logs.md 下 (我節錄 RunStep[1] 的片段, 這些都是 mcp 自己留下的記錄):

---

**RunStep[1], CreateCart**

**action**:
create an empty cart

**context**:
User andrew logged in. Create a new empty cart to test checkout with no items. This should later fail at checkout.

- http(5) Post https://shop.chicken-house.net/api/carts/create => 201 Created
  - [request](005-api-request.txt)
  - [response](005-api-response.txt)

**result**:

**answer**:  
已成功建立一個空購物車（id=163）。執行成功且結果如預期：回傳的購物車內容為空（prodQtyMap 與 lineItems 均為空），可用於後續驗證「空購物車結帳應被拒絕」的測試。

**details**:  
依據執行結果，API 回應內容如下（已轉成可讀敘述）：
- 請求：POST https://shop.chicken-house.net:443/api/carts/create
- 回應 Content-Type: application/json; charset=utf-8
- 回應 JSON 內容：{
    "id": 163,
    "prodQtyMap": {},
    "lineItems": []
  }
敘述說明：系統已建立一個新的購物車，購物車編號為 163；目前購物車內沒有任何商品（prodQtyMap 空物件，lineItems 為空陣列），符合「建立空購物車」的需求與測試前置條件。無 HTTP 錯誤或例外回傳，操作順利完成。

**instructions**:  
下一步建議：
1) 嘗試呼叫 CreateCheckout（payload: { cartId: 163 }）以確認系統在建立結帳交易階段是否拒絕空購物車。期望行為：API 應回傳錯誤（或拒絕建立交易），並說明原因（例如：「購物車為空，無法結帳」）。
2) 若 CreateCheckout 被接受（意外情況），請接著模擬付款並呼叫 CompleteCheckout，檢查是否會在完成結帳時被阻擋；若 API 回傳成功建立訂單，視為系統行為不符合預期，需進一步調查。 
3) 測試檢查要點：確認 API 回傳的錯誤碼與錯誤訊息，並在測試結果中斷言系統應拒絕空購物車結帳以防無效訂單產生。
4) 若需要，我可以代為執行 CreateCheckout 並回報其實際回應內容以完成此測試情境驗證。

**summary**:  
前次 context：使用者 andrew 已登入，目的是測試空購物車結帳場景（購物車內無任何商品，系統應拒絕結帳並顯示錯誤訊息）。
此次 action：CreateCart — 建立一個新的空購物車。
此次 answer：建立成功，系統回傳購物車 id=163，prodQtyMap 與 lineItems 均為空，符合建立空購物車的預期。後續請嘗試呼叫 CreateCheckout (cartId=163) 以驗證系統是否正確阻止空購物車結帳；我可以協助執行並回報結果。


---


## 3-2. Session Logs

前面的 "探索" 完成後, 接下來要看看 session logs 留下了哪些線索給下一步使用｡ 這步驟關鍵的地方在於, 探索的結果不一定每次都是理想的, 你可能會挖到寶藏, 也可能會撲空什麼都沒找到｡  因此我希望 "真人" 能夠透過 session logs 的 review 來確認 AI 探索的結果是否符合期待, 不管是修正 prompts, 或是微調 test case, 直到得到你預期的結果為止｡ 

先來看測試案例 TC-01, 這測試案例講的是, API 應該要拒絕空的購物車結帳請求才對, 不過實際上我的 API 並沒有實作這限制, 所以我預期這個測試應該會成功的失敗才對 (咦?


測試的 test case 主要部分如下:

---

**前置條件 (Given)**
- 使用者已登入 (username: `andrew`, password: `1234`)
- 購物車為空
- 購物車內商品數量:
  - 啤酒: 0 件
  - 可樂: 0 件
  - 綠茶: 0 件

**測試步驟 (When)**
1. 使用者嘗試進行購物車結帳
2. 系統檢查購物車內容

**預期結果 (Then)**
- ❌ 系統拒絕結帳
- ✅ 顯示錯誤訊息: "購物車為空，無法結帳"
- ✅ 不產生訂單
- ✅ 購物車狀態維持為空

---

實際執行的步驟, 到 Step3 之前都還正確 (建立購物車, 計算購物車金額-空車):

![alt text](/images/2025-11-10-vibe-testing-workflow/image-7.png)


然而, 到了真正要執行結帳的時候, api 呼叫就不如預期了:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-8.png)


看到這邊, 可以放心了, agent + mcp 有認真替你執行 test case 的每個步驟, 也有確實檢驗結果是否符合預期｡

## 3-3. Test Suite Result

看完一筆 test case 執行過程, 其他剩下的 test case 我就跳過了, 直接看最後結果 (我只 RUN 了基本測試, TC-01 ~ 06):

我有實作基本功能, 但是基本上邊界的檢驗都沒做, 所以基本測試 TC-01 空購物車結帳測試預期會失敗, 其他正常結帳跟折扣計算應該都會通過 (TC-02 ~ 06), 如果後面還有執行超過 10 件商品的測試應該也會失敗 (應該要拒絕結帳, 但是實際會結帳成功), 這些都驗證測試有發會效用｡ 我擷圖讓各位看一下 AI 給我的摘要報告:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-9.png)

針對測試失敗的部分 (TC-01), AI 也特別說明他判定問題出在哪邊

![alt text](/images/2025-11-10-vibe-testing-workflow/image-10.png)

而這些執行過程的記錄, 通通都鉅細靡遺的保存在 session folder 內:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-11.png)

001, 002 -> oauth 認證過程
003 -> 下載保存 openapi spec
004 -> mcp 呼叫 LLM 處理 function calling
005 -> call api: create cart
... (懶的寫了, 總之當成 postman 來看待就好)

openapi-spec.json, 就是 003 保存下來的 api spec 快照
session-logs.md, 就是呼叫過程中的抽象資訊傳輸記錄, agent / mcp 的通訊內容都再這邊, 基本上這是給 "人" 看的歷程...

而我會設計 mcp 要保留這些資訊, 目的很單純, 這些都是將來要寫 "api 自動化測試" 最重要的規格, 以後這些都會是餵給 spec-kit 執行的養分｡

不過, 這篇我不打算示範到 test code generation, 有興趣的可以自己先拿去玩玩看, 或是等我下一篇文章再來介紹｡



# 4. 共用 Test Case, 同時探索 API / Web

這部份, 對應到流程的第三步, 前面我希望降低文件的數量, 用最小的文件量來抓出最有價值的測試組合, 而另一個關鍵就是要最大化 test case 的使用範圍, 一個 test case 能夠同時應用於多種操作介面 ( 前面示範了 API, 現在我要示範 Web UI )｡ 同時處理這兩個維度, 我就能盡我可能的將關鍵的文件數量壓到最低｡

因此, 我希望 test case 能排除操作細節, 方便 review 以及降低維護文件的負擔｡ 而被省略的操作細節, 將來需要時 (可以靠 AI) 應該要從 spec 對照, 來還原需要的操作細節 (前面 #3 已經證實可行)｡

這件事如果做到極致, 那我就能用同樣的 test case 來對應不同的測試目標｡ 如果我的服務, 同時提供 web 介面, 也提供 api 介面, 理論上這些測試案例應該都是共通的啊 (行為應該一致, 但是有不同的操作手段), 這一段, 我就拿一模一樣的 test case, 把 mcp 從我自己寫的 Text2Calls MCP 換成 Playwright MCP, 看看能否完成同樣的測試行為?

![alt text](/images/2025-11-10-vibe-testing-workflow/image-22.png)

於是, 我用同樣的架構來對比, API 規格換成 UI 規格 (最後我沒有用這個), 能自動呼叫 API 的 MCP (Text2Calls) 換成能自動操作網頁的 MCP (Playwright), 我嘗試復刻一樣的成果出來, 只要能達成同樣的效果, 那就證明我真的能做到 test case 的複用, 也才能真正降低測試案例維護的負擔, 同時不會降低測試的覆蓋率｡

拜 coding agent 所賜, 這次我特地準備了一個有 Web UI 的 "安得魯小舖"｡ 我是前端大外行, 終於也能生出還過的去的純前端測試網站了 XDD, 這是 React + NodeJS 的前後端架構, 背後 call 同樣的 API 搭建的 web app, 有興趣的人可以進去玩玩看:

Andrew Shop: https://shop.chicken-house.net

這次, 同樣來看看我準備了什麼資訊:

1. test case (要測什麼的業務情境, 同 api)
1. ui spec (我沒有提供, 但是我開發的 web ui, 特別遵循無障礙的設計, accessibility, 後面說明)
1. playwright mcp

mcp 的安裝設定我就不講了, 大家自己看 playwright mcp 的官網, 我開啟 copilot chat 輸入這段指令:

```
/testkit.web.run

執行: 正常結帳流程 (R1-R6)

```

這段我沒有另外再寫 MCP 來支持這些操作, 例如 session, logs 等等, 因此這部份就簡單處理 (我直接寫在 prompt 而已), 所以 agent 一樣會幫我建立 session folder, 只是是靠 shell ..

草草的準備好測試環境之後 (咦? , 正式的測試就要開始了, 第一次呼叫 playwright mcp, 就是啟動首頁:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-12.png)


經過一連串的嘗試 (略過), 完成 OAuth2 認證, 登入成功:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-13.png)

接下來你可以看到 agent 不斷的在跑, 而 chrome 也自己在操作, 等了幾分鐘之後, 開始測試結帳:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-14.png)

看起來操作都很順利, 操作的過程都沒有脫離 test case 的敘述, 跑了大約 20 min (真的很慢), 六個測試項目執行完畢｡

比較不一樣的結果是: 這次六個測試都成功通過!

![alt text](/images/2025-11-10-vibe-testing-workflow/image-15.png)

這並不是測試結果不正確喔, 我在刻這個 web ui 的時候, 我在 api 層級沒有加上的限制或是檢查, 我都有在 ui 這層補上｡ 因此 TC-01 空的購物車不能結帳, 這動作在 UI 的確是有實作的, 我擷圖證實一下:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-16.png)

購物車是空的時候, 結帳的按鈕整個被藏起來沒辦法按, 所以 TC-01 測試通過


## 4-1. API 需要 SPEC, UI 不需要?

同樣的, 這題也是過去我卡很久的題目...

不知道有多少人認真拿 playwright mcp 來做類似的事情?

簡單的網頁都很正常, 複雜一點的, 有的光是 login 就卡半天了, 要嘛就是找不到按鈕再哪裡, 要嗎就是按不到, 總之就是各種詭異的狀況, 明明按鈕或是欄位就在那邊, 你看的到, 但是 AI 就是看不到

其實這就是 "規格" 啊, 人看的到, 是因為最終是從 "視覺" 上來判定的, 但是 AI 不是｡ 其實有 solution, 的確是擷取畫面, 然後進行影像辨識來判定的, 這技術可行, 不過運算資源花太兇了, 現在的運算能力還沒辦法做到即時 (一張 4K 的圖, 要判定按鈕再哪, 該不該按下去, 就要幾十秒鐘了)

我試過其他的套件, 例如 selenium mcp, 它的辨識能力就 "好的多", 因為他直接判定 HTML, 基本上頁面所有細節都能檢測的到, 不過 HTML 太肥了, 動不動就 100kb 以上, 很快的 agent 的 context windows 就被佔滿了, 根本無法實際使用

playwright mcp 實用的多, 因為他會先將 HTML 精簡成自訂格式的 yaml 結構, 只是他是如何判定哪些資訊該保留下來? 我爬了很多文, 最後是厲害的同事逆向工程挖出來的, playwright 靠的是 "無障礙" 的網頁標記資訊｡

這招其實很聰明, 我聽到關鍵字 "無障礙" (Accessibility) 我就想通了, 先前看 A16Z - AI Era 趨勢報告, 就提到這一段: [# 6, Accessibility as the universal interface]

跟人類的感官相比, 現在的 AI 對於你的電腦, 就像個殘障人士.. AI agent 既沒有辦法幫你操作鍵盤滑鼠 (除非你授權), 也看不到你畫面上現在呈現的是什麼 (除非你授權擷圖), 沒辦法聽到你在聽什麼音樂, 沒辦法 ...., 而無障礙的設計, 就是讓應用程式能讓這些人也有機會 "看到" 網頁的內容的設計, 看不到字, 可以用唸的, 看不到圖, 可以用文字描述, 唸給你聽..

講到這邊, 串起來了嗎? 如果你的網頁有確實做好 "無障礙" 設計的話, 對於 AI 這種視覺障礙的人士, 它就多了一個能理解網頁資訊的管道了｡ 而 playwright mcp 也大量依靠這些標記來 "認識" 你的網頁怎麼操作｡ 因此, 我的 Andrew Shop WebUI, 除了 OAuth2 Login 那一頁還沒修正之外 (所以 login 的地方卡比較久), 其他都是這次新開發的, 我在開發之初, 就明確的用 spec-kit 說明, 全站都要遵守無障礙的設計原則 (不過具體是什麼我也搞不懂 XDD), 換來的就是 AI 能夠清楚辨識該如何操作我的頁面了｡

無障礙的設計, 是未來 AI 能否讀懂 (以及能否正確操作) 網站的關鍵要求, 做的越好, 你的服務越 "AI Friendly" ｡


# 5. 尚待改版的部分: 將探索結果自動化

這段, 對應到整體流程的最後一步, 把探索的結果, 用生成 test code 的方式來實現自動化｡不過 Generate Test Code 這段我先不展開, 我打算改用 SpecKit 重做一次 PoC 驗證之後, 在下一篇再談｡

![alt text](/images/2025-11-10-vibe-testing-workflow/image-23.png)

整個流程的拼圖, 其實還有最後兩塊, 就是塗上標的藍色部分, 分別是 testkit.api.gencode, 跟 testkit.web.gencode. 這段其實我自己已經有實作出來了, 結果也確實可行, 甚至我連操作錄影都錄好了, 先前也在我 facebook 發表過:

// facebook post share url

不過, 這段我想要用 SpecKit 重新調整一次, 我會覺得更合適｡ 實際上的自動化測試, 背後通常都會有一套測試管理用的平台, 並且搭配 CI/CD, 在適當的時候 ( deployment, or release ) 觸發測試的需求, 並且統一收集測試數據來分析 & 監控告警｡

因此, 我相信這些測試都有一定的標準規格需要符合, 包含:

1. 如何啟動
1. 如何輸出測試結果
1. 測試步驟的客製化注入
1. 測試的環境準備
1. 測試的 secret 管理
1. ...

這些, 都高度需要全系統的統一, 我不覺的是我自己寫一段 prompt 生出能動的 code 就能結束的任務, 因此我決定把這段切出來, 改用 SpecKit 來替代, 我只要準備好 test case 相關的必要規格 (就前面提到的 session logs), 剩下的就交給專家吧! 因此這篇我就不提 gencode, 有興趣的朋友可以等我下一篇 (如果我搞的定的話), 或是各位可以自己捲起袖子嘗試看看!


整個 side project 我做到這邊, 回頭看看先前的流程, 我終於覺得我不再只是拿新工具套用舊流程了, 重新思考過的工作流程, 我覺得人要扮演的角色, 以及 AI 要解決的問題更明確, 也更精準了｡ 觀念 / 流程 / 實作我覺得都有到位, 不知各位看法如何?

<!--
# 6. 結語: 為 AI 設計新流程

回頭看這次的 side project，其實我做的事可以粗略拆成三點：

1. **觀念: 認清「測試通膨」是結構問題，不是人力問題**
   解決結構問題, 會帶來數量級的改變; 替換工具, 會帶來線性的加速, 如果你面對的問題同時包含兩者, 務必優先解決結構問題｡ 結構問題通常都隱藏在流程內, 過度專注工具你會容易忽略掉結構問題｡

2. **流程: 把 AI 放在「左移」的位置**  
   能在問題被擴大之前就解決它是最理想的, 威力越強大的技能, 盡可能的用在問題的根源｡
   - 用 Decision Table 把測試範圍與重要性想清楚  
   - 讓 AI 幫忙把 decision table 展開成成套的 test cases  
   - 再讓 AI 去「探索」一次 API / UI，把探索過程變成操作範例與 log  
   也就是：**AI 重點不是在「多跑幾次」，而是在探索, 在「幫你產出可以重用的自動化資產」**。  
   後面重跑的部分，交給機器、CI/CD 去處理就好。

3. **實作: 刻意留出讓 AI 好發揮的介面**  
   - API 這邊用 Text2Calls MCP，把「怎麼 call API」抽象成 operation + action + context  
   - UI 這邊遵守 accessibility 的設計，讓 Playwright MCP 看得懂你的頁面在幹嘛  
   - 所有執行過程都留下規格化的 log，未來可以直接餵給 Speckit 這類工具產生自動化 test code  


事後回想, 這套 workflow 對我來說，有幾個很明確的收穫：

- 我更少在跟 AI 對話「請幫我多跑幾次 XXX」，而是把心力放在「這個 decision table 長得對不對, 是不是所有的 Rule 我都需要? 如果要區分重要性, 哪些是第一優先?」  
- 我開始刻意設計 API / UI 的「可測試性」，而不是等系統做完才問：欸這要怎麼測？  
- 我可以接受某些地方先暫時用 AI interactive 模式跑，等 log 穩定了，再補自動化測試，而不是一開始就企圖從 0 到 100 分, 把所有的系統測試都自動化。

當然，這篇還只是半場而已。  
真正有趣的下半場，是把這些執行記錄、decision table、test case 規格通通餵進 Speckit 之類的工具，讓它幫我把「會動的自動化測試 code」生出來，然後直接掛進 CI pipeline 裡。

最後，其實我寫在第一段的結尾，就是我的心得，我在文章最後再貼一次:

> "  
> 面對 AI 的方式, 先把它用在 "改善", 能得到立即的效益當然沒問題, 不過別因此而放棄了新流程的研究, 否則很快就會碰到瓶頸, 不適合 AI 的流程遲早需要被替換的。  
>  
>  然而 ai 來的太快, 什麼是正確的 "新流程" 也沒人知道, 一切都要靠自己摸索嘗試, 這才是最困難的環節, 不過也是最有趣, 最有成就感的地方｡  
> "
-->


# 6. 總結

回頭看這次的實驗，其實就是在回答一開始那個問題:

> 如果從 AI 的能力出發，測試這件事應該怎麼做，才不會只是把 AI 塞進舊流程？

這篇文章裡，我最後收斂出來的做法大概是三件事:

1. **用 Decision Table 收斂「有價值的測試」**  
   把 AC 拆成 decision table，再展開成抽象的 test cases，用最少的文件換到最高的涵蓋率，也讓 AI 有一個清楚的「測試規格」可以接手。  

2. **讓 AI 負責「探索」，不是負責「重複執行」**  
   測試步驟的探索交給 AI Agent + MCP，幫你實際走過 API / Web UI 的流程，留下 session logs；而不是每次 release 都請 AI 再從頭解讀一次 test case。  

3. **把探索出來的結果變成可以重複執行的資產**  
   Test case + API spec / UI spec + session logs，本身就是寫自動化測試程式碼最好的 spec。這篇沒有示範 GenCode，但實際上三個步驟走完之後，產生大量 test code 只是時間問題。

整個 workflow 的設計，其實就是刻意把：

- 需要「判斷」的部分交給人腦（決定測什麼）
- 需要「探索」的部分交給 AI（怎麼測）
- 需要「穩定重複」的部分交給程式碼（回歸測試）

在這兩年推 AI 流程的過程裡，我越來越確定一件事:

AI 實際上帶來的是「變革」，不是「改善」。

但是大部分的人, 卻只願意把 AI 用在「改善」。如果只是把它當成更快的工具，硬塞進舊流程，很快就會撞到成本、效能或維護的天花板，很多本來應該被重新設計的流程，反而被舊習慣綁住了，也限制住 AI 的潛力。

不過什麼才算「正確」的新流程? 沒有人說得準，全世界的人都在摸索中，沒人有標準答案，只能靠自己摸索、實驗而已。這大概也是現在做架構、做流程設計最困難、但也最有成就感的地方吧。
