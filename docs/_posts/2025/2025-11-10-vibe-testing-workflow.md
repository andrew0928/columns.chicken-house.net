---
layout: post
title: "AI-First Testing, 以 AI 為核心重新設計測試流程"
categories:
- "系列文章: 架構師觀點"
tags: ["架構師觀點","技術隨筆"]
published: false
comments_disqus: false
comments_facebook: false
comments_gitalk: false
redirect_from:
logo: 
---

首先, 我自己打臉我半年前寫的那篇 [聊聊 Vibe Testing 實驗心得](/2025/05/01/vibe-testing-poc/) 的文章｡ 其實那篇文章研究的 "自動執行 API 測試" 是可行的, 但是應該拿來探索測試步驟用, 而不是拿來當最終重複執行用｡ 當時我的思路是, 現在的流程太花人工了, 如何讓 AI 來替代人工手動測試, 讓測試流程更有效率? 不知不覺就掉到這 "局部最佳化" 的陷阱裡..

不過, 事後發現, 如我我完全沒有既有流程的包袱, 我應該不會這樣做才對..., 有了這警覺, 我重新想了一次這流程, 有了不同的發現 (所以就有了這篇文章)｡ 

不過, 重新想過一輪之後, 這篇我想談: 從 AI 的能力出發, 重新思考 "測試" 這件事應該怎麼進行比較有效? 怎樣更能從來源解決測試的問題? 我嘗試換了個方式來突破拿 AI 來進行軟體測試, 這次的效果比我預期的好, 於是就有了這篇...｡

<!--more-->

這兩年來, 不管在公司, 或是個人, 或是社群, 大家都在拼命地用 AI 工具來提升自己的工作效率, 這是很正常的。不過很多時候我都在想: AI 的原生世代也會這樣做事嗎? 我想起 Sam Altman 去年的一個專訪, 提到不同世代的人, 對 AI 的使用方式會有很大的差異｡

最令我印象深刻的是這段: 20 歲的年輕人, 把 ChatGPT 當成 "作業系統" 來使用...

> "Older people use ChatGPT as a Google replacement."
> "People in their 20s and 30s use it as a life advisor. "
> "People in college use it as an operating system."

https://youtu.be/uVEjlRK0VWE?si=jDQyFJj2n8m4zrug&t=4

所以, 我在想的是, 我之前的做法, 其實是在用 AI 改善舊流程的效率, 而寫這篇文章時我想的是: "如果我想從 AI 已經是作業系統般的存在時, 那測試流程會變成什麼樣子?"

這就是使用深度的差異吧, 對於 Developer 來說, 我想是目前運用 AI Agent 最深度的族群了｡ 我看到很多人已經拿 Claude Code 這類 CLI Tools 來當作它們主要的工作介面, 不只用來寫 code, 也用來寫文件, 更用來做各種其他任務或是自動化..., 而回到半年前我用 vibe testing 的角度來思考測試這件事, 我發現我的思路是:

> " vibe coding 很炫, 那我是不是也能 vibe testing 呢? "
> " 目前的測試都需要仰賴人工操作, 那我是不是也能用 AI 來代替人工操作呢? "
> " 如何讓 AI tools 無痛的融入現有的測試流程, 解放過去投在裡面的人力?? "

其實這些思路都沒錯, 也都得到效率的提升, 但是我總覺得不對勁, 因為最關鍵的流程 (workflow) 並沒有改變, 只是把 "人" 換成 "AI" 而已｡ 如果 AI 的能力已經是個 "變革", 其實整個環節搭配的平衡都已經改變了, 流程勢必要重新檢視一次才對｡ AI 有他擅長的能力 (GPU), 人類也有擅長的能力 (Brain), 而傳統的應用程式也是 (CPU), 能鑑別出這些 "運算資源" 的使用範圍跟邊界, 然後重新設計流程, 才是最重要的事情｡

過去軟體開發的流程, 最大瓶頸就是 "人寫 code 的速度太慢", 所有流程都圍繞著這點在強化跟改善｡ 突然間出現了 coding agent, 讓 "coding" 這件事突然變成 10x, 平衡被打破了, 所以現在軟體開發領域的流程整個被重塑了, 有 vibe coding 的亂象, 也有 SDD 這類嚴謹的流程被提出來..., 這些都是 "基本的流程" 正在被改變的表象

因此, 距離上一篇文章, 隔了六個月, 這段期間我花了不少時間在研究 MCP design, Context Management, 以及 SDD / SpecKit 的使用, 這些基礎我認為都是未來重要的基本技能, 我重新用 TestKit 來包裝我對 Testing Workflow 的想法, 效果還不錯, 於是整理了這篇文章來分享一下我的心得｡

接下來的文章內容, 我會在第一段先介紹想法的轉變, 然後拆成三個步驟來完成 AI-First Testing Workflow 的示範:

- 先用 Decision Table 把「對的測試」定義清楚，而不是一開始就丟給 AI 自己亂測  
- 再把 AI 放在「產生自動化測試資產」的位置，而不是讓它去當永遠跑不完的測試工  
- 最後，用同一份 test case 同時覆蓋 API / UI，讓流程真的有機會縮減，而不是再多生一份文件地獄

下面是我目前的實作版本，還在演化中...
 
 


# 1. AI Native Testing Workflow

對我來說, 測試是件相當頭痛的問題, 主要的痛點在於測試的 "量體" 太大, 背後包含寫不完的測試案例, 準備不完的測試資料, 以及跑不完的手動 & 自動測試｡即使有 AI 加持, 你要測完所有的狀況也幾乎是不可能的事情, 勢必要有一些方法, 有效率並且優先完成重要有價值的測試才是正途｡

為了聚焦我的主題跟思路, 不相關的環節我就大幅簡化了, 我用這張圖來說明:


![alt text](/images/2025-11-10-vibe-testing-workflow/image-17.png)


從左到右, 是從需求規格中的驗收條件 (AC, Acceptance Criteria) 開始, 到測試結果的流程｡ 每個步驟上面標的數字, 就是可能要展開的數量 (數量級)｡ 試著估算看看: 如果系統需求列了一項 AC (ex: 購物車結帳時必須檢查折扣計算規則, 以及購買條件限制), 最終應該展開多少筆 Test Cases ? 在系統的生命週期內你需要執行幾次測試? 成本與代價是什麼?


**測試量體估算**:
- 每 1 個 AC 平均展開 10+ 個 Test Case
- 系統有 4 種操作方式 (操作: Web UI, Android APP, iOS APP, REST API)
- 系統有 10 種非功能性需求 (NFR, Non Functional Requirement, 如資安, 授權, 效能等等)

**測試執行瓶頸**:
1. **測項數量**:  
如果這些組合都展開, 每一個 AC 就必須要維護 4 + 10 + 10 份規格文件, 以及產生出來的 1 x 4 x 10 x 10 = 400 份不同的測試腳本... 這量體, 光是文件就已經難以維護了.. 這是第一個瓶頸, 測項維護｡
2. **執行次數**:  
就算真的有 400 份測試案例, 一年你會需要跑幾輪測試?  
假設一週 release 一次, 每次 release 前內部 QA 至少測一次, release 後正式環境也至少測一次, 那一年應該要跑 50 x 2 = 100 輪, 總共是 400 x 100 = 40,000 次測試...

一年執行 40000 次測試, 若用 AI 來自動執行, 需要花多少錢? 這時, 我回頭看看我半年前自己寫的文章, 我想打臉我自己的想法了啊 XDD... 如果我真的用 vibe testing 用 AI 當下執行 API test, 每跑完一次約需要 2 分鐘, 對應的 api token 約 USD$ 0.03 (剛好約 NTD$ 1.0) 左右, 一年要花 2 x 40000 = 80000 分鐘來執行 (55.56 天), 整個過程共計要花掉 4 萬台幣的 token 費用... 而這只是完成 "一個 AC" 的要求需要的測試而已...

簡單的計算, 就知道這方法還無法實際應用｡ 雖然比單純用 "人力" 來測試已經好很多了, 但是遠遠跟不上測試的通膨啊..., 而且 AI 執行測試也有缺點, 除了速度跟費用之外, AI 每次執行的結果都有一些差異, 這也是另一個頭痛的問題｡

因此, 面對這兩個執行上的瓶頸, 我重新想一次流程, 我應該讓 AI 從負責 "自動化" 測試的任務, 往流程左邊移動, 讓 AI 專注 "探索" 測試步驟｡ 當探索確定後, 剩下單純 "自動化" 的要求, 其實只要讓 AI 一次性的翻譯成對應的自動化測試程式就好, 不必要每次執行都還要靠 AI 當下解讀 test case 跟比對 api spec .., 既提高效率, 降低成本, 同時也能確保每次執行的測試步驟都會一模一樣｡

同樣的, 測試案例的數量問題, 也同樣能 "左移", 如果我能將 AC + 決策表維護好, 是否就能穩定的輸出展開的 test cases? 如果我能同時顧好 api spec, 也能維持 test case 不涉及操作細節, 是否就能讓 test case 高度複用? 領域流程沒變, test case 就不需要改變, 也不需要為了不同的操作方式寫好幾份 (要探索的當下, 再把 test case 跟 api spec 放在一起展開成實際的 operation steps 就好)｡ 因此流程要調整的是:

1. **"文件的維護" 左移**:  
不要維護最終操作特定介面的 test case, 只維護控制測試範圍的 decision table, 以及展開的抽象化 test case 即可｡ 其他的需求改由 AI 及時合併操作規格 (api spec) 及領域層級的 test case 替代｡ 已前面的例子來說, 若這些都準備妥當的話, 其實我真正要維護的 test case 數量, 就從 4 x 10 x 10 = 400 份文件, 降低為 10 份文件 (這合理多了)

2. **"自動化執行" 左移**:
自動化執行再拆解, 困難的地方有兩個, 一個是探索 (看規格跟情境, 對應出操作步驟的過程), 這在過去通常都要靠工程師的經驗來翻譯, 講白了就是看需求跟看規格, 呼叫 api 來完成任務｡ 而另一個環節, 則是讓他自動化執行｡ 能拆開執行的關鍵, 就是 "探索" 的結果是否有經過 review? 探索的結果是否有效的被記錄下來, 傳遞給下一階當作需求規格? 這環節解掉了, AI 自動化執行就能夠 "左移"｡


改良過的流程, 我用這張圖來表達:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-18.png)

看起來流程的安排, 之前的疑慮跟困擾都已經解開了 :D, 剩下的就是親自找一個實際案例來驗證看看了, 這也是這篇文章我想要展示的主要內容｡

後面的 demo, 我就按這順序來說明我怎麼構思這個流程:

1. **TestKit.GenTest**:  
如何從 decision table 展開 ac 的測試要求
2. **TestKit.API.Run**:  
如何從 (1) 做好 api auto test 的準備, 用 ai (用我自己開發的 mcp) 找出執行步驟
3. **TestKit.WEB.Run**:  
如何從 (1) 做好 web auto test 的準備, 用 ai (用 playwright mcp) 找出執行步驟
4. **TestKit.API.GenCode**:  
從 (1) 及 (2) 的結果, 透過 AI 生成能重複執行測試的 test code.
5. **TestKit.WEB.GenCode**:  
從 (1) 及 (3) 的結果, 透過 AI 生成能重複執行測試的 test code.


畫成流程圖, 以 "產出" 文件的視角來看, 大概就是這樣:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-19.png)

其實, 這樣的安排, 我特地把需要 "決策" 的部分交給 人腦 來判定, 而需要 "探索" 的部分交給 AI 負責, 而需要可靠且重複執行的回歸測試, 則讓 AI 一次性的生成能自動化執行測試的程式碼｡ 控制整個流程的三大核心關鍵產出, 就是:

1. 決定測試的範圍, 產出: decision table, test case(s)
1. 探索測試的步驟, 產出: test session log, test summary / report
1. 自動化執行測試, 產出: test code

實際操作這個流程, (1) 我完全靠 AI agent 來輔助 "專家" 來做決策, AI 負責內容整理的任務｡ (2) 我靠 MCP 來輔助 AI agent 能順利完成探索的步驟 (自己要先跑一次), 而記錄執行步驟, 這很機械化, 我就封裝在 MCP 內部處理掉就好了, 降低 AI agent 的認知負擔, 也有助於提升 AI 的表現｡ (3) 我則想要善用 SDD, 套用 SpecKit 來簡化我生成自動化測試程式碼的門檻, 生產出大量, 行為一致的自動化測試｡

試想一下 (3), 我需要什麼 spec, 才能寫出夠理想的測試程式碼? 其實所有必要的資訊, 都包含在 workflow 的每個階段了啊:

1. **測試案例** (test case), 測試本身要做的事情是什麼... (流程已經準備好了)
2. **操作規格** (以 api 而言, 就是 api spec), 說明操作方式跟細節, 讓 AI 能有跡可尋
3. **操作範例** (探索測試留下來的 session log 就是最好的操作範例了)

看來主要問題都解決了, 新的流程也看不出有哪邊不效率或是繞遠路, 感覺每個環節都搭配的剛剛好, 步驟分配也很平衡, 沒有哪一關特別慢或是特別花資源｡ 流程的設計大致上就到此告一段落, 接下來就是驗證是否可行了｡

想到這邊, 我總算可以說說我對 AI native workflow 的想法了｡ 最近將近兩年的時間, 我都跟著團隊推動各種 AI 相關的工作流程改變, 以及各種 AI 產品開發的架構設計, 我始終有一個感覺, AI 實際上帶來的是 "變革", 不是 "改善"｡ 我如果只把 AI 當作更快速的新工具, 直接拿來套用在 "舊流程" 內, 這樣是否是最理想的安排? 然而 ai 來的太快, 什麼是正確的 "新流程" 也沒人知道, 一切都要靠自己摸索嘗試, 這才是最困難的環節, 不過也是最有趣, 最有成就感的地方｡

最後, 補上我在 facebook 看到幾篇類似觀點的 PO 文 (當然也包括我自己寫的) 給各位參考:

- (my post)
https://www.facebook.com/share/p/177qgafB5Z/
https://www.facebook.com/share/p/1BYCtCqZod/

- (AI 變革, 但是國內企業大多停留在 輔助 的階段)
https://www.facebook.com/share/p/1HoM5rq2D9/






# 2. 用 Decision Table 定義 "有價值的測試"

前面講完整個流程的想法後, 要面對的第一個大魔王, 是測試的 "量體問題", 而我第一個要解決的題目就是:

> "  
> 我怎麼用最少的文件, 來決定真正有價值的測試是哪些?  
> "

於是, 經過一番研究, 我找到了 decision table 的做法, 這是老早就存在的技巧, 跟 AI 沒有直接的關聯, 但是當你能用 decision table 說清楚你要測試的範圍時, AI 很容易就能接手幫你完成後面各種繁瑣的苦工..

這環節就是這麼重要, 你可以讓 AI 幫你整理, 但是你一定要 review, 最終的決定權還是在你身上

--

回到起點來看測試: 測試看的就是涵蓋率, 所有的條件組合中, 有哪些情境你的測試會踩過｡ 這時你沒有用有系統的方式把所有組合列出來的話, 其實你很難確認涵蓋率的, 當然你就更難決定哪些條件組合是重要的, 以及是否有漏掉重要的路徑沒安排測試｡

決策表 (decision table), 是有系統的列出所有組合的方法｡ 我第一個示範,就是用 decision table 來讓 AI 替我建立 decision table, 讓我確認我該怎麼測, 該測哪些組合｡

![alt text](/images/2025-11-10-vibe-testing-workflow/image-20.png)

對應到前面提到的流程圖, 那麼這個章節, 我要實作的, 就是藍色框起來的部分, 從 AC 展開決策表, 決定測試範圍, 最後展開成對應的測試案例｡

先講結論, 這是很關鍵的步驟, 你可以大量依賴 AI 幫你整理跟建議, 但是這絕對是要你親自 review 的重要環節, 千萬別讓 AI 列了一版, 然後你看都沒看就回覆 OK... 我自己使用的心得是, AI 替你抓的 decision table 格式通常都正確, 但是他建議的 criteria / action / rule 通常都很糟糕, 你需要花一些心思來 review 這張表格的設計, 這會決定你後面的測試有沒有效率, 有沒有精準涵蓋你的系統流程｡

你需要先掌握 decision table 的使用方式, 你才有能力挑出 AI 的建議哪邊需要修改, 這環節省不掉的, 我強烈建議:

1. 先花一點時間, 好好看懂 decision table 在幹嘛
2. 如果要上課或是找專家, 我推薦敏捷三叔公的文章
3. 你也可以找 AI 教你, 像是 ChatGPT 就有學習模式

接下來, 我就拿我的案例實際操作一次吧! 首先要先準備我自己包裝的 testkit (github repo), 因為是 PoC, 我沒有做太多相關工具, 只有準備關鍵的 prompt, 因此操作要手動...




## 2-1. Init TestKit

有用過 speckit 的大概都清楚我要講什麼, 不過我還沒包裝成套件, 所以... 陽春一點, 就自己把 prompts 跟 mcp.json 複製到你的 workspace 吧, mcp server 我還沒準備好 release, 各位可以先玩 /testkit.gencode ..

測試的標的, 我就用之前我為了驗證 "安得魯小舖" 而開發的 API, 我簡單做了改版, 目的是用來示範這次的 AI native test workflow.. 

簡單交代幾個 "安得魯小舖" 重要的 url:
- web ui: https://shop.chicken-house.net/ , 我是前端大外行, 這是完全用 vibe coding 生出來的 web ui, 破綻很多, 純示範用途..
- rest api: https://shop.chicken-house.net/api , 自帶支援 oauth2
- open api spec: https://shop.chicken-house.net/swagger/index.html


## 2-2. Prepare Decision Table

首先, 我以 vscode + github copilot 當作我主要示範的環境 (你要換 claude code 也可), 我選用的 model: claude haiku 4.5

```

/testkit.gentest

購物車結帳時, 必須依據購物車內的商品項目與數量，配合折扣規則正確計算結帳金額

目前可接受的折扣規則有：
- 指定商品第二件六折

商店系統有這三件商品上架:
1. 啤酒, 第二件六折, 單價 65
2. 可樂, 單價 18
3. 綠茶, 單價 25

系統限制
- 單一訂單內，每種商品限購 10 個

驗證購物的身分：
- username: andrew
- password: 1234

```

其實我已經把驗收條件簡化到不能再簡化了, 這邊敘述的是我要測試結帳流程, 關鍵的驗證是 結帳金額 (包含折扣) 必須正確, 另外結帳限制也必須符合, 不能結帳的條件必須擋下來 (這部分我 api 沒有實作, 預期這個測試會失敗)｡

這些條件, 關聯到 api 以及背後系統的設定, 這邊我預先都先準備好了 (商品已經先上架, 折扣已經先設定好, 帳號已經先準備好)｡

以下是 AI 給我第一版的 decision table:


| 規則 | C1<br>有啤酒 | C2<br>啤酒≥2 | C3<br>有可樂 | C4<br>有綠茶 | C5<br>數量≤10 | A1<br>啤酒折扣 | A2<br>可樂計價 | A3<br>綠茶計價 | A4<br>正確總額 | A5<br>拒絕結帳 |
|------|:------:|:------:|:------:|:------:|:-------:|:--------:|:--------:|:--------:|:--------:|:--------:|
| **R1** | Y | Y | N | N | Y | ✔ | – | – | ✔ | – |
| **R2** | Y | N | N | N | Y | – | – | – | ✔ | – |
| **R3** | N | N | Y | N | Y | – | ✔ | – | ✔ | – |
| **R4** | N | N | N | Y | Y | – | – | ✔ | ✔ | – |
| **R5** | Y | Y | Y | N | Y | ✔ | ✔ | – | ✔ | – |
| **R6** | Y | Y | N | Y | Y | ✔ | – | ✔ | ✔ | – |
| **R7** | Y | Y | Y | Y | Y | ✔ | ✔ | ✔ | ✔ | – |
| **R8** | Y | N | Y | Y | Y | – | ✔ | ✔ | ✔ | – |
| **R9** | Y | Y | N | N | N | – | – | – | – | ✔ |


這是個符合 "規範" 的決策表, 但是並不是個理想的決策表, 因為決策表的條件 (criteria) 部分, 他已經簡化成:

- **C1**: 購物車中包含啤酒商品
- **C2**: 啤酒數量 ≥ 2 件 (可適用第二件六折)
- **C3**: 購物車中包含可樂商品
- **C4**: 購物車中包含綠茶商品
- **C5**: 所有商品單項數量都 ≤ 10 件

而動作 (action) 部分, 也已經簡化成結果, 只陳述有沒有符合:

- **A1**: 計算啤酒折扣 (第二件起六折)
- **A2**: 計算可樂金額 (無折扣)
- **A3**: 計算綠茶金額 (無折扣)
- **A4**: 回傳正確的總金額
- **A5**: 拒絕結帳 (超過購買限制)

因為系統是我設計的, 我很清楚內部運作原理, 以及我想要測試什麼｡ Decision Table 使用技巧我這邊不打算談太多, 有興趣的可以參考我上面給的方式補強｡ 我直接要求 AI 把 decision table 改成我要的樣子:

(對談過程略過, 直接列出最終的 decision table 版本)


---

| 規則 | C1<br>啤酒數量 | C2<br>可樂數量 | C3<br>綠茶數量 | A1<br>啤酒優惠組數 | A2<br>總金額 | A3<br>總優惠 | A4<br>結帳金額 | A5<br>允許結帳 |
|------|:--------:|:--------:|:--------:|:------------:|:--------:|:--------:|:----------:|:--------:|
| **R1** | 0 | 0 | 0 | 0 | $0 | $0 | $0 | ❌ 拒絕 |
| **R2** | 1 | 0 | 0 | 0 | $65 | $0 | $65 | ✅ 允許 |
| **R3** | 2 | 0 | 0 | 1 | $130 | -$26 | $104 | ✅ 允許 |
| **R4** | 3 | 0 | 0 | 1 | $195 | -$26 | $169 | ✅ 允許 |
| **R5** | 4 | 0 | 0 | 2 | $260 | -$52 | $208 | ✅ 允許 |
| **R6** | 10 | 0 | 0 | 5 | $650 | -$130 | $520 | ✅ 允許 |
| **R7** | 11 | 0 | 0 | 5 | $715 | -$130 | $585 | ❌ 拒絕 |
| **R8** | 12 | 0 | 0 | 6 | $780 | -$156 | $624 | ❌ 拒絕 |
| **R9** | 0 | 5 | 0 | 0 | $90 | $0 | $90 | ✅ 允許 |
| **R10** | 0 | 0 | 4 | 0 | $100 | $0 | $100 | ✅ 允許 |
| **R11** | 2 | 3 | 0 | 1 | $184 | -$26 | $158 | ✅ 允許 |
| **R12** | 3 | 2 | 4 | 1 | $331 | -$26 | $305 | ✅ 允許 |
| **R13** | 0 | 11 | 0 | 0 | $198 | $0 | $198 | ❌ 拒絕 |
| **R14** | 5 | 5 | 11 | 2 | $740 | -$52 | $688 | ❌ 拒絕 |

條件定義 (Conditions)
- **C1**: 購買啤酒的數量 (單價 $65，每 2 件中第 2 件 $39)
- **C2**: 購買可樂的數量 (單價 $18)
- **C3**: 購買綠茶的數量 (單價 $25)

動作定義 (Actions)
- **A1**: 啤酒優惠組數
  - 計算公式: `floor(啤酒數量 / 2)`
  - 例: 3 件 → 1 組，4 件 → 2 組，10 件 → 5 組
- **A2**: 總金額 (未折扣)
  - 計算公式: `啤酒數量 × 65 + 可樂數量 × 18 + 綠茶數量 × 25`
- **A3**: 總優惠 (負數)
  - 計算公式: `啤酒優惠組數 × -26`
  - 說明: 每組優惠省 $26 (原價 $65 - 六折 $39)
- **A4**: 結帳金額
  - 計算公式: `總金額 + 總優惠`
- **A5**: 是否允許結帳判定
  - ✅ 允許: 所有商品數量 ≤ 10 且購物車不為空
  - ❌ 拒絕: 任一商品數量 > 10 或購物車為空

規則說明 (測試企圖與重要性)
- **R1**: 空購物車
  - **測試企圖**: 驗證系統能正確拒絕空購物車結帳
  - **重要性**: 防止產生無效訂單，確保業務邏輯完整性

- **R2**: 單件啤酒無優惠
  - **測試企圖**: 驗證未達優惠門檻時，系統按原價計算
  - **重要性**: 確認優惠規則不會誤觸發，保護商家利益

- **R3**: 2 件啤酒觸發優惠 (1組)
  - **測試企圖**: 驗證剛好達到優惠門檻時，系統正確計算 1 組優惠
  - **重要性**: 這是優惠規則的**最小觸發邊界**，必須精準計算以建立客戶信任

- **R4**: 3 件啤酒 (1組優惠 + 1件原價)
  - **測試企圖**: 驗證奇數件數時，優惠組數計算正確 (只有 1 組，第 3 件原價)
  - **重要性**: 確認系統能正確處理**不完整組合**，避免多給或少給優惠

- **R5**: 4 件啤酒 (2組優惠)
  - **測試企圖**: 驗證多組優惠時，系統能正確累加計算
  - **重要性**: 確認優惠規則能正確累加，測試**整數組合**的計算邏輯

- **R6**: 10 件啤酒 (5組優惠，上限邊界)
  - **測試企圖**: 驗證達到系統限制上限時，優惠計算與允許結帳邏輯都正確
  - **重要性**: 測試**最大合法邊界**，確認 5 組優惠計算無誤，且系統不會錯誤拒絕合法訂單


啤酒超限測試
- **R7**: 啤酒超過 10 件 (11件，5組優惠)
  - **測試企圖**: 驗證系統能正確拒絕超過單項限購數量的訂單，但仍能正確計算優惠組數 (供錯誤訊息使用)
  - **重要性**: 測試**最小違規邊界** (10+1)，確保限購機制有效，且錯誤處理時金額計算邏輯不崩潰

- **R8**: 啤酒超過 10 件 (12件，6組優惠)
  - **測試企圖**: 驗證更大的違規數量 (10+2) 也能被拒絕，且能正確計算 6 組優惠
  - **重要性**: 確認限購機制不是**硬編碼單一數值**，而是真正的邏輯判斷 (>10 皆拒絕)

非優惠商品測試
- **R9**: 僅購買可樂 (無優惠商品)
  - **測試企圖**: 驗證非優惠商品的正常計價流程
  - **重要性**: 確保優惠邏輯不會影響非優惠商品，系統能正確區分商品類別

- **R10**: 僅購買綠茶 (無優惠商品)
  - **測試企圖**: 驗證另一個非優惠商品的計價邏輯
  - **重要性**: 與 R9 互補，確保所有非優惠商品都能正確處理

混合商品測試
- **R11**: 混合商品 (含 1 組優惠)
  - **測試企圖**: 驗證優惠商品與非優惠商品混合購買時，系統能正確分別計算
  - **重要性**: 這是**真實購物場景**，必須確保優惠只套用在啤酒，其他商品不受影響

- **R12**: 三種商品混合 (含 1 組優惠)
  - **測試企圖**: 驗證最複雜的購物組合，所有商品類型都存在且啤酒為奇數件時的計算正確性
  - **重要性**: 測試系統在**最複雜情境**下的穩定性，確保優惠計算邏輯不會互相干擾

其他商品超限測試
- **R13**: 可樂超過 10 件 (11件，無優惠)
  - **測試企圖**: 驗證限購規則適用於所有商品，不只優惠商品
  - **重要性**: 確認限購是**全商品通用規則**，避免規則漏洞

- **R14**: 混合商品但綠茶超限 (11件，含 2 組啤酒優惠)
  - **測試企圖**: 驗證混合購物時，系統能正確檢查每項商品的限購規則，即使其他商品合法
  - **重要性**: 確保限購檢查不會因為多商品混合而失效，保護庫存管理

---


我再強調一次, Decision Table 的內容真的要仔細看, 這是跟 AI 對焦測試內容的理解, 補足我前面過度簡略的敘述｡ Decision Table 沒弄好, 可能整組測試都沒有發揮效益 (沒測到有價值的範圍)｡ Decision Table 可以當作是 test case 的規格書, 你只要把這幫表格確定好, 基本上衍生出來的這包 test case 就安全了, 基本上最關鍵的部分就完成了｡

不得不說, 只要你給的回饋正確, 有確實 review, 也有正確回答你的期待, 其實 AI 整理的結果品質是不錯的｡ 這結果很正確, 而令我驚豔的是 AI 抓測試目的其實抓的很正確, 我要他列出測試的企圖跟重要性, 我覺得都有回達到我預期的結果｡

上表的數字, Criteria, Action 的刪減, Rules 的刪減, Table 的內容我都修正過了, 最終結果定案, 就可以繼續下一步了, 真正展開測試案例｡


## 2-3. Generate All Test Cases via Decision Table

我一直強調, 最關鍵的是 decision table, 其實確認完之後, 後面就都是 AI 的 "苦工" 了, 基本上格式在 prompt 都定義好了, 測試範圍在 decision table 都驗證好了, 這步驟就是把所有的 test case 寫出來而已｡

按照前面 decision table 的內容, 總共有 14 個 test case 需要展開, 我直接貼 AI 產完給我的摘要:

---

📁 目錄結構

```
/tests/shopping-cart-checkout/
  ├── decision-table.md              (決策表文件)
  ├── tc-01-空購物車.md
  ├── tc-02-單件啤酒無優惠.md
  ├── tc-03-2件啤酒觸發優惠.md
  ├── tc-04-3件啤酒1組優惠.md
  ├── tc-05-4件啤酒2組優惠.md
  ├── tc-06-10件啤酒5組優惠-上限邊界.md
  ├── tc-07-11件啤酒超限拒絕.md
  ├── tc-08-12件啤酒超限拒絕.md
  ├── tc-09-僅購買可樂.md
  ├── tc-10-僅購買綠茶.md
  ├── tc-11-啤酒加可樂混合.md
  ├── tc-12-三種商品全混合.md
  ├── tc-13-可樂超限拒絕.md
  └── tc-14-混合但綠茶超限.md
```

📊 測試覆蓋率分析
- **條件數**: 3 (啤酒、可樂、綠茶數量)
- **動作數**: 5 (優惠組數、總金額、總優惠、結帳金額、允許判定)
- **規則數**: 14
- **測試案例**: 14

🎯 測試類型分類
1. **正常結帳流程** (R1-R6): 空購物車、啤酒 1-10 件
2. **啤酒超限測試** (R7-R8): 11-12 件
3. **非優惠商品** (R9-R10): 可樂、綠茶
4. **混合購物** (R11-R12): 多商品組合
5. **其他商品超限** (R13-R14): 可樂、綠茶超限

🎁 每個測試案例包含
- ✅ Given-When-Then 格式
- ✅ 明確的測試資料 (輸入/輸出)
- ✅ 詳細的金額計算明細
- ✅ 業務規則驗證點
- ✅ API 呼叫序列建議
- ✅ 測試重要性說明

---


實際上的 test case 我也貼一下, 其實也寫的很不錯, 我的要求都有做到｡ 總共有 14 份, 我貼一份當作代表就好, 我貼 R2:

(為了不影響文章結構, 我移除了 markdown 的章節資訊)


---
**TC-02 單件啤酒無優惠**

**測試目標**
驗證未達優惠門檻時，系統按原價計算，確認優惠規則不會誤觸發。

**前置條件 (Given)**
- 使用者已登入 (username: `andrew`, password: `1234`)
- 購物車內商品:
  - 啤酒: 1 件 (單價 $65)
  - 可樂: 0 件
  - 綠茶: 0 件

**測試步驟 (When)**
1. 使用者將 1 件啤酒加入購物車
2. 使用者進行購物車結帳
3. 系統計算結帳金額

**預期結果 (Then)**
- ✅ 系統允許結帳
- ✅ 啤酒按原價計算 (無優惠)
- ✅ 總金額: $65
- ✅ 總優惠: $0
- ✅ 結帳金額: $65
- ✅ 訂單成立


**測試資料**

輸入參數
- 啤酒數量: 1
- 可樂數量: 0
- 綠茶數量: 0

預期輸出
- 允許結帳: ✅ 允許
- 啤酒優惠組數: 0 (未達 2 件門檻)
- 總金額 (未折扣): $65
- 總優惠: $0
- 結帳金額: $65

金額計算明細
```
啤酒: 1 件 × $65 = $65
可樂: 0 件 × $18 = $0
綠茶: 0 件 × $25 = $0
----------------------------------------
總金額 (未折扣): $65
啤酒優惠組數: floor(1/2) = 0
總優惠: 0 × -$26 = $0
----------------------------------------
結帳金額: $65 + $0 = $65
```

**業務規則驗證**
- ✅ 單件啤酒不觸發優惠 (需 2 件才享第二件六折)
- ✅ 優惠門檻控制正確

**API 呼叫序列**
1. `add_to_cart` - 加入 1 件啤酒
2. `get_cart` - 取得購物車內容
3. `calculate_total` - 計算結帳金額
4. `checkout` - 執行結帳

**測試重要性**
確認優惠規則不會誤觸發，保護商家利益。這是優惠計算的基準線測試。
---





# 3. 讓 AI 真的把 Test Case 跑起來

其實在做這個 side project, 最有成就感的環節就是, 你把一切都準備好, 然後看著他按照你要求動起來的那瞬間, 就像你疊完整屋子的骨牌, 然後推倒的那一瞬間一樣... 

![alt text](../../images/2025-11-10-vibe-testing-workflow/image-21.png)

這段, 對應到流程圖, 就是藍色框起來的部分,也就是 /testkit.api.run 負責的部分｡ 前面產完的 test case, 現在這步驟就要開始執行了｡ 來複習一下, 目前手上我有哪些測試相關的資訊:

1. test case (要測什麼的業務情境)
1. api spec (api 的操作規格, 也就是 swagger)
1. text-to-calls mcp (就是上一篇文章的 PoC 的程式碼, 重新封裝成 mcp server)

理論上有了 (1) + (2), 就有足夠資訊推導出 api 該怎麼呼叫了, 而 (3) 就是讓 AI agent 能真正自動化執行它的工具 (這是給 agent 的工具, 不是給我)

現在萬事具備了, 我就開始啟動它｡ 一次跑完太花時間了, 既然我前面都讓 AI 幫我標記重要性了, 我就挑選第一組: 正常結帳流程 (R1-R6) 來示範｡


## 3-1. Text2Calls MCP

這個步驟對應到流程, 就是第二步, 讓 AI 負責 "探索", 從規格與測試案例中找出最理想的執行步驟, 並且存成 session logs 讓後續的步驟可以依循｡


--
首先, 我在 copilot chat 視窗下了這道指令:

```
/testkit.api.run

執行: 正常結帳流程 (R1-R6)
```

整個程序就開始啟動了, 開始之前, 我簡單交代一下我的 MCP 做了什麼... 首先, 我封裝了 4 個 tools 給 agent:

- QuickStart
- ListOperations
- CreateSession
- RunStep


Agent 第一次使用, 就按照 tools description 的指示, 先呼叫 QuickStart. 其實這做法是從 Shopify 學來的 (參照我 facebook post), 我用 tools 的方式傳回明確 (而且可以是動態的提示) 的使用說明給 Agent, 最大化的正確引導 agent 來用我的 mcp:

![alt text](/images/2025-11-10-vibe-testing-workflow/image.png)


接下來, agent 果然按照 QuickStart 的指示, 先查詢 api 可用的操作有哪些了:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-1.png)

這邊特別說一下, 我的 mcp 目的是要將 api 呼叫抽象化｡ 我曾經實作過一個版本, 直接將 api 的 swagger 所有規格都直接提交出去, 不過這時我就在想, 這樣的話我還需要多包一層 mcp 嗎? 市面上有現成的 swagger -> mcp 可以用, 我其實不必要自己寫｡ 不過實驗之後, 我放棄這條路了, 因為:

- api 包含太多不必曝露給 agent 的細節
- api spec / request / response 資訊過多, 呼叫一兩次就把 agent 的 context window 塞爆了
- api 包含太多不適合給 agent 處理的動作 (尤其是機械式的操作, 例如 oauth2)

因此, 我想清楚我自己包一層 mcp 的重點了, 核心任務就是將 "呼叫 api" 這件事抽象化, 讓主要的 agent (在這裡就是 github copilot + claude sonnet 4.5) 能夠專注在主要任務 - 執行 test case 上, 排除一切干擾 (包含不必要的 context, 不必要麻煩他操作的細節)

所以我的抽象化切角就是:

- ListOperations, 用文字敘述 (只有 api-name 是明確 id) 告訴 agent 有哪些 api 可以用
- CreateStssion, 明確告訴 agent, 建立 session 來收納一個 test case 的執行過程記錄
- RunStep, 接受 agent 的指示, 在當前的 session 內要執行指定的 operation, 並且用文字敘述告知 mcp 操作的內容 ( mcp 內部還有另一個 LLM, 會真正負責解析 text -> api call 的參數對應跟生成)

經過這樣拆分, 整個 testkit + mcp 的執行才開始順暢起來, 先前的版本總是卡在奇怪的地方, 然後 agent 就暴走亂回答一通了｡ 這是某種形態的 context engineering, 有效率的控制主要 agent 的 context window 不要被干擾的案例, 這也是某種形式的 sub-agent 縮影, 只是中間通訊的 protocol 是 mcp (model context protocol), 而不是 a2a (agent to agent protocol) 而已.

也因為 RunStep 做的事情是翻譯 text -> api call, 這個 mcp server 我才會取名為 Text2Calls MCP.

Agent 透過 list operations, 並且對照 TC-01 test case, 列出可用的操作有這四個:

- GetCart - 取得購物車內容
- CreateCart - 建立新購物車
- EstimatePrice - 試算結帳金額
- CreateCheckout - 建立結帳交易

用這四個操作就足以組合出 test case 01 的情境了, 接下來 agent 真正要開始執行, 於是就先執行 CreateSession:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-2.png)

建立完 session, 其實 MCP 在背後也先把執行過程中必要的 OAuth2 認證先跑完, 並且順利取得 access token .. 這些操作如果不是在 MCP 內用 code 處理, 我不知道還要寫多少 prompt 才教的會 AI 來做這些事...

![alt text](/images/2025-11-10-vibe-testing-workflow/image-3.png)


接著, 開始真正執行 RunStep, 呼叫 API:

// ![alt text](/images/2025-11-10-vibe-testing-workflow/image-4.png)

![alt text](/images/2025-11-10-vibe-testing-workflow/image-5.png)

我特別貼一下 agent 傳給 RunStep 的資訊, 都被抽象化了, 只剩下 operation, action, context:

```json
{ 
    "operation": "CreateCart", 
    "action": "create an empty cart", 
    "context": "User andrew logged in. Create a new empty cart to test checkout with no items. This should later fail at checkout." 
}
```

基本上 agent 只是交代 mcp 要 run 哪個 api (operation), 想要他完成的任務 (action) 是什麼, 完成這任務的背景是什麼 (context), 其餘就是 mcp 按照這些資訊, 搭配 mcp 內部自己 own 的 LLM, 觸發 function calling 機制來完成 api call 的任務

RunStep 這操作, 執行過程中可能會呼叫 1 ~ N 次不同的 api, 這些過程都會在 _session 目錄下留下足跡｡ 舉例來說 RunStep -> CreateCart, 就真正對應到 POST /api/carts/create, 具體的 api call request / response 都有被記錄下來:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-6.png)

而抽象化的執行順序, 也會被記錄在 session-logs.md 下 (我節錄 RunStep[1] 的片段, 這些都是 mcp 自己留下的記錄):

---

**RunStep[1], CreateCart**

**action**:
create an empty cart

**context**:
User andrew logged in. Create a new empty cart to test checkout with no items. This should later fail at checkout.

- http(5) Post https://shop.chicken-house.net/api/carts/create => 201 Created
  - [request](005-api-request.txt)
  - [response](005-api-response.txt)

**result**:

**answer**:  
已成功建立一個空購物車（id=163）。執行成功且結果如預期：回傳的購物車內容為空（prodQtyMap 與 lineItems 均為空），可用於後續驗證「空購物車結帳應被拒絕」的測試。

**details**:  
依據執行結果，API 回應內容如下（已轉成可讀敘述）：
- 請求：POST https://shop.chicken-house.net:443/api/carts/create
- 回應 Content-Type: application/json; charset=utf-8
- 回應 JSON 內容：{
    "id": 163,
    "prodQtyMap": {},
    "lineItems": []
  }
敘述說明：系統已建立一個新的購物車，購物車編號為 163；目前購物車內沒有任何商品（prodQtyMap 空物件，lineItems 為空陣列），符合「建立空購物車」的需求與測試前置條件。無 HTTP 錯誤或例外回傳，操作順利完成。

**instructions**:  
下一步建議：
1) 嘗試呼叫 CreateCheckout（payload: { cartId: 163 }）以確認系統在建立結帳交易階段是否拒絕空購物車。期望行為：API 應回傳錯誤（或拒絕建立交易），並說明原因（例如：「購物車為空，無法結帳」）。
2) 若 CreateCheckout 被接受（意外情況），請接著模擬付款並呼叫 CompleteCheckout，檢查是否會在完成結帳時被阻擋；若 API 回傳成功建立訂單，視為系統行為不符合預期，需進一步調查。 
3) 測試檢查要點：確認 API 回傳的錯誤碼與錯誤訊息，並在測試結果中斷言系統應拒絕空購物車結帳以防無效訂單產生。
4) 若需要，我可以代為執行 CreateCheckout 並回報其實際回應內容以完成此測試情境驗證。

**summary**:  
前次 context：使用者 andrew 已登入，目的是測試空購物車結帳場景（購物車內無任何商品，系統應拒絕結帳並顯示錯誤訊息）。
此次 action：CreateCart — 建立一個新的空購物車。
此次 answer：建立成功，系統回傳購物車 id=163，prodQtyMap 與 lineItems 均為空，符合建立空購物車的預期。後續請嘗試呼叫 CreateCheckout (cartId=163) 以驗證系統是否正確阻止空購物車結帳；我可以協助執行並回報結果。


---


## 3-2. Session Logs

前面的 "探索" 完成後, 接下來要看看 session logs 留下了哪些線索給下一步使用｡ 這步驟關鍵的地方在於, 探索的結果不一定每次都是理想的, 你可能會挖到寶藏, 也可能會撲空什麼都沒找到｡  因此我希望 "真人" 能夠透過 session logs 的 review 來確認 AI 探索的結果是否符合期待, 不管是修正 prompts, 或是微調 test case, 直到得到你預期的結果為止｡ 

先來看測試案例 TC-01, 這測試案例講的是, API 應該要拒絕空的購物車結帳請求才對, 不過實際上我的 API 並沒有實作這限制, 所以我預期這個測試應該會成功的失敗才對 (咦?


測試的 test case 主要部分如下:

---

**前置條件 (Given)**
- 使用者已登入 (username: `andrew`, password: `1234`)
- 購物車為空
- 購物車內商品數量:
  - 啤酒: 0 件
  - 可樂: 0 件
  - 綠茶: 0 件

**測試步驟 (When)**
1. 使用者嘗試進行購物車結帳
2. 系統檢查購物車內容

**預期結果 (Then)**
- ❌ 系統拒絕結帳
- ✅ 顯示錯誤訊息: "購物車為空，無法結帳"
- ✅ 不產生訂單
- ✅ 購物車狀態維持為空

---

實際執行的步驟, 到 Step3 之前都還正確 (建立購物車, 計算購物車金額-空車):

![alt text](/images/2025-11-10-vibe-testing-workflow/image-7.png)


然而, 到了真正要執行結帳的時候, api 呼叫就不如預期了:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-8.png)


看到這邊, 可以放心了, agent + mcp 有認真替你執行 test case 的每個步驟, 也有確實檢驗結果是否符合預期｡

## 3-3. Test Suite Result

看完一筆 test case 執行過程, 其他剩下的 test case 我就跳過了, 直接看最後結果 (我只 RUN 了基本測試, TC-01 ~ 06):

我有實作基本功能, 但是基本上邊界的檢驗都沒做, 所以基本測試 TC-01 空購物車結帳測試預期會失敗, 其他正常結帳跟折扣計算應該都會通過 (TC-02 ~ 06), 如果後面還有執行超過 10 件商品的測試應該也會失敗 (應該要拒絕結帳, 但是實際會結帳成功), 這些都驗證測試有發會效用｡ 我擷圖讓各位看一下 AI 給我的摘要報告:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-9.png)

針對測試失敗的部分 (TC-01), AI 也特別說明他判定問題出在哪邊

![alt text](/images/2025-11-10-vibe-testing-workflow/image-10.png)

而這些執行過程的記錄, 通通都鉅細靡遺的保存在 session folder 內:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-11.png)

001, 002 -> oauth 認證過程
003 -> 下載保存 openapi spec
004 -> mcp 呼叫 LLM 處理 function calling
005 -> call api: create cart
... (懶的寫了, 總之當成 postman 來看待就好)

openapi-spec.json, 就是 003 保存下來的 api spec 快照
session-logs.md, 就是呼叫過程中的抽象資訊傳輸記錄, agent / mcp 的通訊內容都再這邊, 基本上這是給 "人" 看的歷程...

而我會設計 mcp 要保留這些資訊, 目的很單純, 這些都是將來要寫 "api 自動化測試" 最重要的規格, 以後這些都會是餵給 spec-kit 執行的養分｡

不過, 這篇我不打算示範到 test code generation, 有興趣的可以自己先拿去玩玩看, 或是等我下一篇文章再來介紹｡



# 4. 同時適用於 API 與 UI 的 Test Case

這部份, 對應到流程的第三步, 前面我希望降低文件的數量, 用最小的文件量來抓出最有價值的測試組合, 而另一個關鍵就是要最大化 test case 的使用範圍, 一個 test case 能夠同時應用於多種操作介面 ( 前面示範了 API, 現在我要示範 Web UI )｡ 同時處理這兩個維度, 我就能盡我可能的將關鍵的文件數量壓到最低｡

因此, 我希望 test case 能排除操作細節, 方便 review 以及降低維護文件的負擔｡ 而被省略的操作細節, 將來需要時 (可以靠 AI) 應該要從 spec 對照, 來還原需要的操作細節 (前面 #3 已經證實可行)｡

這件事如果做到極致, 那我就能用同樣的 test case 來對應不同的測試目標｡ 如果我的服務, 同時提供 web 介面, 也提供 api 介面, 理論上這些測試案例應該都是共通的啊 (行為應該一致, 但是有不同的操作手段), 這一段, 我就拿一模一樣的 test case, 把 mcp 從我自己寫的 Text2Calls MCP 換成 Playwright MCP, 看看能否完成同樣的測試行為?

![alt text](/images/2025-11-10-vibe-testing-workflow/image-22.png)

於是, 我用同樣的架構來對比, API 規格換成 UI 規格 (最後我沒有用這個), 能自動呼叫 API 的 MCP (Text2Calls) 換成能自動操作網頁的 MCP (Playwright), 我嘗試復刻一樣的成果出來, 只要能達成同樣的效果, 那就證明我真的能做到 test case 的複用, 也才能真正降低測試案例維護的負擔, 同時不會降低測試的覆蓋率｡

拜 coding agent 所賜, 這次我特地準備了一個有 Web UI 的 "安得魯小舖"｡ 我是前端大外行, 終於也能生出還過的去的純前端測試網站了 XDD, 這是 React + NodeJS 的前後端架構, 背後 call 同樣的 API 搭建的 web app, 有興趣的人可以進去玩玩看:

Andrew Shop: https://shop.chicken-house.net

這次, 同樣來看看我準備了什麼資訊:

1. test case (要測什麼的業務情境, 同 api)
1. ui spec (我沒有提供, 但是我開發的 web ui, 特別遵循無障礙的設計, accessibility, 後面說明)
1. playwright mcp

mcp 的安裝設定我就不講了, 大家自己看 playwright mcp 的官網, 我開啟 copilot chat 輸入這段指令:

```
/testkit.web.run

執行: 正常結帳流程 (R1-R6)

```

這段我沒有另外再寫 MCP 來支持這些操作, 例如 session, logs 等等, 因此這部份就簡單處理 (我直接寫在 prompt 而已), 所以 agent 一樣會幫我建立 session folder, 只是是靠 shell ..

草草的準備好測試環境之後 (咦? , 正式的測試就要開始了, 第一次呼叫 playwright mcp, 就是啟動首頁:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-12.png)


經過一連串的嘗試 (略過), 完成 OAuth2 認證, 登入成功:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-13.png)

接下來你可以看到 agent 不斷的在跑, 而 chrome 也自己在操作, 等了幾分鐘之後, 開始測試結帳:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-14.png)

看起來操作都很順利, 操作的過程都沒有脫離 test case 的敘述, 跑了大約 20 min (真的很慢), 六個測試項目執行完畢｡

比較不一樣的結果是: 這次六個測試都成功通過!

![alt text](/images/2025-11-10-vibe-testing-workflow/image-15.png)

這並不是測試結果不正確喔, 我在刻這個 web ui 的時候, 我在 api 層級沒有加上的限制或是檢查, 我都有在 ui 這層補上｡ 因此 TC-01 空的購物車不能結帳, 這動作在 UI 的確是有實作的, 我擷圖證實一下:

![alt text](/images/2025-11-10-vibe-testing-workflow/image-16.png)

購物車是空的時候, 結帳的按鈕整個被藏起來沒辦法按, 所以 TC-01 測試通過


## 4-1. API 需要 SPEC, UI 不需要?

同樣的, 這題也是過去我卡很久的題目...

不知道有多少人認真拿 playwright mcp 來做類似的事情?

簡單的網頁都很正常, 複雜一點的, 有的光是 login 就卡半天了, 要嘛就是找不到按鈕再哪裡, 要嗎就是按不到, 總之就是各種詭異的狀況, 明明按鈕或是欄位就在那邊, 你看的到, 但是 AI 就是看不到

其實這就是 "規格" 啊, 人看的到, 是因為最終是從 "視覺" 上來判定的, 但是 AI 不是｡ 其實有 solution, 的確是擷取畫面, 然後進行影像辨識來判定的, 這技術可行, 不過運算資源花太兇了, 現在的運算能力還沒辦法做到即時 (一張 4K 的圖, 要判定按鈕再哪, 該不該按下去, 就要幾十秒鐘了)

我試過其他的套件, 例如 selenium mcp, 它的辨識能力就 "好的多", 因為他直接判定 HTML, 基本上頁面所有細節都能檢測的到, 不過 HTML 太肥了, 動不動就 100kb 以上, 很快的 agent 的 context windows 就被佔滿了, 根本無法實際使用

playwright mcp 實用的多, 因為他會先將 HTML 精簡成自訂格式的 yaml 結構, 只是他是如何判定哪些資訊該保留下來? 我爬了很多文, 最後是厲害的同事逆向工程挖出來的, playwright 靠的是 "無障礙" 的網頁標記資訊｡

這招其實很聰明, 我聽到關鍵字 "無障礙" (Accessibility) 我就想通了, 先前看 A16Z - AI Era 趨勢報告, 就提到這一段: [# 6, Accessibility as the universal interface]

跟人類的感官相比, 現在的 AI 對於你的電腦, 就像個殘障人士.. AI agent 既沒有辦法幫你操作鍵盤滑鼠 (除非你授權), 也看不到你畫面上現在呈現的是什麼 (除非你授權擷圖), 沒辦法聽到你在聽什麼音樂, 沒辦法 ...., 而無障礙的設計, 就是讓應用程式能讓這些人也有機會 "看到" 網頁的內容的設計, 看不到字, 可以用唸的, 看不到圖, 可以用文字描述, 唸給你聽..

講到這邊, 串起來了嗎? 如果你的網頁有確實做好 "無障礙" 設計的話, 對於 AI 這種視覺障礙的人士, 它就多了一個能理解網頁資訊的管道了｡ 而 playwright mcp 也大量依靠這些標記來 "認識" 你的網頁怎麼操作｡ 因此, 我的 Andrew Shop WebUI, 除了 OAuth2 Login 那一頁還沒修正之外 (所以 login 的地方卡比較久), 其他都是這次新開發的, 我在開發之初, 就明確的用 spec-kit 說明, 全站都要遵守無障礙的設計原則 (不過具體是什麼我也搞不懂 XDD), 換來的就是 AI 能夠清楚辨識該如何操作我的頁面了

簡單摘要一下, 我嘗試過的幾種 讓 AI (use playwright mcp) 能 "認得" 你網頁方法:

1. 不處理, 直接上
1. 做好無障礙的設計
1. 在 testcase 標記 HTML id



有興趣的人可以試試看, 我把這個測試專案放到 GitHub, 你可以用 specstory 看我跟 agent 的對談記錄, 也可以看測試產生的測試報告, 你會發現雖然 UI test 跑很慢 (6 個測試案例就花掉 2x 分鐘)


# 5. 未完成的部分: Generate Test Code

這段, 對應到整體流程的最後一步, 把探索的結果, 用生成 test code 的方式來實現自動化｡不過 Generate Test Code 這段我先不展開, 我打算改用 SpecKit 重做一次 PoC 驗證之後, 在下一篇再談｡

![alt text](/images/2025-11-10-vibe-testing-workflow/image-23.png)

整個流程的拼圖, 其實還有最後兩塊, 就是塗上標的藍色部分, 分別是 testkit.api.gencode, 跟 testkit.web.gencode. 這段其實我自己已經有實作出來了, 結果也確實可行, 甚至我連操作錄影都錄好了, 先前也在我 facebook 發表過:

// facebook post share url

不過, 這段我想要用 SpecKit 重新調整一次, 我會覺得更合適｡ 實際上的自動化測試, 背後通常都會有一套測試管理用的平台, 並且搭配 CI/CD, 在適當的時候 ( deployment, or release ) 觸發測試的需求, 並且統一收集測試數據來分析 & 監控告警｡

因此, 我相信這些測試都有一定的標準規格需要符合, 包含:

1. 如何啟動
1. 如何輸出測試結果
1. 測試步驟的客製化注入
1. 測試的環境準備
1. 測試的 secret 管理
1. ...

這些, 都高度需要全系統的統一, 我不覺的是我自己寫一段 prompt 生出能動的 code 就能結束的任務, 因此我決定把這段切出來, 改用 SpecKit 來替代, 我只要準備好 test case 相關的必要規格 (就前面提到的 session logs), 剩下的就交給專家吧! 因此這篇我就不提 gencode, 有興趣的朋友可以等我下一篇 (如果我搞的定的話), 或是各位可以自己捲起袖子嘗試看看!


整個 side project 我做到這邊, 回頭看看先前的流程, 我終於覺得我不再只是拿新工具套用舊流程了, 重新思考過的工作流程, 我覺得人要扮演的角色, 以及 AI 要解決的問題更明確, 也更精準了｡ 觀念 / 流程 / 實作我覺得都有到位, 不知各位看法如何?


# 6. 結語: 為 AI 設計新流程, 而不是替舊流程加速

回頭看這次的 side project，其實我做的事可以粗略拆成三點：

1. **觀念: 認清「測試通膨」是結構問題，不是人力問題**
   解決結構問題, 會帶來數量級的改變; 替換工具, 會帶來線性的加速, 如果你面對的問題同時包含兩者, 務必優先解決結構問題｡ 結構問題通常都隱藏在流程內, 過度專注工具你會容易忽略掉結構問題｡

2. **流程: 把 AI 放在「左移」的位置**  
   能在問題被擴大之前就解決它是最理想的, 威力越強大的技能, 盡可能的用在問題的根源｡
   - 用 Decision Table 把測試範圍與重要性想清楚  
   - 讓 AI 幫忙把 decision table 展開成成套的 test cases  
   - 再讓 AI 去「探索」一次 API / UI，把探索過程變成操作範例與 log  
   也就是：**AI 重點不是在「多跑幾次」，而是在探索, 在「幫你產出可以重用的自動化資產」**。  
   後面重跑的部分，交給機器、CI/CD 去處理就好。

3. **實作: 刻意留出讓 AI 好發揮的介面**  
   - API 這邊用 Text2Calls MCP，把「怎麼 call API」抽象成 operation + action + context  
   - UI 這邊遵守 accessibility 的設計，讓 Playwright MCP 看得懂你的頁面在幹嘛  
   - 所有執行過程都留下規格化的 log，未來可以直接餵給 Speckit 這類工具產生自動化 test code  


事後回想, 這套 workflow 對我來說，有幾個很明確的收穫：

- 我更少在跟 AI 對話「請幫我多跑幾次 XXX」，而是把心力放在「這個 decision table 長得對不對」  
- 我開始刻意設計 API / UI 的「可測試性」，而不是等系統做完才問：欸這要怎麼測？  
- 我可以接受某些地方先暫時用 AI interactive 模式跑，等 log 穩定了，再補自動化測試，而不是一開始就企圖從 0 到 100 分

當然，這篇還只是半場而已。  
真正有趣的下半場，是把這些執行記錄、decision table、test case 規格通通餵進 Speckit 之類的工具，讓它幫我把「會動的自動化測試 code」生出來，然後直接掛進 CI pipeline 裡。

如果你現在也在推 AI 導入，尤其是測試／QA 流程相關，我會建議你可以先問自己這個問題：

> 我現在做的是「讓 AI 加速舊流程」，  
> 還是「因為有了 AI，所以我敢把流程拆開重組」？

這篇文章就是我的一個版本答案，後面還會繼續修正，但至少，我已經不太滿足於「多了一個 AI 小幫手」的那種導入方式了。



>>>

