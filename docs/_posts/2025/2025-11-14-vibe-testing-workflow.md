---
layout: post
title: "Vibe Testing Workflow 實作"
categories:
- "系列文章: 架構師觀點"
tags: ["架構師觀點","技術隨筆"]
published: false
comments_disqus: false
comments_facebook: false
comments_gitalk: false
redirect_from:
logo: 
---

 
<!--more-->


上一篇: [從 Intent 到 Assertion #1, 聊聊 Vibe Testing 實驗心得](/2025/05/01/vibe-testing-poc/)











# 1, AI native testing workflow

對我來說, 測試是件相當頭痛的問題, 主要的痛點在於測試的 "量體" 太大, 背後包含寫不完的測試案例, 準備不完的測試資料, 以及跑不完的手動 & 自動測試｡即使有 AI 加持, 你要測完所有的狀況也幾乎是不可能的事情, 勢必要有一些方法, 有效率並且優先完成重要有價值的測試才是正途｡

為了聚焦我的主題跟思路, 不相關的環節我就大幅簡化了, 我用這張圖來說明:

// P39, https://docs.google.com/presentation/d/1ZYtaDZ0igCbUaN4UwKAPeo06YViOFO0CAPlfQYke0uY/edit?slide=id.g3a1eaa2cc57_1_24#slide=id.g3a1eaa2cc57_1_24

從左到右, 是從需求規格中的驗收條件 (AC, Acceptance Criteria) 開始, 到測試結果的流程｡ 每個步驟上面標的數字, 就是可能要展開的數量 (我只抓數量級, 簡單示意用)｡試想一下, 如果你的系統需求列了一項驗收條件 (ex: 購物車結帳時必須檢查折扣計算規則, 以及購買條件限制), 最終應該展開多少測試案例? 在系統的生命週期內你需要執行幾次測試? 成本與代價是什麼?

**量體估算**:

- 1 個 AC, 可能會展開 10 個 Test Case
- 你的系統可能會提供至少 4 種操作方式 (我想像的例子: WebUI, Android APP, iOS APP, RESTFUL API)
- 10 種非功能性需求 (NFR, Non Functional Requirement, 例如資安, 授權, 效能, 交易正確性等等)


**執行瓶頸**:

1. **測項數量**:  
如果這些組合都展開, 每一個 AC 就必須要維護 4 + 10 + 10 份規格文件, 以及產生出來的 1 x 4 x 10 x 10 = 400 份不同的測試腳本... 這量體, 光是文件就已經難以維護了..  

2. **測試執行次數**:  
就算真的有 400 份測試案例, 一年你會需要跑幾輪測試?  
假設一週 release 一次就好, 每次 release 前內部 QA 至少測一次就好, release 後正式環境也至少測一次, 那一年應該要跑 50 x 2 = 100 輪, 總共是 400 x 100 = 40,000 次測試...

這時, 我回頭看看我半年前自己寫的文章, 我想打臉我自己的想法了啊 XDD, 如果我真的用 vibe testing, 每次都讓 AI 讀取 test case 當下執行測試, 試算一下跑 40000 次要花多少時間跟費用... 每跑完一次大約需要花掉 2 分鐘, 而對應的 api token 支出, 大概也要花掉 USD$ 0.03 (剛好約 NTD$ 1.0) 左右吧, 一年下來要跑 2 x 40000 = 80000 分鐘, 不眠不休就要跑 55.56 天 (如果不平行處理的話), 整個過程共計要花掉 4 萬台幣的 token 費用... 而這只是完成 "一個 AC" 的要求需要的測試而已...

怎麼想都不可能啊 XDDD... 這時, 我發現我掉到拿 "新工具" 套 "舊流程" 的陷阱了｡ 我的想法的確比單純用 "人力" 來測試好很多, 但是遠遠跟不上測試的通膨..., 而且 AI 執行測試也有缺點, 除了速度跟費用之外, AI 每次執行的結果都有一些差異, 這也是另一個頭痛的問題｡

因此, 我重新想過這兩個瓶頸, 我的直覺告訴我 AI 是能搞定這問題的, 但是絕對不是用 AI 代替人工這麼簡單｡ 我的流程應該要重新思考, 否則我只是讓 AI 自動的執行 "手動測試" 而已 (把 AI 當真人看的話, 我只是拿 token 換時薪而已)｡ 我真正該做的, 是讓 AI 有效率的做真正的自動化測試 (寫自動化測試的程式碼, 然後系統化的自動執行)


因此, 我調整想法, 這兩個瓶頸, 應該要在產生之前就解決掉他｡ 過去這類 pattern 都是 "左移" 就能搞定, 因此我想要:

1. **"文件的維護" 左移**:  
如果我要維護的文件範圍往流程的左邊移動, 我就能在文件的數量放大之前就維護好的話, 數量級就降下來了｡ 假設我能簡化 test case 的內容, 只保留抽象的執行步驟, 不要有任何具體的操作細節 (例如: 按下某個按鈕, 或是呼叫某個明確的 API + 參數...), 我要維護的 test case 就只有 10 個了, 不過最終我仍然需要展開 source code, 這時我需要 "明確操作的規格說明書", 以 api 來說就是 api spec, 加上這些都準備妥當的話, 其實我真正要維護的文件數量, 就從 4 x 10 x 10 + 4 + 10 + 10 = 425 份文件, 降低為 4 + 10 + 10 = 25 份文件 (這合理多了)

2. **"自動化執行" 左移**:
從原本想像的 AI 自動的 "手動執行", 變成一次性的執行, 之後把這次執行的步驟輸出成 source code, 我只需要花費一次 AI 執行的時間成本 (GPU 費用), 之後重複執行的就都是單純運算成本 (CPU 的費用) 了｡ 整個流程只要能讓 AI 寫完 400 份自動測試的 code, 後面的執行成本就低到可以不用特別計算了 (你的網頁被點閱 40000 次要多少錢? 基本上很便宜, 你甚至用既有的測試環境就足以負荷)

改良過的流程, 變成這張圖:

// P40, https://docs.google.com/presentation/d/1ZYtaDZ0igCbUaN4UwKAPeo06YViOFO0CAPlfQYke0uY/edit?slide=id.g3a1eaa2cc57_1_69#slide=id.g3a1eaa2cc57_1_69


不過, 這是 "理想", 實作的過程中藏了不少貓膩, 像是 "執行記錄" 就是之一, 後面我介紹實作的時候再來聊這段｡

到這裡為止, 就是我對真正導入 AI testing 的想法, 包含了技術的掌握 (上一篇: 用 LLM function calling 的能力來執行 API 自動呼叫的能力), 也包含了流程的調整 (就是這篇: AI testing workflow)｡

後面的 demo, 我就按這順序來說明我怎麼構思這個流程｡ 在做這個 side project 的過程中, 正好碰到 speckit 的興起, 我突然發現用這樣的方式交付我構想的流程好像還不錯, 因此我就花了點時間把這些流程包裝成 "testkit" 了｡對比 speckit 有一系列的步驟 (ex: specify, clearify, plan, task ... etc), 我也把 test 的步驟依樣話葫蘆的展開:

1. ```/testkit.gentest```,  
如何從 decision table 展開 ac 的測試要求
2. ```/testkit.api.run```, 如何從 (1) 做好 api auto test 的準備, 用 ai (用我自己開發的 mcp) 找出執行步驟
3. ```/testkit.web.run```, 如何從 (1) 做好 web auto test 的準備, 用 ai (用 playwright mcp) 找出執行步驟

而 (2), (3) 會產出執行記錄, 這很關鍵,我最早測試的版本, 是把 api spec, 跟 test case 直接丟給 AI 當作規格, 讓他直接生成 test code, 這其實可行, 但是產生的 api 呼叫步驟不一定 100% 正確, 我通常要整個 code 寫完才能測試, 效率並不是很好｡ 關鍵原因主要在於, api spec 的資訊, 一般而言都只專注在參數規格, 整個使用的流程並沒有交代的很清楚｡ 而 [上一篇] 談 vibe testing 的示範, 因為是互動的流程, AI 有機會探索 + 重複嘗試, 反而這樣的過程比完全靜態 (只看文件) 更有機會找出正確的執行步驟｡ 我突然聯想到, 真人在寫 code 呼叫 api 的時候, 不也都會在真正寫 code 前, 先用 postman 試著跑看看嗎? 搞懂了那些 api 跟參數用法之後, 才真的把他寫成 code 會容易的多｡ 因此我也在 ai testing 的流程中插入先前研究的 vibe testing, 先讓 ai 跑一次, 並且把執行過程的詳細記錄都保留下來, 就像你讓 ai 先用 postman 跑完後再把 postman 的記錄檔案翻譯成 auto test code 一樣｡

而最後我發現最有效的方法, 也不是自己寫 prompt 讓 ai 寫 code 了, 因為這段研究過程中 speckit 出現了 (這世界到底要變化的多快啊...), 我開始將策略改成: 提供足夠精準的 auto test 需求規格, 用 speckit 幫我寫成 auto test code.. 這是目前我能想到最理想的流程, 於是, 後半段的關鍵就是 - 我能給多好的規格?

從這角度來想, 我發現我都準備好了啊, 一個理想的 auto test code 需要的規格, 不外乎:

1. **測試案例** (test case), 測試本身要做的事情是什麼... (這前面 ac 展開的 test case 就是了)
2. **操作規格** (以 api 而言, 就是 api spec), 說明操作方式跟細節, 讓 AI 能有跡可尋
3. **操作範例** (就是前面提到, postman 執行的記錄, 可以讓 ai 有時記得案例參考, 能一次到位, 寫出正確的 test code)

受惠於上一篇 vibe testing 的研究成果, 只要把 ai 執行的 log 忠實的存下來, 我就能很輕易的補足 (3) 了, 因此後半段的作業流程也都補齊了, 測試效果也很令人滿意｡ 不過礙於篇幅, 後半段的 demo 我就留到下一篇再聊, 這篇我先把前半段的部分補完｡

再繼續往下拆解 demo 過程之前, 我在這邊先總結一下我對 ai (native) test workflow 的想法吧｡ 最近將近兩年的時間, 我都跟著團隊推動各種 AI 相關的工作流程改變, 以及各種 AI 產品開發的架構設計, 我始終有一個感覺, AI 帶來其實是個 "變革", 不只是 "改善" 而已｡ 我一直在提醒自己, 別掉入用 "新工具" 套 "舊流程" 的陷阱, 而 ai 來的太快, 什麼是正確的 "新流程" 也沒人知道, 一切都要靠自己摸索嘗試, 這才是最困難的環節, 不過也是最有趣, 最有成就感的地方｡

最後, 補上我在 facebook 看到幾篇類似觀點的 PO 文 (當然也包括我自己寫的) 給各位參考:

- (my post)
- (AI 變革, 但是國內企業大多停留在 輔助 的階段)





# 2, 從 “接受條件” (AC) 展開 "測試項目” (Test Case), 重點在於決定哪些是有效的測試｡ 

如果單純從 "涵蓋率” 來看待測試, 你可能會浪費很多時間再不必要的測試上, 而這些 "重要性” 的判斷, 到目前為止, 我覺得還是由熟悉領域的人來判定還是比較合適｡ 這就跟目前即使 AI 的程式碼寫的再好, 現在仍然少不了 code review / unit test 的階段是一樣的道理｡

不過, 即使要人工判定, 不代表所有事情都需要人工來進行, AI 仍然有能夠貢獻的地方｡

AI 能力很強, 很多人都直接讓他一步到位, 但是我想反其道而行, 只讓他展開 “簡單易懂”, 只描述商業情境的測式項目, 因為我希望這測試內容能重複使用, 因此我不能在這步驟納入詳細的 "操作” 資訊｡操作資訊如果能延後到在下個環節再各自補上, 我就能維持這個階段產出 (測試項目) 的通用性｡

這步驟完成後, 基本上測試的範圍就確認了, 接下來就是展開測試步驟 (對應到 API 的執行步驟)

# 3, 從 "測試項目” (Test Case), 對照設計規格 (API Spec), 重點再理解需求與規格,並找出 (探索) 合理的 API 執行步驟

這部份, 我從過去的經驗來切入吧｡ 以 API 自動化測試來當例子, 其實 API 相對於其他性質的系統, 算是最好自動化測試的標的了｡ 因為 API 先天就是為了跨系統整合的目的而生的, API 自動化測試, 換個角度想, 其實也只是 “待測 API” 與 "測試管理平台” 之間的 "整合” 而已啊! 

而負責 "整合” 的角色, 就是工程師了｡ 不過整合測試 (integration test) 跟單元測試 (unit testing) 屬性不同, 有些團隊不一定會讓原本負責開發 API 的工程師來負責, 這時這就是 "測試工程師” 的主要任務了｡

這會有什麼問題? 能稱作 "工程師”, 通常技術能力都有一定的水準｡ 這些人來寫自動化測試, 其實最大的障礙就是他不一定熟悉､或是完全理解 API 的用途｡傳統的做法 (如果是我), 我拿到一個新的 API, 要我寫自動化測試, 我大概會這樣做:

1. 先閱讀 API 規格, 了解技術限制, 或是搭配框架等等｡ 例如是 REST, 或是 gRPC … 等等
2. 先看看測試需求, 是否已有人準備好測試案例? 以我待會要示範的例子來說, 購物車放哪些商品, 能符合什麼折扣? 這些若已經有定義好, 我的任務就更明確了
3. 按照測試情境, 可能有些先決條件或是環境要準備｡ 以我待會要示範的情境來說, 我需要先準備的是上架商品, 跟折扣的設定｡能夠預先標準化最好, 否則你就需要再測試步驟中包含他｡
4. 這些測試情境若定義明確, 下個步驟就是把步驟對應成 API 呼叫了｡ 我覺得這是最燒腦袋的任務, 過去沒 AI 輔助的情況下, 這完全就是吃工程師的開發技能｡ 越複雜的任務, 對應 API 的結果越多樣, 甚至也有可能因為規格設計不佳, 導致這個需求情境根本無法用現在的 API 規格完成｡

這步驟完成後, 基本上測試的步驟就確定了, 接下來就是如何精確有效率的重複執行 & 驗證結果｡

# 4, 確認後的執行步驟, 要有效率 (速度 + 成本) 且精準的重複執行

只要是測試, 基本的期待都一樣, 就是保護機制｡ 我改了程式, 或是做了任何變更, 我需要測試很快告訴我我的變更有沒有不知不覺的影響了其他功能而不自知? 這就是測試的主要目的｡因此測試越多, 涵蓋的範圍越廣, 防護就越周全｡

但是, 偏偏這幾點 (速度, 成本, 重複執行的一致性) 都是目前生成式 AI 的罩門啊…

// 正確手段應該是善用 AI 的能力, 把 1-2 的步驟番寫成 test code, 而非每次都當場跑一次 1-2 ..