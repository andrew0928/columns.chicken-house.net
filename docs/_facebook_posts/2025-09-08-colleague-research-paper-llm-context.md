---
date: 2025-09-08
datetime: 2025-09-08T19:36:45+08:00
timestamp_utc: 1757331405
title: "同事丟了篇論文給我研究 (推坑? , 結果還蠻有意思的, 決定 PO 文記錄一下。這篇論文是在探討"
---

同事丟了篇論文給我研究 (推坑? , 結果還蠻有意思的, 決定 PO 文記錄一下。這篇論文是在探討 long input (context) 對 LLM 的性能表現有多大影響?

簡單舉個例子，如果你問了 LLM 一個問題，同時丟了一段 1K 的文字給他 (答案在裡面)，你可以預期模型能夠完美的回答。

如果同樣問題，但是丟了一段 100K 的文字呢? ( 假設 99K 都是完全不相關的雜訊) 這影響，就是這論文想要探討的。從結果來說，影響比預期大很多，不管你的模型是否支援 long context window ...

再來，如果同樣問題，但是給的 100K 文字，裡面除了有答案, 還包含干擾的資訊 (相似度高，但是跟答案不相關) 呢?

我拿一張圖當作代表: Needle Similarity Performance by Context Window and Performance Level, 這是這篇論文第一個實驗結果數據 (如圖)

同樣的任務要求, 你給他的 context 越大, 模型的表現會急速的下降… 大概就根圖上看到的一樣，超過 10K 表現下降的幅度就變大了，100K 更慘，就像斷崖一樣… 總之，這篇論文就做了這類各種不同的測試，來評估 "超長 context” 對 LLM 性能表現的影響。最終這些實驗，歸納了幾個結論:

1️⃣ 沒有干擾的情況下，就如上圖一般，輸入長度會影響模型表現

2️⃣ 有干擾項的情況下，會加劇性能退化的現象

3️⃣ 輸入的內容如果是有思考脈絡 (例如前面的思路接著後面的推論)，模型表現反而變差。LLM 對於沒有邏輯連貫性的長文，表現會比較好 (這點出乎我意料之外)

4️⃣ 重複詞彙的表現 (要求模型按照 context 重複輸出其中的一段內容)，Claude 輾壓 OpenAI / Google 的模型 XDDD

以上大概是這份論文的摘要，有興趣的可以看看。我其實不大擅長解讀這種研究論文，但是這主題正好是我有興趣的，所以我還是整理了一下我的心得，以下是主觀的見解，請自行評估是否採納 😀

過去幾個月，我聽過很多人跟我說…

>
> LLM 支援 Long Context Window, 所以 RAG 沒有用了
>

>
> 有 Long Context Window 的話，XXX 問題就解決掉了，以後不用傷腦筋了
>

大家普遍對 long context "過度樂觀”, 也因此我看過很多過度依賴 long context 模型能力的人, 而不肯自己做好 context engineering … 這篇論文等於狠狠地賞了這些人耳光 XDD, 因為有效的控制好你要餵給 LLM 的 context, 不管你的模型是否支援 long context window, 都是很重要的。

對我而言，我更確信了這些設計原則 (以前只是沒論文替我證實這些推論而已):

➡️ RAG 仍然是重要的
因為 RAG 的基本原理就是先透過檢索的技術 (不管是不是向量搜尋)，將相關的資訊挑出來再給 LLM 處理。本質上就是把 context 大小降低幾個數量及的做法。看了實驗數據，現在的主流模型，10K 以內的 context window 是不大需要擔心的, 100K 大致上是能接受的極限了, 再超過就雪崩了

➡️ 排除干擾是另一個關鍵的因素
論文內比較了干擾項對於性能的影響。所謂的干擾就是: 主題相近但是完全對答案沒有幫助的訊息。有點像是向量檢索的結果中，高相似度 != 高相關度一樣。這些干擾因素過多其實是會降低模型的表現。因此對我的 insight 是: 如果內容的品質有機會提升, 或是能做好前置處理 (或是重新生成) 讓這些干擾因素降低，其實都能大幅提升 RAG 的成果

➡️ 前後文有邏輯連貫的脈絡時，應先處理後再進行 RAG
這是另一個令我訝異 (同時也很興奮) 的結論。有脈絡的長文 (不就是在講我的部落格文章嗎 XDD) 其實不是 LLM 擅長的 context 類型，應該先針對問題處理過再放進 context 比較好。這正好跟我部落格改造工程的做法一致，誤打誤撞還走在對的道路上 XDD

➡️ “編輯能力” 仍然是模型的弱項
上面的實驗有個很有趣的 "重複詞彙"，就是叫模型 repeat 一串看似有規則但是卻隱含例外的文字 (範例舉了 apple apple apple apples apple apple apple apple apple apple apple ), 仔細看其中有一個是 apples, 其他都是 apple. 有的模型會 "眼花"，有的會照做，有的會無法執行，有的會問你 apples 到底是例外還是是你要的… 而大部分 copy 只改寫其中一小部分內容，是 LLM 執行 "編輯" 任務時會不斷重複的動作啊，看到這測試的表現，Claude 模型完全輾壓另外兩家 XDD, 這也難怪 coding 領域的應用大家都挑 claude ..

其實我的摘要，略過很多細節，而這些結論都對於你該 "如何" 有效運用大小有限的 context window 有很大的幫助。我認為這是 context engineering 很重要的基礎, 有興趣的人可以直接看原文, 大推 ~

![](/images/facebook-posts/facebook-posts-attachment-2025-09-08-001-001.png)
